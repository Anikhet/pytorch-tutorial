{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 23: Production Inference Patterns & MLOps\n",
    "\n",
    "**The Reality**: 87% of ML models never make it to production. The ones that do often fail within months.\n",
    "\n",
    "**Why?** Training a model is 10% of the work. The other 90% is:\n",
    "- Choosing the right inference pattern\n",
    "- Monitoring for drift and degradation\n",
    "- Cost optimization (inference = 90% of ML costs!)\n",
    "- Safe deployments and rollbacks\n",
    "- A/B testing and experimentation\n",
    "\n",
    "This notebook teaches you **production ML engineering** - the skills that separate researchers from production ML engineers at FAANG companies.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Master the **Feature/Training/Inference (FTI) Pipeline** architecture\n",
    "2. Choose between **batch vs online inference** correctly\n",
    "3. Implement **model monitoring** and drift detection\n",
    "4. Understand **cost optimization** strategies\n",
    "5. Deploy safely with **canary, blue-green**, and **A/B testing**\n",
    "6. Build **production-grade MLOps** workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The FTI Pipeline Architecture (2025 Standard)\n",
    "\n",
    "### Old Mental Model (Outdated)\n",
    "```\n",
    "Training Pipeline â†’ Model â†’ Serving\n",
    "```\n",
    "**Problem**: Treats ML as a monolith, hard to maintain\n",
    "\n",
    "### New Mental Model: FTI Pipelines\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Feature Pipelineâ”‚ â† Runs continuously\n",
    "â”‚  (hourly/daily) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“ (writes to Feature Store)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Training Pipelineâ”‚ â† Runs on schedule (daily/weekly)\n",
    "â”‚ (reads features)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“ (publishes to Model Registry)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Inference Pipelineâ”‚ â† Serves predictions\n",
    "â”‚ (reads features+model)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "**Three independent pipelines** sharing a common storage layer:\n",
    "- **Feature Store** (features)\n",
    "- **Model Registry** (models)\n",
    "- **Prediction Store** (outputs)\n",
    "\n",
    "**Benefits**:\n",
    "- Features computed once, used everywhere (no training-serving skew!)\n",
    "- Model updates independent of feature changes\n",
    "- Easy to debug each pipeline separately\n",
    "- Enables both batch AND real-time ML systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch vs Online Inference - The Decision Framework\n",
    "\n",
    "### Decision Tree\n",
    "```\n",
    "Do you need predictions in < 1 second?\n",
    "  â”œâ”€ Yes â†’ Online Inference\n",
    "  â””â”€ No â†’ Can you predict all possible inputs ahead of time?\n",
    "            â”œâ”€ Yes â†’ Batch Inference (cheaper!)\n",
    "            â””â”€ No â†’ Online Inference (long tail)\n",
    "```\n",
    "\n",
    "### Detailed Comparison\n",
    "\n",
    "| Aspect | Batch Inference | Online Inference |\n",
    "|--------|----------------|------------------|\n",
    "| **Latency** | Hours to days | < 100ms |\n",
    "| **Cost** | Low (scheduled) | High (always-on) |\n",
    "| **Complexity** | Simple | Complex (load balancing, caching) |\n",
    "| **Freshness** | Stale (hours old) | Real-time |\n",
    "| **Throughput** | Very high (parallel processing) | Limited by hardware |\n",
    "| **Compute** | Batch jobs (Spark, Airflow) | API servers (FastAPI, vLLM) |\n",
    "| **Storage** | Database (PostgreSQL, Redis) | In-memory cache |\n",
    "| **Use Cases** | Recommendations, email campaigns | Fraud detection, chatbots |\n",
    "\n",
    "### Hybrid Pattern (Best of Both)\n",
    "Many systems use BOTH:\n",
    "1. **Batch**: Pre-compute predictions for common cases\n",
    "2. **Online**: Fallback for cache misses or real-time needs\n",
    "\n",
    "**Example**: YouTube recommendations\n",
    "- Batch: Pre-compute top 100 videos for each user (nightly)\n",
    "- Online: Rerank based on current session + real-time signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "import random\n",
    "\n",
    "# Example: Hybrid Inference System\n",
    "class HybridInferenceSystem:\n",
    "    \"\"\"\n",
    "    Combines batch pre-computation with online fallback.\n",
    "    Used at Netflix, YouTube, Amazon, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, cache_size=1000):\n",
    "        self.model = model\n",
    "        self.cache = {}  # In production: Redis\n",
    "        self.cache_size = cache_size\n",
    "        self.stats = {'cache_hits': 0, 'cache_misses': 0}\n",
    "    \n",
    "    def batch_precompute(self, user_ids: List[str], features_batch: torch.Tensor):\n",
    "        \"\"\"Batch job: Pre-compute predictions for known users.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(features_batch)\n",
    "        \n",
    "        # Store in cache\n",
    "        for user_id, pred in zip(user_ids, predictions):\n",
    "            self.cache[user_id] = pred.tolist()\n",
    "        \n",
    "        print(f\"âœ… Pre-computed {len(user_ids)} predictions\")\n",
    "    \n",
    "    def online_predict(self, user_id: str, features: Optional[torch.Tensor] = None) -> List[float]:\n",
    "        \"\"\"Online API: Get prediction (cache or compute).\"\"\"\n",
    "        # Try cache first\n",
    "        if user_id in self.cache:\n",
    "            self.stats['cache_hits'] += 1\n",
    "            return self.cache[user_id]  # < 1ms latency!\n",
    "        \n",
    "        # Cache miss: Compute on-the-fly\n",
    "        self.stats['cache_misses'] += 1\n",
    "        if features is None:\n",
    "            features = torch.randn(1, 10)  # In production: fetch from feature store\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(features)\n",
    "        \n",
    "        result = prediction[0].tolist()\n",
    "        \n",
    "        # Update cache\n",
    "        if len(self.cache) < self.cache_size:\n",
    "            self.cache[user_id] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache_hit_rate(self) -> float:\n",
    "        total = self.stats['cache_hits'] + self.stats['cache_misses']\n",
    "        return self.stats['cache_hits'] / total if total > 0 else 0.0\n",
    "\n",
    "# Demo\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = DemoModel()\n",
    "model.eval()\n",
    "system = HybridInferenceSystem(model)\n",
    "\n",
    "# Step 1: Batch pre-computation (runs nightly)\n",
    "print(\"\\nğŸ“¦ BATCH JOB: Pre-computing predictions...\\n\")\n",
    "common_users = [f\"user_{i}\" for i in range(100)]\n",
    "batch_features = torch.randn(100, 10)\n",
    "system.batch_precompute(common_users, batch_features)\n",
    "\n",
    "# Step 2: Online requests\n",
    "print(\"\\nâš¡ ONLINE API: Serving requests...\\n\")\n",
    "for i in range(10):\n",
    "    # Mix of common (cached) and rare (uncached) users\n",
    "    if random.random() < 0.8:  # 80% are cached\n",
    "        user_id = f\"user_{random.randint(0, 99)}\"\n",
    "    else:  # 20% are new/rare users\n",
    "        user_id = f\"user_new_{i}\"\n",
    "    \n",
    "    start = time.time()\n",
    "    pred = system.online_predict(user_id)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    cached = \"âœ… CACHE\" if user_id in system.cache else \"âŒ COMPUTE\"\n",
    "    print(f\"{cached} | {user_id} | {latency:.2f}ms\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Cache hit rate: {system.get_cache_hit_rate()*100:.1f}%\")\n",
    "print(\"ğŸ’¡ In production, 90%+ hit rate saves millions in compute costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cost Optimization Strategies\n",
    "\n",
    "### The Economics of Inference\n",
    "**Fact**: Inference costs account for **90% of production ML expenses**.\n",
    "\n",
    "For a model serving 1M requests/day:\n",
    "- **Naive deployment**: $50,000/month\n",
    "- **Optimized deployment**: $5,000/month\n",
    "\n",
    "**10x cost reduction** is possible with these techniques:\n",
    "\n",
    "### 1. Model Optimization\n",
    "- **Quantization**: INT8 â†’ 4x smaller, 2-3x faster\n",
    "- **Distillation**: Train small model to mimic large one (10x cheaper)\n",
    "- **Pruning**: Remove unimportant weights (30-50% speedup)\n",
    "\n",
    "### 2. Batching\n",
    "- **Static batching**: Wait to fill batch â†’ 5-10x throughput\n",
    "- **Dynamic batching**: Flexible batch size â†’ 3-5x throughput\n",
    "- **Micro-batching**: Small batches (e.g., 10ms window) â†’ balance latency/throughput\n",
    "\n",
    "### 3. Caching\n",
    "- **Result caching**: Cache predictions for common inputs\n",
    "- **Feature caching**: Avoid recomputing features\n",
    "- **KV cache**: For LLMs (see Notebook 21)\n",
    "\n",
    "### 4. Hardware Selection\n",
    "- **CPU vs GPU**: Use CPU for <50 QPS, GPU for >50 QPS\n",
    "- **Spot instances**: 70% cost savings (with fault tolerance)\n",
    "- **Inference-optimized chips**: AWS Inferentia, Google TPU Edge\n",
    "\n",
    "### 5. Autoscaling\n",
    "- Scale down during low traffic (nights, weekends)\n",
    "- **Kubernetes HPA**: Scale based on QPS or latency\n",
    "- **Serverless**: AWS Lambda for bursty workloads\n",
    "\n",
    "### Cost Breakdown Example (Llama-3 8B)\n",
    "```\n",
    "Option 1: Naive (HuggingFace + A100)\n",
    "- 50 QPS @ $3/hour = $2,160/month\n",
    "\n",
    "Option 2: vLLM + Batching (A100)\n",
    "- 150 QPS @ $3/hour = $720/month (3x cheaper!)\n",
    "\n",
    "Option 3: vLLM + Quantization + Spot (A10g)\n",
    "- 100 QPS @ $0.50/hour = $360/month (6x cheaper!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Monitoring & Drift Detection\n",
    "\n",
    "### What to Monitor\n",
    "\n",
    "#### 1. Input Distribution Drift\n",
    "Are the inputs changing over time?\n",
    "\n",
    "#### 2. Output Distribution Drift  \n",
    "Are the predictions changing?\n",
    "\n",
    "#### 3. Performance Metrics\n",
    "When labels arrive, how accurate is the model?\n",
    "\n",
    "#### 4. System Metrics\n",
    "Latency, throughput, error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"Detect distribution drift using Population Stability Index (PSI).\"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_data, bins=10, threshold=0.2):\n",
    "        self.bins = bins\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Compute baseline distribution\n",
    "        self.baseline_hist, self.bin_edges = np.histogram(\n",
    "            baseline_data, bins=bins\n",
    "        )\n",
    "        self.baseline_hist = self.baseline_hist / len(baseline_data)  # Normalize\n",
    "    \n",
    "    def compute_psi(self, current_data):\n",
    "        \"\"\"\n",
    "        Compute Population Stability Index.\n",
    "        PSI < 0.1: No drift\n",
    "        PSI 0.1-0.2: Moderate drift\n",
    "        PSI > 0.2: Significant drift (retrain!)\n",
    "        \"\"\"\n",
    "        current_hist, _ = np.histogram(current_data, bins=self.bin_edges)\n",
    "        current_hist = current_hist / len(current_data)\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        current_hist = np.clip(current_hist, 1e-10, 1)\n",
    "        baseline_hist = np.clip(self.baseline_hist, 1e-10, 1)\n",
    "        \n",
    "        psi = np.sum(\n",
    "            (current_hist - baseline_hist) * np.log(current_hist / baseline_hist)\n",
    "        )\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def check_drift(self, current_data):\n",
    "        psi = self.compute_psi(current_data)\n",
    "        has_drift = psi > self.threshold\n",
    "        \n",
    "        if has_drift:\n",
    "            return True, f\"ğŸš¨ DRIFT DETECTED! PSI = {psi:.3f}\"\n",
    "        else:\n",
    "            return False, f\"âœ… No drift. PSI = {psi:.3f}\"\n",
    "\n",
    "# Demo: Simulating drift over time\n",
    "print(\"\\nğŸ“Š Drift Detection Demo\\n\")\n",
    "\n",
    "# Baseline: Training data distribution\n",
    "baseline = np.random.normal(0, 1, 10000)\n",
    "detector = DriftDetector(baseline, threshold=0.2)\n",
    "\n",
    "# Simulate production data over time\n",
    "scenarios = [\n",
    "    (\"Week 1\", np.random.normal(0, 1, 1000)),      # No drift\n",
    "    (\"Week 2\", np.random.normal(0.1, 1, 1000)),    # Slight drift\n",
    "    (\"Week 3\", np.random.normal(0.5, 1.2, 1000)),  # Moderate drift\n",
    "    (\"Week 4\", np.random.normal(2, 2, 1000)),      # Severe drift!\n",
    "]\n",
    "\n",
    "for period, data in scenarios:\n",
    "    has_drift, message = detector.check_drift(data)\n",
    "    print(f\"{period}: {message}\")\n",
    "    if has_drift:\n",
    "        print(\"   â†’ Recommend: Retrain model with recent data\")\n",
    "\n",
    "print(\"\\nğŸ’¡ In production, automate this check daily and trigger retraining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Safe Deployment Patterns\n",
    "\n",
    "### Pattern 1: Canary Deployment\n",
    "Gradually roll out new model to small percentage of traffic.\n",
    "\n",
    "```\n",
    "5% traffic â†’ New Model v2 â†’ Monitor for 1 hour\n",
    "            â†“ (if good)\n",
    "20% traffic â†’ New Model v2 â†’ Monitor for 1 hour\n",
    "            â†“ (if good)\n",
    "100% traffic â†’ New Model v2\n",
    "```\n",
    "\n",
    "### Pattern 2: Blue-Green Deployment\n",
    "Run two environments, switch traffic instantly.\n",
    "\n",
    "```\n",
    "Blue (Old Model v1) â† 100% traffic\n",
    "Green (New Model v2) â† 0% traffic (testing)\n",
    "            â†“ (when ready)\n",
    "Blue â† 0% traffic\n",
    "Green â† 100% traffic (instant switch!)\n",
    "```\n",
    "\n",
    "**Benefit**: Instant rollback if issues detected\n",
    "\n",
    "### Pattern 3: A/B Testing\n",
    "Run two models simultaneously, compare business metrics.\n",
    "\n",
    "```\n",
    "50% users â†’ Model A â†’ Track revenue, engagement\n",
    "50% users â†’ Model B â†’ Track revenue, engagement\n",
    "            â†“ (after 1-2 weeks)\n",
    "Choose winner based on business metrics\n",
    "```\n",
    "\n",
    "**Key**: Don't just compare model metrics (AUC, F1) - compare business metrics (revenue, engagement)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple A/B Testing Framework\n",
    "class ABTestingFramework:\n",
    "    def __init__(self, model_a, model_b, split_ratio=0.5):\n",
    "        self.model_a = model_a  # Control\n",
    "        self.model_b = model_b  # Treatment\n",
    "        self.split_ratio = split_ratio\n",
    "        \n",
    "        self.metrics_a = {'predictions': [], 'latencies': []}\n",
    "        self.metrics_b = {'predictions': [], 'latencies': []}\n",
    "    \n",
    "    def route_request(self, user_id: str, features: torch.Tensor):\n",
    "        \"\"\"Route user to model A or B based on user_id hash.\"\"\"\n",
    "        # Deterministic assignment based on user_id\n",
    "        user_hash = hash(user_id) % 100\n",
    "        use_model_b = (user_hash / 100) < self.split_ratio\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if use_model_b:\n",
    "            with torch.no_grad():\n",
    "                pred = self.model_b(features)\n",
    "            latency = time.time() - start\n",
    "            self.metrics_b['predictions'].append(pred.item())\n",
    "            self.metrics_b['latencies'].append(latency)\n",
    "            variant = 'B'\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = self.model_a(features)\n",
    "            latency = time.time() - start\n",
    "            self.metrics_a['predictions'].append(pred.item())\n",
    "            self.metrics_a['latencies'].append(latency)\n",
    "            variant = 'A'\n",
    "        \n",
    "        return {\n",
    "            'prediction': pred.item(),\n",
    "            'variant': variant,\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'Model A': {\n",
    "                'requests': len(self.metrics_a['predictions']),\n",
    "                'avg_prediction': np.mean(self.metrics_a['predictions']),\n",
    "                'avg_latency_ms': np.mean(self.metrics_a['latencies']) * 1000\n",
    "            },\n",
    "            'Model B': {\n",
    "                'requests': len(self.metrics_b['predictions']),\n",
    "                'avg_prediction': np.mean(self.metrics_b['predictions']),\n",
    "                'avg_latency_ms': np.mean(self.metrics_b['latencies']) * 1000\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Demo\n",
    "model_a = DemoModel()  # Control\n",
    "model_b = DemoModel()  # Treatment (slightly modified)\n",
    "model_a.eval()\n",
    "model_b.eval()\n",
    "\n",
    "ab_test = ABTestingFramework(model_a, model_b, split_ratio=0.5)\n",
    "\n",
    "print(\"\\nğŸ”¬ A/B Test Simulation\\n\")\n",
    "for i in range(100):\n",
    "    user_id = f\"user_{random.randint(1, 1000)}\"\n",
    "    features = torch.randn(1, 10)\n",
    "    result = ab_test.route_request(user_id, features)\n",
    "\n",
    "summary = ab_test.get_summary()\n",
    "print(\"\\nğŸ“Š A/B Test Results:\\n\")\n",
    "for model_name, metrics in summary.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Requests: {metrics['requests']}\")\n",
    "    print(f\"  Avg Prediction: {metrics['avg_prediction']:.4f}\")\n",
    "    print(f\"  Avg Latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ In production, track business metrics (revenue, CTR, engagement) not just model metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production MLOps Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [ ] Model performance validated on holdout set\n",
    "- [ ] Latency tested under load (stress testing)\n",
    "- [ ] Feature consistency verified (training vs serving)\n",
    "- [ ] Model size optimized (quantization, pruning)\n",
    "- [ ] Rollback plan documented\n",
    "- [ ] Monitoring dashboards created\n",
    "- [ ] Alerts configured (latency, error rate, drift)\n",
    "\n",
    "### Deployment\n",
    "- [ ] Use canary or blue-green deployment\n",
    "- [ ] Start with 5% traffic\n",
    "- [ ] Monitor for 1-2 hours before scaling up\n",
    "- [ ] Have kill switch ready (instant rollback)\n",
    "- [ ] Document deployment in runbook\n",
    "\n",
    "### Post-Deployment\n",
    "- [ ] Monitor drift daily (PSI, KL divergence)\n",
    "- [ ] Track business metrics (revenue, engagement)\n",
    "- [ ] Collect feedback labels for model retraining\n",
    "- [ ] Schedule retraining (weekly/monthly)\n",
    "- [ ] Review costs and optimize\n",
    "- [ ] Conduct post-mortem if issues occurred\n",
    "\n",
    "### Continuous Improvement\n",
    "- [ ] A/B test new model versions\n",
    "- [ ] Experiment with different architectures\n",
    "- [ ] Optimize inference costs (batching, caching)\n",
    "- [ ] Update feature engineering\n",
    "- [ ] Retrain on fresh data\n",
    "- [ ] Archive old model versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Production ML Engineering in 2025\n",
    "\n",
    "### Core Principles\n",
    "1. **FTI Pipeline Architecture**: Separate feature, training, and inference pipelines\n",
    "2. **Choose the Right Pattern**: Batch for cost, online for latency\n",
    "3. **Monitor Everything**: Drift, performance, costs\n",
    "4. **Deploy Safely**: Canary, blue-green, A/B testing\n",
    "5. **Optimize Costs**: Inference is 90% of expenses!\n",
    "\n",
    "### Technology Stack\n",
    "- **Feature Store**: Feast, Tecton\n",
    "- **Model Registry**: MLflow, Weights & Biases\n",
    "- **Orchestration**: Airflow, Prefect, Dagster\n",
    "- **Monitoring**: Prometheus, Grafana, Evidently\n",
    "- **Deployment**: Kubernetes, Docker\n",
    "- **Serving**: FastAPI, vLLM, TensorRT-LLM\n",
    "\n",
    "### What FAANG Expects\n",
    "âœ… Understanding of FTI pipeline architecture\n",
    "âœ… Ability to choose batch vs online correctly\n",
    "âœ… Experience with monitoring and drift detection\n",
    "âœ… Knowledge of cost optimization strategies\n",
    "âœ… Familiarity with safe deployment patterns\n",
    "âœ… A/B testing and experimentation skills\n",
    "âœ… Production debugging and incident response\n",
    "\n",
    "### Anti-Patterns to Avoid\n",
    "âŒ Deploying without monitoring\n",
    "âŒ Ignoring training-serving skew\n",
    "âŒ 100% rollout without testing\n",
    "âŒ Not tracking business metrics\n",
    "âŒ Neglecting cost optimization\n",
    "âŒ No rollback plan\n",
    "\n",
    "### Further Reading\n",
    "- [MLOps Principles (Google)](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "- [Chip Huyen: Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)\n",
    "- [FTI Pipeline Architecture (Hopsworks)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n",
    "\n",
    "**You now have production ML engineering skills! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
