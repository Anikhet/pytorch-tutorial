{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 23: Production Inference Patterns & MLOps\n",
    "\n",
    "**The Reality**: 87% of ML models never make it to production. The ones that do often fail within months.\n",
    "\n",
    "**Why?** Training a model is 10% of the work. The other 90% is:\n",
    "- Choosing the right inference pattern\n",
    "- Monitoring for drift and degradation\n",
    "- Cost optimization (inference = 90% of ML costs!)\n",
    "- Safe deployments and rollbacks\n",
    "- A/B testing and experimentation\n",
    "\n",
    "This notebook teaches you **production ML engineering** - the skills that separate researchers from production ML engineers at FAANG companies.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Master the **Feature/Training/Inference (FTI) Pipeline** architecture\n",
    "2. Choose between **batch vs online inference** correctly\n",
    "3. Implement **model monitoring** and drift detection\n",
    "4. Understand **cost optimization** strategies\n",
    "5. Deploy safely with **canary, blue-green**, and **A/B testing**\n",
    "6. Build **production-grade MLOps** workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: The FTI Pipeline Architecture (2025 Standard)\n\n### Old Mental Model (Outdated)\n```\nTraining Pipeline â†’ Model â†’ Serving\n```\n**Problem**: Treats ML as a monolith, hard to maintain\n\n### New Mental Model: FTI Pipelines\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Feature Pipelineâ”‚ â† Runs continuously\nâ”‚  (hourly/daily) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â†“ (writes to Feature Store)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Training Pipelineâ”‚ â† Runs on schedule (daily/weekly)\nâ”‚ (reads features)â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â†“ (publishes to Model Registry)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Inference Pipelineâ”‚ â† Serves predictions\nâ”‚ (reads features+model)â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Key Insight\n**Three independent pipelines** sharing a common storage layer:\n- **Feature Store** (features)\n- **Model Registry** (models)\n- **Prediction Store** (outputs)\n\n**Benefits**:\n- Features computed once, used everywhere (no training-serving skew!)\n- Model updates independent of feature changes\n- Easy to debug each pipeline separately\n- Enables both batch AND real-time ML systems\n\n### Training-Serving Skew (The Silent Killer)\n\nThis is the most common production ML bug and one of the hardest to detect:\n\n**What it is**: When the features used during training differ from the features used during inference. The model was trained on one \"version\" of the data but sees a different version in production.\n\n**Common causes**:\n1. **Different code paths**: Training preprocesses data in Python/Pandas, serving preprocesses in Java/Go â€” subtle differences accumulate\n2. **Time leakage**: Training uses features computed with future information that's unavailable at prediction time\n3. **Stale features**: Feature store hasn't been updated, so serving uses outdated values\n4. **Different library versions**: NumPy or Pandas version difference causes rounding differences\n\n**How to prevent it**: The Feature Store pattern solves this by computing features exactly once and storing them. Both training and serving read from the same store, guaranteeing consistency.\n\n### Feature Store Deep Dive\n\nA Feature Store is a centralized repository that manages the lifecycle of ML features:\n\n```\nRaw Data â†’ Feature Pipeline â†’ Feature Store â†’ {Training, Serving}\n                                    â”‚\n                              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n                              â”‚ Offline    â”‚ â† Historical features for training\n                              â”‚ (batch)    â”‚    (e.g., Hive/S3/BigQuery)\n                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                              â”‚ Online     â”‚ â† Low-latency features for serving\n                              â”‚ (real-time)â”‚    (e.g., Redis/DynamoDB)\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\nPopular Feature Stores: **Feast** (open-source), **Tecton** (managed), **Hopsworks** (open-source), **Databricks Feature Store**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch vs Online Inference - The Decision Framework\n",
    "\n",
    "### Decision Tree\n",
    "```\n",
    "Do you need predictions in < 1 second?\n",
    "  â”œâ”€ Yes â†’ Online Inference\n",
    "  â””â”€ No â†’ Can you predict all possible inputs ahead of time?\n",
    "            â”œâ”€ Yes â†’ Batch Inference (cheaper!)\n",
    "            â””â”€ No â†’ Online Inference (long tail)\n",
    "```\n",
    "\n",
    "### Detailed Comparison\n",
    "\n",
    "| Aspect | Batch Inference | Online Inference |\n",
    "|--------|----------------|------------------|\n",
    "| **Latency** | Hours to days | < 100ms |\n",
    "| **Cost** | Low (scheduled) | High (always-on) |\n",
    "| **Complexity** | Simple | Complex (load balancing, caching) |\n",
    "| **Freshness** | Stale (hours old) | Real-time |\n",
    "| **Throughput** | Very high (parallel processing) | Limited by hardware |\n",
    "| **Compute** | Batch jobs (Spark, Airflow) | API servers (FastAPI, vLLM) |\n",
    "| **Storage** | Database (PostgreSQL, Redis) | In-memory cache |\n",
    "| **Use Cases** | Recommendations, email campaigns | Fraud detection, chatbots |\n",
    "\n",
    "### Hybrid Pattern (Best of Both)\n",
    "Many systems use BOTH:\n",
    "1. **Batch**: Pre-compute predictions for common cases\n",
    "2. **Online**: Fallback for cache misses or real-time needs\n",
    "\n",
    "**Example**: YouTube recommendations\n",
    "- Batch: Pre-compute top 100 videos for each user (nightly)\n",
    "- Online: Rerank based on current session + real-time signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "import random\n",
    "\n",
    "# Example: Hybrid Inference System\n",
    "class HybridInferenceSystem:\n",
    "    \"\"\"\n",
    "    Combines batch pre-computation with online fallback.\n",
    "    Used at Netflix, YouTube, Amazon, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, cache_size=1000):\n",
    "        self.model = model\n",
    "        self.cache = {}  # In production: Redis\n",
    "        self.cache_size = cache_size\n",
    "        self.stats = {'cache_hits': 0, 'cache_misses': 0}\n",
    "    \n",
    "    def batch_precompute(self, user_ids: List[str], features_batch: torch.Tensor):\n",
    "        \"\"\"Batch job: Pre-compute predictions for known users.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(features_batch)\n",
    "        \n",
    "        # Store in cache\n",
    "        for user_id, pred in zip(user_ids, predictions):\n",
    "            self.cache[user_id] = pred.tolist()\n",
    "        \n",
    "        print(f\"âœ… Pre-computed {len(user_ids)} predictions\")\n",
    "    \n",
    "    def online_predict(self, user_id: str, features: Optional[torch.Tensor] = None) -> List[float]:\n",
    "        \"\"\"Online API: Get prediction (cache or compute).\"\"\"\n",
    "        # Try cache first\n",
    "        if user_id in self.cache:\n",
    "            self.stats['cache_hits'] += 1\n",
    "            return self.cache[user_id]  # < 1ms latency!\n",
    "        \n",
    "        # Cache miss: Compute on-the-fly\n",
    "        self.stats['cache_misses'] += 1\n",
    "        if features is None:\n",
    "            features = torch.randn(1, 10)  # In production: fetch from feature store\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(features)\n",
    "        \n",
    "        result = prediction[0].tolist()\n",
    "        \n",
    "        # Update cache\n",
    "        if len(self.cache) < self.cache_size:\n",
    "            self.cache[user_id] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache_hit_rate(self) -> float:\n",
    "        total = self.stats['cache_hits'] + self.stats['cache_misses']\n",
    "        return self.stats['cache_hits'] / total if total > 0 else 0.0\n",
    "\n",
    "# Demo\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = DemoModel()\n",
    "model.eval()\n",
    "system = HybridInferenceSystem(model)\n",
    "\n",
    "# Step 1: Batch pre-computation (runs nightly)\n",
    "print(\"\\nğŸ“¦ BATCH JOB: Pre-computing predictions...\\n\")\n",
    "common_users = [f\"user_{i}\" for i in range(100)]\n",
    "batch_features = torch.randn(100, 10)\n",
    "system.batch_precompute(common_users, batch_features)\n",
    "\n",
    "# Step 2: Online requests\n",
    "print(\"\\nâš¡ ONLINE API: Serving requests...\\n\")\n",
    "for i in range(10):\n",
    "    # Mix of common (cached) and rare (uncached) users\n",
    "    if random.random() < 0.8:  # 80% are cached\n",
    "        user_id = f\"user_{random.randint(0, 99)}\"\n",
    "    else:  # 20% are new/rare users\n",
    "        user_id = f\"user_new_{i}\"\n",
    "    \n",
    "    start = time.time()\n",
    "    pred = system.online_predict(user_id)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    cached = \"âœ… CACHE\" if user_id in system.cache else \"âŒ COMPUTE\"\n",
    "    print(f\"{cached} | {user_id} | {latency:.2f}ms\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Cache hit rate: {system.get_cache_hit_rate()*100:.1f}%\")\n",
    "print(\"ğŸ’¡ In production, 90%+ hit rate saves millions in compute costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cost Optimization Strategies\n",
    "\n",
    "### The Economics of Inference\n",
    "**Fact**: Inference costs account for **90% of production ML expenses**.\n",
    "\n",
    "For a model serving 1M requests/day:\n",
    "- **Naive deployment**: $50,000/month\n",
    "- **Optimized deployment**: $5,000/month\n",
    "\n",
    "**10x cost reduction** is possible with these techniques:\n",
    "\n",
    "### 1. Model Optimization\n",
    "- **Quantization**: INT8 â†’ 4x smaller, 2-3x faster\n",
    "- **Distillation**: Train small model to mimic large one (10x cheaper)\n",
    "- **Pruning**: Remove unimportant weights (30-50% speedup)\n",
    "\n",
    "### 2. Batching\n",
    "- **Static batching**: Wait to fill batch â†’ 5-10x throughput\n",
    "- **Dynamic batching**: Flexible batch size â†’ 3-5x throughput\n",
    "- **Micro-batching**: Small batches (e.g., 10ms window) â†’ balance latency/throughput\n",
    "\n",
    "### 3. Caching\n",
    "- **Result caching**: Cache predictions for common inputs\n",
    "- **Feature caching**: Avoid recomputing features\n",
    "- **KV cache**: For LLMs (see Notebook 21)\n",
    "\n",
    "### 4. Hardware Selection\n",
    "- **CPU vs GPU**: Use CPU for <50 QPS, GPU for >50 QPS\n",
    "- **Spot instances**: 70% cost savings (with fault tolerance)\n",
    "- **Inference-optimized chips**: AWS Inferentia, Google TPU Edge\n",
    "\n",
    "### 5. Autoscaling\n",
    "- Scale down during low traffic (nights, weekends)\n",
    "- **Kubernetes HPA**: Scale based on QPS or latency\n",
    "- **Serverless**: AWS Lambda for bursty workloads\n",
    "\n",
    "### Cost Breakdown Example (Llama-3 8B)\n",
    "```\n",
    "Option 1: Naive (HuggingFace + A100)\n",
    "- 50 QPS @ $3/hour = $2,160/month\n",
    "\n",
    "Option 2: vLLM + Batching (A100)\n",
    "- 150 QPS @ $3/hour = $720/month (3x cheaper!)\n",
    "\n",
    "Option 3: vLLM + Quantization + Spot (A10g)\n",
    "- 100 QPS @ $0.50/hour = $360/month (6x cheaper!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Model Monitoring & Drift Detection\n\n### What to Monitor\n\n#### 1. Input Distribution Drift (Data Drift)\nAre the inputs changing over time? For example, if your fraud detection model was trained on transactions averaging $50, but now the average is $500 (holiday season), the model may perform poorly.\n\n**Types of drift**:\n- **Covariate shift**: Input distribution changes (most common)\n- **Concept drift**: The relationship between inputs and outputs changes (e.g., \"fraud patterns\" evolve)\n- **Label drift**: The distribution of labels changes (e.g., fraud rate increases)\n\n#### 2. Output Distribution Drift  \nAre the predictions changing? A sudden shift in prediction distribution (e.g., the model predicts \"fraud\" 10x more than yesterday) indicates something is wrong â€” either the inputs changed or the model is malfunctioning.\n\n#### 3. Performance Metrics\nWhen labels arrive, how accurate is the model? In many production systems, ground truth labels arrive with a delay (e.g., you know if a transaction was actually fraud only after investigation, which could take days).\n\n#### 4. System Metrics\nLatency (P50, P95, P99), throughput (requests/sec), error rate, GPU utilization, memory usage.\n\n### PSI (Population Stability Index) â€” The Standard Drift Metric\n\nPSI compares two distributions by measuring how much one has \"shifted\" from the other:\n\n$$ PSI = \\sum_{i=1}^{N} (p_i^{current} - p_i^{baseline}) \\times \\ln\\left(\\frac{p_i^{current}}{p_i^{baseline}}\\right) $$\n\nInterpretation thresholds:\n- **PSI < 0.1**: No drift â€” distributions are stable\n- **PSI 0.1 - 0.2**: Moderate drift â€” investigate, consider retraining\n- **PSI > 0.2**: Significant drift â€” retrain the model\n\n**Why PSI over simpler metrics?** A simple mean/variance comparison can miss distribution shape changes. PSI compares the full histogram, catching subtle shifts like bimodal distributions or tail changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"Detect distribution drift using Population Stability Index (PSI).\"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_data, bins=10, threshold=0.2):\n",
    "        self.bins = bins\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Compute baseline distribution\n",
    "        self.baseline_hist, self.bin_edges = np.histogram(\n",
    "            baseline_data, bins=bins\n",
    "        )\n",
    "        self.baseline_hist = self.baseline_hist / len(baseline_data)  # Normalize\n",
    "    \n",
    "    def compute_psi(self, current_data):\n",
    "        \"\"\"\n",
    "        Compute Population Stability Index.\n",
    "        PSI < 0.1: No drift\n",
    "        PSI 0.1-0.2: Moderate drift\n",
    "        PSI > 0.2: Significant drift (retrain!)\n",
    "        \"\"\"\n",
    "        current_hist, _ = np.histogram(current_data, bins=self.bin_edges)\n",
    "        current_hist = current_hist / len(current_data)\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        current_hist = np.clip(current_hist, 1e-10, 1)\n",
    "        baseline_hist = np.clip(self.baseline_hist, 1e-10, 1)\n",
    "        \n",
    "        psi = np.sum(\n",
    "            (current_hist - baseline_hist) * np.log(current_hist / baseline_hist)\n",
    "        )\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def check_drift(self, current_data):\n",
    "        psi = self.compute_psi(current_data)\n",
    "        has_drift = psi > self.threshold\n",
    "        \n",
    "        if has_drift:\n",
    "            return True, f\"ğŸš¨ DRIFT DETECTED! PSI = {psi:.3f}\"\n",
    "        else:\n",
    "            return False, f\"âœ… No drift. PSI = {psi:.3f}\"\n",
    "\n",
    "# Demo: Simulating drift over time\n",
    "print(\"\\nğŸ“Š Drift Detection Demo\\n\")\n",
    "\n",
    "# Baseline: Training data distribution\n",
    "baseline = np.random.normal(0, 1, 10000)\n",
    "detector = DriftDetector(baseline, threshold=0.2)\n",
    "\n",
    "# Simulate production data over time\n",
    "scenarios = [\n",
    "    (\"Week 1\", np.random.normal(0, 1, 1000)),      # No drift\n",
    "    (\"Week 2\", np.random.normal(0.1, 1, 1000)),    # Slight drift\n",
    "    (\"Week 3\", np.random.normal(0.5, 1.2, 1000)),  # Moderate drift\n",
    "    (\"Week 4\", np.random.normal(2, 2, 1000)),      # Severe drift!\n",
    "]\n",
    "\n",
    "for period, data in scenarios:\n",
    "    has_drift, message = detector.check_drift(data)\n",
    "    print(f\"{period}: {message}\")\n",
    "    if has_drift:\n",
    "        print(\"   â†’ Recommend: Retrain model with recent data\")\n",
    "\n",
    "print(\"\\nğŸ’¡ In production, automate this check daily and trigger retraining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5: Safe Deployment Patterns\n\n### Pattern 1: Canary Deployment\nGradually roll out new model to small percentage of traffic.\n\n```\n5% traffic â†’ New Model v2 â†’ Monitor for 1 hour\n            â†“ (if good)\n20% traffic â†’ New Model v2 â†’ Monitor for 1 hour\n            â†“ (if good)\n100% traffic â†’ New Model v2\n```\n\n**When to use**: Default choice for most production ML deployments. The gradual rollout catches problems early before they affect all users.\n\n**What to monitor during canary**: Compare error rates, latency, and business metrics (revenue, engagement) between canary and control traffic. If the canary shows degradation, roll back immediately.\n\n### Pattern 2: Blue-Green Deployment\nRun two environments, switch traffic instantly.\n\n```\nBlue (Old Model v1) â† 100% traffic\nGreen (New Model v2) â† 0% traffic (testing)\n            â†“ (when ready)\nBlue â† 0% traffic\nGreen â† 100% traffic (instant switch!)\n```\n\n**Benefit**: Instant rollback if issues detected â€” just switch traffic back to Blue.\n\n**When to use**: When you need zero-downtime deployment and fast rollback. More expensive (2x infrastructure) but safer for critical systems.\n\n### Pattern 3: Shadow Deployment\nRun the new model alongside production without serving its predictions to users.\n\n```\n100% traffic â†’ Model v1 â†’ Serve to users\n      â””â”€â”€â”€â”€â”€â†’ Model v2 â†’ Log predictions (but don't serve)\n```\n\nCompare predictions offline. Only promote v2 when its logged predictions consistently outperform v1.\n\n**When to use**: High-risk models (medical, financial) where you need extensive validation before serving.\n\n### Pattern 4: A/B Testing\nRun two models simultaneously, compare business metrics.\n\n```\n50% users â†’ Model A â†’ Track revenue, engagement\n50% users â†’ Model B â†’ Track revenue, engagement\n            â†“ (after 1-2 weeks)\nChoose winner based on business metrics\n```\n\n**Key**: Don't just compare model metrics (AUC, F1) â€” compare business metrics (revenue, engagement)! A model with lower AUC might actually drive more revenue due to better calibration or different failure modes.\n\n### Statistical Rigor in A/B Tests\n\nCommon mistakes that lead to wrong conclusions:\n1. **Peeking**: Checking results daily and stopping when p < 0.05. This inflates false positive rate from 5% to 25%+. Use sequential testing or wait for the predetermined sample size.\n2. **Underpowered tests**: Not enough samples to detect small effects. Calculate required sample size before starting.\n3. **Multiple comparisons**: Testing 20 metrics and declaring victory when one is significant. Apply Bonferroni correction.\n4. **Network effects**: User A's experience changes user B's behavior (social networks). Use cluster randomization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple A/B Testing Framework\n",
    "class ABTestingFramework:\n",
    "    def __init__(self, model_a, model_b, split_ratio=0.5):\n",
    "        self.model_a = model_a  # Control\n",
    "        self.model_b = model_b  # Treatment\n",
    "        self.split_ratio = split_ratio\n",
    "        \n",
    "        self.metrics_a = {'predictions': [], 'latencies': []}\n",
    "        self.metrics_b = {'predictions': [], 'latencies': []}\n",
    "    \n",
    "    def route_request(self, user_id: str, features: torch.Tensor):\n",
    "        \"\"\"Route user to model A or B based on user_id hash.\"\"\"\n",
    "        # Deterministic assignment based on user_id\n",
    "        user_hash = hash(user_id) % 100\n",
    "        use_model_b = (user_hash / 100) < self.split_ratio\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if use_model_b:\n",
    "            with torch.no_grad():\n",
    "                pred = self.model_b(features)\n",
    "            latency = time.time() - start\n",
    "            self.metrics_b['predictions'].append(pred.item())\n",
    "            self.metrics_b['latencies'].append(latency)\n",
    "            variant = 'B'\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = self.model_a(features)\n",
    "            latency = time.time() - start\n",
    "            self.metrics_a['predictions'].append(pred.item())\n",
    "            self.metrics_a['latencies'].append(latency)\n",
    "            variant = 'A'\n",
    "        \n",
    "        return {\n",
    "            'prediction': pred.item(),\n",
    "            'variant': variant,\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'Model A': {\n",
    "                'requests': len(self.metrics_a['predictions']),\n",
    "                'avg_prediction': np.mean(self.metrics_a['predictions']),\n",
    "                'avg_latency_ms': np.mean(self.metrics_a['latencies']) * 1000\n",
    "            },\n",
    "            'Model B': {\n",
    "                'requests': len(self.metrics_b['predictions']),\n",
    "                'avg_prediction': np.mean(self.metrics_b['predictions']),\n",
    "                'avg_latency_ms': np.mean(self.metrics_b['latencies']) * 1000\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Demo\n",
    "model_a = DemoModel()  # Control\n",
    "model_b = DemoModel()  # Treatment (slightly modified)\n",
    "model_a.eval()\n",
    "model_b.eval()\n",
    "\n",
    "ab_test = ABTestingFramework(model_a, model_b, split_ratio=0.5)\n",
    "\n",
    "print(\"\\nğŸ”¬ A/B Test Simulation\\n\")\n",
    "for i in range(100):\n",
    "    user_id = f\"user_{random.randint(1, 1000)}\"\n",
    "    features = torch.randn(1, 10)\n",
    "    result = ab_test.route_request(user_id, features)\n",
    "\n",
    "summary = ab_test.get_summary()\n",
    "print(\"\\nğŸ“Š A/B Test Results:\\n\")\n",
    "for model_name, metrics in summary.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Requests: {metrics['requests']}\")\n",
    "    print(f\"  Avg Prediction: {metrics['avg_prediction']:.4f}\")\n",
    "    print(f\"  Avg Latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ In production, track business metrics (revenue, CTR, engagement) not just model metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production MLOps Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [ ] Model performance validated on holdout set\n",
    "- [ ] Latency tested under load (stress testing)\n",
    "- [ ] Feature consistency verified (training vs serving)\n",
    "- [ ] Model size optimized (quantization, pruning)\n",
    "- [ ] Rollback plan documented\n",
    "- [ ] Monitoring dashboards created\n",
    "- [ ] Alerts configured (latency, error rate, drift)\n",
    "\n",
    "### Deployment\n",
    "- [ ] Use canary or blue-green deployment\n",
    "- [ ] Start with 5% traffic\n",
    "- [ ] Monitor for 1-2 hours before scaling up\n",
    "- [ ] Have kill switch ready (instant rollback)\n",
    "- [ ] Document deployment in runbook\n",
    "\n",
    "### Post-Deployment\n",
    "- [ ] Monitor drift daily (PSI, KL divergence)\n",
    "- [ ] Track business metrics (revenue, engagement)\n",
    "- [ ] Collect feedback labels for model retraining\n",
    "- [ ] Schedule retraining (weekly/monthly)\n",
    "- [ ] Review costs and optimize\n",
    "- [ ] Conduct post-mortem if issues occurred\n",
    "\n",
    "### Continuous Improvement\n",
    "- [ ] A/B test new model versions\n",
    "- [ ] Experiment with different architectures\n",
    "- [ ] Optimize inference costs (batching, caching)\n",
    "- [ ] Update feature engineering\n",
    "- [ ] Retrain on fresh data\n",
    "- [ ] Archive old model versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Production ML Engineering in 2025\n",
    "\n",
    "### Core Principles\n",
    "1. **FTI Pipeline Architecture**: Separate feature, training, and inference pipelines\n",
    "2. **Choose the Right Pattern**: Batch for cost, online for latency\n",
    "3. **Monitor Everything**: Drift, performance, costs\n",
    "4. **Deploy Safely**: Canary, blue-green, A/B testing\n",
    "5. **Optimize Costs**: Inference is 90% of expenses!\n",
    "\n",
    "### Technology Stack\n",
    "- **Feature Store**: Feast, Tecton\n",
    "- **Model Registry**: MLflow, Weights & Biases\n",
    "- **Orchestration**: Airflow, Prefect, Dagster\n",
    "- **Monitoring**: Prometheus, Grafana, Evidently\n",
    "- **Deployment**: Kubernetes, Docker\n",
    "- **Serving**: FastAPI, vLLM, TensorRT-LLM\n",
    "\n",
    "### What FAANG Expects\n",
    "âœ… Understanding of FTI pipeline architecture\n",
    "âœ… Ability to choose batch vs online correctly\n",
    "âœ… Experience with monitoring and drift detection\n",
    "âœ… Knowledge of cost optimization strategies\n",
    "âœ… Familiarity with safe deployment patterns\n",
    "âœ… A/B testing and experimentation skills\n",
    "âœ… Production debugging and incident response\n",
    "\n",
    "### Anti-Patterns to Avoid\n",
    "âŒ Deploying without monitoring\n",
    "âŒ Ignoring training-serving skew\n",
    "âŒ 100% rollout without testing\n",
    "âŒ Not tracking business metrics\n",
    "âŒ Neglecting cost optimization\n",
    "âŒ No rollback plan\n",
    "\n",
    "### Further Reading\n",
    "- [MLOps Principles (Google)](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "- [Chip Huyen: Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)\n",
    "- [FTI Pipeline Architecture (Hopsworks)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n",
    "\n",
    "**You now have production ML engineering skills! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}