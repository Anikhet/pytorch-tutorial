{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Working with Real Data\n",
    "\n",
    "In the real world, data doesn't come in nice pre-packaged tensors. It comes in CSV files, image folders, text documents, and databases. This notebook teaches you how to handle real-world data in PyTorch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand `Dataset` and `DataLoader` classes\n",
    "- Create custom datasets for CSV and image data\n",
    "- Use DataLoaders for batching and shuffling\n",
    "- Apply data transformations and augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## The PyTorch Data Pipeline\n",
    "\n",
    "The standard pipeline for handling data in PyTorch involves two main classes:\n",
    "\n",
    "1. **`torch.utils.data.Dataset`**: Stores the samples and their corresponding labels.\n",
    "2. **`torch.utils.data.DataLoader`**: Wraps an iterable around the Dataset to enable easy access to the samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset from CSV\n",
    "\n",
    "Let's simulate a common scenario: you have a CSV file with features and labels.\n",
    "\n",
    "First, we'll create a dummy CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy CSV file\n",
    "data = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'feature3': np.random.rand(100),\n",
    "    'label': np.random.randint(0, 2, 100)  # Binary classification\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('dummy_data.csv', index=False)\n",
    "print(\"Created dummy_data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a custom Dataset class to read this CSV.\n",
    "\n",
    "A custom Dataset must implement three functions:\n",
    "- `__init__`: Initialize data, download, etc.\n",
    "- `__len__`: Return the size of the dataset\n",
    "- `__getitem__`: Return one sample (feature, label) at the given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features (columns 0-2)\n",
    "        features = self.data.iloc[idx, 0:3].values.astype(np.float32)\n",
    "        # Get label (column 3)\n",
    "        label = self.data.iloc[idx, 3]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        features = torch.tensor(features)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "# Test the dataset\n",
    "dataset = CSVDataset('dummy_data.csv')\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "features, label = dataset[0]\n",
    "print(f\"First sample features: {features}\")\n",
    "print(f\"First sample label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using DataLoader\n",
    "\n",
    "The `DataLoader` handles batching, shuffling, and parallel data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "print(\"Iterating through DataLoader:\")\n",
    "for i, (features, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Features shape: {features.shape}\")\n",
    "    print(f\"  Labels: {labels}\")\n",
    "    if i == 2:  # Stop after 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Data and Transforms\n",
    "\n",
    "For images, we often use `torchvision.transforms` for preprocessing and augmentation.\n",
    "\n",
    "Common transforms:\n",
    "- `Resize`: Change image size\n",
    "- `ToTensor`: Convert image to tensor (scales to 0-1, moves channels first)\n",
    "- `Normalize`: Normalize with mean and std\n",
    "- `RandomHorizontalFlip`: Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Augmentation\n",
    "    transforms.ToTensor(),  # Converts [0, 255] to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "print(\"Transforms defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ImageFolder\n",
    "\n",
    "If your images are organized in folders by class (e.g., `data/cats/`, `data/dogs/`), you can use `torchvision.datasets.ImageFolder`.\n",
    "\n",
    "```python\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# dataset = ImageFolder(root='path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "This automatically assigns labels based on folder names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways (Part 1)\n\n1. **Dataset Class**: Implement `__len__` and `__getitem__` to handle your custom data.\n2. **DataLoader**: Use this for efficient batching and shuffling.\n3. **Transforms**: Essential for image preprocessing and augmentation.\n4. **ImageFolder**: The easiest way to load classification datasets organized by folder.\n\n---\n\n# PART 2: HANDLING IMBALANCED DATA (CRITICAL for Real-World ML)\n\nThis is one of the most common problems in production ML!"
  },
  {
   "cell_type": "markdown",
   "id": "j2uupdgndcm",
   "source": "## 5. The Imbalanced Data Problem\n\n### What is Class Imbalance?\nWhen classes have very different numbers of samples:\n- **Fraud detection**: 99.9% normal, 0.1% fraud\n- **Medical diagnosis**: 95% healthy, 5% disease\n- **Click prediction**: 98% no-click, 2% click\n\n### Why It's a Problem\n- Model learns to predict majority class always\n- **Example**: 99% accuracy by always predicting \"normal\" (useless!)\n- Minority class (the important one!) gets ignored\n\n### The Naive Model\n```python\n# A \"smart\" model that always predicts class 0\ndef predict(x):\n    return 0  # Always predict majority class\n\n# Achieves 99% accuracy on imbalanced data!\n# But completely useless for detecting fraud/disease\n```\n\n### FAANG Interview Question\n**\"How do you handle imbalanced datasets?\"** â† Asked at ALL FAANG companies\n\n**Answer (4 strategies):**\n1. **Class Weights**: Penalize mistakes on minority class more\n2. **Oversampling**: Duplicate minority class samples\n3. **Undersampling**: Remove majority class samples\n4. **Focal Loss**: Automatically focus on hard examples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4fop6uwy5rk",
   "source": "# Example: Training with proper metrics for imbalanced data\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Simulate predictions from a model\ntorch.manual_seed(42)\nmodel_simple = nn.Sequential(\n    nn.Linear(10, 32),\n    nn.ReLU(),\n    nn.Linear(32, 2)\n)\n\n# Get predictions\nwith torch.no_grad():\n    logits = model_simple(features)\n    probs = F.softmax(logits, dim=1)\n    predictions = logits.argmax(dim=1)\n\n# Calculate metrics\nprint(\"=\" * 60)\nprint(\"IMBALANCED DATA METRICS (What FAANG Interviewers Expect)\")\nprint(\"=\" * 60)\n\n# Confusion Matrix\ncm = confusion_matrix(labels.numpy(), predictions.numpy())\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\nprint(f\"  True Negatives:  {cm[0,0]}\")\nprint(f\"  False Positives: {cm[0,1]}\")\nprint(f\"  False Negatives: {cm[1,0]}\")\nprint(f\"  True Positives:  {cm[1,1]}\")\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(labels.numpy(), predictions.numpy(), \n                          target_names=['Class 0 (Majority)', 'Class 1 (Minority)']))\n\n# Calculate metrics manually for understanding\ntp = cm[1, 1]  # True Positives\nfp = cm[0, 1]  # False Positives\nfn = cm[1, 0]  # False Negatives\n\nprecision = tp / (tp + fp) if (tp + fp) > 0 else 0\nrecall = tp / (tp + fn) if (tp + fn) > 0 else 0\nf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\nprint(f\"\\nðŸ“Š Key Metrics for Class 1 (Minority):\")\nprint(f\"  Precision: {precision:.3f} (Of predicted frauds, how many were real?)\")\nprint(f\"  Recall:    {recall:.3f} (Of actual frauds, how many did we catch?)\")\nprint(f\"  F1-Score:  {f1:.3f} (Harmonic mean of precision and recall)\")\n\n# ROC-AUC (requires probabilities)\ntry:\n    auc = roc_auc_score(labels.numpy(), probs[:, 1].numpy())\n    print(f\"  AUC-ROC:   {auc:.3f} (Overall discrimination ability)\")\nexcept:\n    print(\"  AUC-ROC:   N/A (need more diverse predictions)\")\n\nprint(\"\\nâœ… These are the metrics that matter for imbalanced data!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('dummy_data.csv'):\n",
    "    os.remove('dummy_data.csv')\n",
    "    print(\"Removed dummy_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}