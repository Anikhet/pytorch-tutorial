{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Working with Real Data\n",
    "\n",
    "In the real world, data doesn't come in nice pre-packaged tensors. It comes in CSV files, image folders, text documents, and databases. This notebook teaches you how to handle real-world data in PyTorch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand `Dataset` and `DataLoader` classes\n",
    "- Create custom datasets for CSV and image data\n",
    "- Use DataLoaders for batching and shuffling\n",
    "- Apply data transformations and augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## The PyTorch Data Pipeline\n",
    "\n",
    "The standard pipeline for handling data in PyTorch involves two main classes:\n",
    "\n",
    "1. **`torch.utils.data.Dataset`**: Stores the samples and their corresponding labels.\n",
    "2. **`torch.utils.data.DataLoader`**: Wraps an iterable around the Dataset to enable easy access to the samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset from CSV\n",
    "\n",
    "Let's simulate a common scenario: you have a CSV file with features and labels.\n",
    "\n",
    "First, we'll create a dummy CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy CSV file\n",
    "data = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'feature3': np.random.rand(100),\n",
    "    'label': np.random.randint(0, 2, 100)  # Binary classification\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('dummy_data.csv', index=False)\n",
    "print(\"Created dummy_data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now, let's create a custom Dataset class to read this CSV.\n\nA custom Dataset must implement three functions:\n- `__init__`: Initialize data, download, etc.\n- `__len__`: Return the size of the dataset\n- `__getitem__`: Return one sample (feature, label) at the given index\n\n### Design Decisions in `__getitem__`\n\n**Lazy loading vs eager loading**: In `__init__`, you can either load all data into memory (eager) or store only file paths and load on-demand (lazy).\n- **Eager**: `self.data = pd.read_csv(file)` â€” fast access, high memory usage. Good for datasets that fit in RAM.\n- **Lazy**: `self.file = file` and read one row in `__getitem__` â€” slow access, low memory. Good for huge datasets or images.\n\n**Transform placement**: Apply transforms in `__getitem__`, not `__init__`. This way, data augmentation (random flips, crops) gives a different result each epoch, effectively increasing your dataset size.\n\n**Return types**: Always return tensors (not numpy arrays). PyTorch's DataLoader expects tensors for automatic batching and GPU transfer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features (columns 0-2)\n",
    "        features = self.data.iloc[idx, 0:3].values.astype(np.float32)\n",
    "        # Get label (column 3)\n",
    "        label = self.data.iloc[idx, 3]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        features = torch.tensor(features)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return features, label\n",
    "\n",
    "# Test the dataset\n",
    "dataset = CSVDataset('dummy_data.csv')\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "features, label = dataset[0]\n",
    "print(f\"First sample features: {features}\")\n",
    "print(f\"First sample label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using DataLoader\n",
    "\n",
    "The `DataLoader` handles batching, shuffling, and parallel data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "print(\"Iterating through DataLoader:\")\n",
    "for i, (features, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Features shape: {features.shape}\")\n",
    "    print(f\"  Labels: {labels}\")\n",
    "    if i == 2:  # Stop after 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Image Data and Transforms\n\nFor images, we often use `torchvision.transforms` for preprocessing and augmentation.\n\nCommon transforms:\n- `Resize`: Change image size\n- `ToTensor`: Convert image to tensor (scales to 0-1, moves channels first)\n- `Normalize`: Normalize with mean and std\n- `RandomHorizontalFlip`: Data augmentation\n\n### Why Normalize?\n\nNeural networks train faster when input features are on a similar scale. Raw pixel values (0-255) create large activations and gradients. Normalization centers data around 0 with unit variance.\n\nThe standard ImageNet normalization values (`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`) are used everywhere because most pretrained models were trained with these values. Using different normalization would break transfer learning.\n\n### Data Augmentation â€” Why It Works\n\nAugmentation artificially increases dataset size by applying random transformations:\n- **RandomHorizontalFlip**: A cat flipped horizontally is still a cat\n- **RandomCrop**: Forces the model to recognize objects anywhere in the frame\n- **ColorJitter**: Makes the model robust to lighting changes\n- **RandAugment**: Applies a random subset of augmentations (modern approach)\n\n**The key insight**: Augmentation acts as a regularizer. It tells the model \"these variations don't change the label,\" preventing overfitting to exact pixel patterns.\n\n### Train vs Test Transforms\n\nAlways use **different transforms** for training and evaluation:\n```python\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),    # Random augmentation\n    transforms.RandomHorizontalFlip(),    # Random augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),               # Deterministic\n    transforms.CenterCrop(224),           # Deterministic\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n```\nUsing random augmentations during evaluation would make results non-reproducible."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Augmentation\n",
    "    transforms.ToTensor(),  # Converts [0, 255] to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "print(\"Transforms defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ImageFolder\n",
    "\n",
    "If your images are organized in folders by class (e.g., `data/cats/`, `data/dogs/`), you can use `torchvision.datasets.ImageFolder`.\n",
    "\n",
    "```python\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# dataset = ImageFolder(root='path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "This automatically assigns labels based on folder names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways (Part 1)\n\n1. **Dataset Class**: Implement `__len__` and `__getitem__` to handle your custom data. Choose eager vs lazy loading based on dataset size.\n2. **DataLoader**: Use this for efficient batching and shuffling. Set `num_workers > 0` for parallel loading and `pin_memory=True` for GPU training.\n3. **Transforms**: Essential for image preprocessing and augmentation. Always use different transforms for training (random augmentations) and evaluation (deterministic).\n4. **ImageFolder**: The easiest way to load classification datasets organized by folder.\n5. **Normalization**: Standardize inputs so the network trains faster. Use ImageNet stats for transfer learning.\n\n---\n\n# PART 2: HANDLING IMBALANCED DATA (CRITICAL for Real-World ML)\n\nThis is one of the most common problems in production ML!"
  },
  {
   "cell_type": "markdown",
   "id": "j2uupdgndcm",
   "source": "## 5. The Imbalanced Data Problem\n\n### What is Class Imbalance?\nWhen classes have very different numbers of samples:\n- **Fraud detection**: 99.9% normal, 0.1% fraud\n- **Medical diagnosis**: 95% healthy, 5% disease\n- **Click prediction**: 98% no-click, 2% click\n\n### Why It's a Problem\n- Model learns to predict majority class always\n- **Example**: 99% accuracy by always predicting \"normal\" (useless!)\n- Minority class (the important one!) gets ignored\n\n### The Four Strategies to Handle Imbalanced Data\n\n**1. Class Weights (Most Common)**\nPenalize mistakes on the minority class more heavily in the loss function:\n```python\n# Inverse frequency weighting\nweights = 1.0 / class_counts\nloss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n```\nThis tells the model: \"Getting a fraud case wrong costs 1000x more than getting a normal case wrong.\"\n\n**2. Oversampling (SMOTE)**\nDuplicate or synthetically generate minority class samples. SMOTE creates new samples by interpolating between existing minority samples.\n- Pro: No information loss\n- Con: Can overfit to minority class patterns\n\n**3. Undersampling**\nRemove majority class samples to balance the dataset.\n- Pro: Faster training\n- Con: Throws away potentially useful data\n\n**4. Focal Loss**\nAutomatically focuses training on hard examples by down-weighting easy (confident) predictions:\n```\nFL(p) = -Î±(1-p)^Î³ Â· log(p)\n```\nWhen `Î³ > 0`, well-classified examples (high p) contribute less to the loss. Originally developed for object detection (RetinaNet), now widely used for any imbalanced problem.\n\n### Which Metric to Use?\n\n| Metric | When to Use | Why |\n|--------|------------|-----|\n| **Accuracy** | Balanced classes only | Misleading for imbalanced data |\n| **Precision** | When false positives are costly (spam filter) | \"Of everything flagged as fraud, how many were real fraud?\" |\n| **Recall** | When false negatives are costly (cancer detection) | \"Of all actual cancers, how many did we catch?\" |\n| **F1-Score** | When you need a balance | Harmonic mean of precision and recall |\n| **AUC-ROC** | Overall model quality | Threshold-independent; measures ranking ability |\n| **PR-AUC** | Severely imbalanced data | More informative than AUC-ROC when positive class is rare |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4fop6uwy5rk",
   "source": "# Example: Training with proper metrics for imbalanced data\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Simulate predictions from a model\ntorch.manual_seed(42)\nmodel_simple = nn.Sequential(\n    nn.Linear(10, 32),\n    nn.ReLU(),\n    nn.Linear(32, 2)\n)\n\n# Get predictions\nwith torch.no_grad():\n    logits = model_simple(features)\n    probs = F.softmax(logits, dim=1)\n    predictions = logits.argmax(dim=1)\n\n# Calculate metrics\nprint(\"=\" * 60)\nprint(\"IMBALANCED DATA METRICS (What FAANG Interviewers Expect)\")\nprint(\"=\" * 60)\n\n# Confusion Matrix\ncm = confusion_matrix(labels.numpy(), predictions.numpy())\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\nprint(f\"  True Negatives:  {cm[0,0]}\")\nprint(f\"  False Positives: {cm[0,1]}\")\nprint(f\"  False Negatives: {cm[1,0]}\")\nprint(f\"  True Positives:  {cm[1,1]}\")\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(labels.numpy(), predictions.numpy(), \n                          target_names=['Class 0 (Majority)', 'Class 1 (Minority)']))\n\n# Calculate metrics manually for understanding\ntp = cm[1, 1]  # True Positives\nfp = cm[0, 1]  # False Positives\nfn = cm[1, 0]  # False Negatives\n\nprecision = tp / (tp + fp) if (tp + fp) > 0 else 0\nrecall = tp / (tp + fn) if (tp + fn) > 0 else 0\nf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\nprint(f\"\\nðŸ“Š Key Metrics for Class 1 (Minority):\")\nprint(f\"  Precision: {precision:.3f} (Of predicted frauds, how many were real?)\")\nprint(f\"  Recall:    {recall:.3f} (Of actual frauds, how many did we catch?)\")\nprint(f\"  F1-Score:  {f1:.3f} (Harmonic mean of precision and recall)\")\n\n# ROC-AUC (requires probabilities)\ntry:\n    auc = roc_auc_score(labels.numpy(), probs[:, 1].numpy())\n    print(f\"  AUC-ROC:   {auc:.3f} (Overall discrimination ability)\")\nexcept:\n    print(\"  AUC-ROC:   N/A (need more diverse predictions)\")\n\nprint(\"\\nâœ… These are the metrics that matter for imbalanced data!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('dummy_data.csv'):\n",
    "    os.remove('dummy_data.csv')\n",
    "    print(\"Removed dummy_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}