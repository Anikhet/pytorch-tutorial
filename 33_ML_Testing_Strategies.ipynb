{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Testing Strategies\n",
    "\n",
    "This notebook covers comprehensive testing strategies for ML systems - critical for FAANG-level ML engineering.\n",
    "\n",
    "## Topics Covered\n",
    "1. **Unit Testing ML Components** - Testing transformers, models, and utilities\n",
    "2. **Integration Testing** - End-to-end pipeline testing\n",
    "3. **Model Validation** - Performance and regression testing\n",
    "4. **Data Quality Testing** - Schema validation and data contracts\n",
    "5. **A/B Testing** - Statistical significance and experiment design\n",
    "6. **Load & Chaos Testing** - Performance and resilience testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Callable, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import logging\n",
    "from scipy import stats\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch, MagicMock\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unit Testing ML Components\n",
    "\n",
    "Testing individual components: transformers, feature engineering, and model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ML components to test\n",
    "\n",
    "class FeatureTransformer:\n",
    "    \"\"\"Feature transformation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, normalize: bool = True, fill_na: float = 0.0):\n",
    "        self.normalize = normalize\n",
    "        self.fill_na = fill_na\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: np.ndarray) -> 'FeatureTransformer':\n",
    "        \"\"\"Fit the transformer on training data\"\"\"\n",
    "        # Handle NaN values\n",
    "        X_clean = np.nan_to_num(X, nan=self.fill_na)\n",
    "        \n",
    "        if self.normalize:\n",
    "            self.mean_ = np.mean(X_clean, axis=0)\n",
    "            self.std_ = np.std(X_clean, axis=0)\n",
    "            # Prevent division by zero\n",
    "            self.std_[self.std_ == 0] = 1.0\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform data using fitted parameters\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformer must be fitted before transform\")\n",
    "        \n",
    "        X_clean = np.nan_to_num(X, nan=self.fill_na)\n",
    "        \n",
    "        if self.normalize:\n",
    "            return (X_clean - self.mean_) / self.std_\n",
    "        return X_clean\n",
    "    \n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Simple neural network classifier for testing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFeatureTransformer(unittest.TestCase):\n",
    "    \"\"\"Unit tests for FeatureTransformer\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures\"\"\"\n",
    "        self.transformer = FeatureTransformer(normalize=True)\n",
    "        self.X_train = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        self.X_test = np.array([[2.0, 3.0], [4.0, 5.0]])\n",
    "    \n",
    "    def test_fit_sets_parameters(self):\n",
    "        \"\"\"Test that fit() computes mean and std correctly\"\"\"\n",
    "        self.transformer.fit(self.X_train)\n",
    "        \n",
    "        expected_mean = np.mean(self.X_train, axis=0)\n",
    "        expected_std = np.std(self.X_train, axis=0)\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(self.transformer.mean_, expected_mean)\n",
    "        np.testing.assert_array_almost_equal(self.transformer.std_, expected_std)\n",
    "        self.assertTrue(self.transformer.is_fitted)\n",
    "    \n",
    "    def test_transform_normalizes_data(self):\n",
    "        \"\"\"Test that transform() normalizes data correctly\"\"\"\n",
    "        self.transformer.fit(self.X_train)\n",
    "        X_transformed = self.transformer.transform(self.X_train)\n",
    "        \n",
    "        # After normalization, mean should be ~0, std should be ~1\n",
    "        np.testing.assert_array_almost_equal(\n",
    "            np.mean(X_transformed, axis=0), \n",
    "            np.zeros(2), \n",
    "            decimal=5\n",
    "        )\n",
    "    \n",
    "    def test_transform_before_fit_raises_error(self):\n",
    "        \"\"\"Test that transform without fit raises ValueError\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.transformer.transform(self.X_train)\n",
    "    \n",
    "    def test_handles_nan_values(self):\n",
    "        \"\"\"Test NaN handling\"\"\"\n",
    "        X_with_nan = np.array([[1.0, np.nan], [3.0, 4.0], [5.0, 6.0]])\n",
    "        transformer = FeatureTransformer(normalize=True, fill_na=-999)\n",
    "        \n",
    "        # Should not raise\n",
    "        X_transformed = transformer.fit_transform(X_with_nan)\n",
    "        \n",
    "        # No NaN in output\n",
    "        self.assertFalse(np.any(np.isnan(X_transformed)))\n",
    "    \n",
    "    def test_zero_std_handling(self):\n",
    "        \"\"\"Test that zero std columns are handled\"\"\"\n",
    "        X_constant = np.array([[1.0, 5.0], [1.0, 5.0], [1.0, 5.0]])\n",
    "        self.transformer.fit(X_constant)\n",
    "        \n",
    "        # Should not raise division by zero\n",
    "        X_transformed = self.transformer.transform(X_constant)\n",
    "        self.assertFalse(np.any(np.isinf(X_transformed)))\n",
    "\n",
    "\n",
    "class TestSimpleClassifier(unittest.TestCase):\n",
    "    \"\"\"Unit tests for neural network model\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.model = SimpleClassifier(input_dim=10, hidden_dim=32, num_classes=3)\n",
    "        self.batch_size = 4\n",
    "    \n",
    "    def test_output_shape(self):\n",
    "        \"\"\"Test that output has correct shape\"\"\"\n",
    "        x = torch.randn(self.batch_size, 10)\n",
    "        output = self.model(x)\n",
    "        \n",
    "        self.assertEqual(output.shape, (self.batch_size, 3))\n",
    "    \n",
    "    def test_forward_with_single_sample(self):\n",
    "        \"\"\"Test forward pass with single sample\"\"\"\n",
    "        x = torch.randn(1, 10)\n",
    "        output = self.model(x)\n",
    "        \n",
    "        self.assertEqual(output.shape, (1, 3))\n",
    "    \n",
    "    def test_gradient_flow(self):\n",
    "        \"\"\"Test that gradients flow through all layers\"\"\"\n",
    "        x = torch.randn(self.batch_size, 10, requires_grad=True)\n",
    "        output = self.model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check all parameters have gradients\n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.assertIsNotNone(param.grad, f\"No gradient for {name}\")\n",
    "            self.assertFalse(\n",
    "                torch.all(param.grad == 0), \n",
    "                f\"Zero gradient for {name}\"\n",
    "            )\n",
    "    \n",
    "    def test_deterministic_output(self):\n",
    "        \"\"\"Test that model produces deterministic output in mode\"\"\"\n",
    "        self.model.train(False)\n",
    "        x = torch.randn(self.batch_size, 10)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output1 = self.model(x)\n",
    "            output2 = self.model(x)\n",
    "        \n",
    "        torch.testing.assert_close(output1, output2)\n",
    "\n",
    "\n",
    "# Run tests\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestFeatureTransformer)\n",
    "suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestSimpleClassifier))\n",
    "\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(f\"\\nTests run: {result.testsRun}\")\n",
    "print(f\"Failures: {len(result.failures)}\")\n",
    "print(f\"Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integration Testing\n",
    "\n",
    "Testing complete ML pipelines end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for ML pipeline\"\"\"\n",
    "    input_dim: int\n",
    "    hidden_dim: int\n",
    "    num_classes: int\n",
    "    normalize: bool = True\n",
    "    batch_size: int = 32\n",
    "\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"\n",
    "    Complete ML pipeline for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.transformer = FeatureTransformer(normalize=config.normalize)\n",
    "        self.model = SimpleClassifier(\n",
    "            config.input_dim, \n",
    "            config.hidden_dim, \n",
    "            config.num_classes\n",
    "        )\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def preprocess(self, X: np.ndarray, fit: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Preprocess input data\"\"\"\n",
    "        if fit:\n",
    "            X_transformed = self.transformer.fit_transform(X)\n",
    "        else:\n",
    "            X_transformed = self.transformer.transform(X)\n",
    "        return torch.tensor(X_transformed, dtype=torch.float32)\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 10) -> Dict[str, List[float]]:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        X_tensor = self.preprocess(X, fit=True)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        \n",
    "        history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (predictions == y_tensor).float().mean().item()\n",
    "            \n",
    "            history['loss'].append(loss.item())\n",
    "            history['accuracy'].append(accuracy)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        self.model.train(False)\n",
    "        X_tensor = self.preprocess(X, fit=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        return predictions.numpy()\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        self.model.train(False)\n",
    "        X_tensor = self.preprocess(X, fit=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        return probabilities.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMLPipelineIntegration(unittest.TestCase):\n",
    "    \"\"\"Integration tests for ML pipeline\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up test data once for all tests\"\"\"\n",
    "        np.random.seed(42)\n",
    "        cls.n_samples = 100\n",
    "        cls.n_features = 10\n",
    "        cls.n_classes = 3\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        cls.X = np.random.randn(cls.n_samples, cls.n_features)\n",
    "        cls.y = np.random.randint(0, cls.n_classes, cls.n_samples)\n",
    "        \n",
    "        cls.config = PipelineConfig(\n",
    "            input_dim=cls.n_features,\n",
    "            hidden_dim=32,\n",
    "            num_classes=cls.n_classes\n",
    "        )\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up fresh pipeline for each test\"\"\"\n",
    "        self.pipeline = MLPipeline(self.config)\n",
    "    \n",
    "    def test_end_to_end_training(self):\n",
    "        \"\"\"Test complete training workflow\"\"\"\n",
    "        history = self.pipeline.train(self.X, self.y, epochs=5)\n",
    "        \n",
    "        # Check training completed\n",
    "        self.assertTrue(self.pipeline.is_trained)\n",
    "        self.assertEqual(len(history['loss']), 5)\n",
    "        self.assertEqual(len(history['accuracy']), 5)\n",
    "        \n",
    "        # Loss should generally decrease\n",
    "        self.assertLess(history['loss'][-1], history['loss'][0] * 2)\n",
    "    \n",
    "    def test_prediction_after_training(self):\n",
    "        \"\"\"Test that predictions work after training\"\"\"\n",
    "        self.pipeline.train(self.X, self.y, epochs=5)\n",
    "        \n",
    "        predictions = self.pipeline.predict(self.X)\n",
    "        \n",
    "        self.assertEqual(len(predictions), self.n_samples)\n",
    "        self.assertTrue(all(0 <= p < self.n_classes for p in predictions))\n",
    "    \n",
    "    def test_probability_predictions_sum_to_one(self):\n",
    "        \"\"\"Test that probabilities sum to 1\"\"\"\n",
    "        self.pipeline.train(self.X, self.y, epochs=5)\n",
    "        \n",
    "        probabilities = self.pipeline.predict_proba(self.X)\n",
    "        \n",
    "        self.assertEqual(probabilities.shape, (self.n_samples, self.n_classes))\n",
    "        np.testing.assert_array_almost_equal(\n",
    "            probabilities.sum(axis=1), \n",
    "            np.ones(self.n_samples)\n",
    "        )\n",
    "    \n",
    "    def test_prediction_before_training_fails(self):\n",
    "        \"\"\"Test that prediction before training raises error\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.pipeline.predict(self.X)\n",
    "    \n",
    "    def test_new_data_prediction(self):\n",
    "        \"\"\"Test prediction on new unseen data\"\"\"\n",
    "        self.pipeline.train(self.X, self.y, epochs=5)\n",
    "        \n",
    "        # New data with same features\n",
    "        X_new = np.random.randn(10, self.n_features)\n",
    "        predictions = self.pipeline.predict(X_new)\n",
    "        \n",
    "        self.assertEqual(len(predictions), 10)\n",
    "\n",
    "\n",
    "# Run integration tests\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMLPipelineIntegration)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(f\"\\nIntegration Tests - Run: {result.testsRun}, Failures: {len(result.failures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Validation & Regression Testing\n",
    "\n",
    "Ensuring model quality and detecting performance regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Metrics for model validation\"\"\"\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    latency_p50_ms: float\n",
    "    latency_p99_ms: float\n",
    "    memory_mb: float\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "\n",
    "class ModelValidator:\n",
    "    \"\"\"\n",
    "    Validates ML models against quality thresholds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_accuracy: float = 0.8,\n",
    "        min_precision: float = 0.75,\n",
    "        min_recall: float = 0.75,\n",
    "        max_latency_p99_ms: float = 100,\n",
    "        max_memory_mb: float = 1000\n",
    "    ):\n",
    "        self.thresholds = {\n",
    "            'accuracy': min_accuracy,\n",
    "            'precision': min_precision,\n",
    "            'recall': min_recall,\n",
    "            'latency_p99_ms': max_latency_p99_ms,\n",
    "            'memory_mb': max_memory_mb\n",
    "        }\n",
    "    \n",
    "    def compute_metrics(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        latencies: List[float],\n",
    "        memory_mb: float\n",
    "    ) -> ModelMetrics:\n",
    "        \"\"\"Compute all model metrics\"\"\"\n",
    "        # Basic metrics\n",
    "        accuracy = np.mean(y_true == y_pred)\n",
    "        \n",
    "        # Per-class metrics (simplified for multi-class)\n",
    "        unique_classes = np.unique(y_true)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "            fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "            fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall + 1e-8)\n",
    "        \n",
    "        return ModelMetrics(\n",
    "            accuracy=accuracy,\n",
    "            precision=avg_precision,\n",
    "            recall=avg_recall,\n",
    "            f1_score=f1,\n",
    "            latency_p50_ms=np.percentile(latencies, 50),\n",
    "            latency_p99_ms=np.percentile(latencies, 99),\n",
    "            memory_mb=memory_mb\n",
    "        )\n",
    "    \n",
    "    def validate(self, metrics: ModelMetrics) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate metrics against thresholds\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        if metrics.accuracy < self.thresholds['accuracy']:\n",
    "            violations.append(\n",
    "                f\"Accuracy {metrics.accuracy:.3f} < {self.thresholds['accuracy']}\"\n",
    "            )\n",
    "        \n",
    "        if metrics.precision < self.thresholds['precision']:\n",
    "            violations.append(\n",
    "                f\"Precision {metrics.precision:.3f} < {self.thresholds['precision']}\"\n",
    "            )\n",
    "        \n",
    "        if metrics.recall < self.thresholds['recall']:\n",
    "            violations.append(\n",
    "                f\"Recall {metrics.recall:.3f} < {self.thresholds['recall']}\"\n",
    "            )\n",
    "        \n",
    "        if metrics.latency_p99_ms > self.thresholds['latency_p99_ms']:\n",
    "            violations.append(\n",
    "                f\"Latency P99 {metrics.latency_p99_ms:.1f}ms > {self.thresholds['latency_p99_ms']}ms\"\n",
    "            )\n",
    "        \n",
    "        if metrics.memory_mb > self.thresholds['memory_mb']:\n",
    "            violations.append(\n",
    "                f\"Memory {metrics.memory_mb:.1f}MB > {self.thresholds['memory_mb']}MB\"\n",
    "            )\n",
    "        \n",
    "        return len(violations) == 0, violations\n",
    "\n",
    "\n",
    "class RegressionTester:\n",
    "    \"\"\"\n",
    "    Tests for model regressions compared to baseline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, allowed_regression_pct: float = 5.0):\n",
    "        self.allowed_regression_pct = allowed_regression_pct\n",
    "        self.baseline_metrics: Optional[ModelMetrics] = None\n",
    "    \n",
    "    def set_baseline(self, metrics: ModelMetrics) -> None:\n",
    "        \"\"\"Set baseline metrics for comparison\"\"\"\n",
    "        self.baseline_metrics = metrics\n",
    "    \n",
    "    def check_regression(\n",
    "        self,\n",
    "        current_metrics: ModelMetrics\n",
    "    ) -> Tuple[bool, Dict[str, Any]]:\n",
    "        \"\"\"Check for regressions against baseline\"\"\"\n",
    "        if self.baseline_metrics is None:\n",
    "            return True, {\"message\": \"No baseline set\"}\n",
    "        \n",
    "        results = {\n",
    "            \"regressions\": [],\n",
    "            \"improvements\": [],\n",
    "            \"details\": {}\n",
    "        }\n",
    "        \n",
    "        # Check accuracy (higher is better)\n",
    "        acc_change_pct = (\n",
    "            (current_metrics.accuracy - self.baseline_metrics.accuracy) / \n",
    "            self.baseline_metrics.accuracy * 100\n",
    "        )\n",
    "        results[\"details\"][\"accuracy_change_pct\"] = acc_change_pct\n",
    "        \n",
    "        if acc_change_pct < -self.allowed_regression_pct:\n",
    "            results[\"regressions\"].append(\n",
    "                f\"Accuracy regressed by {abs(acc_change_pct):.1f}%\"\n",
    "            )\n",
    "        elif acc_change_pct > self.allowed_regression_pct:\n",
    "            results[\"improvements\"].append(\n",
    "                f\"Accuracy improved by {acc_change_pct:.1f}%\"\n",
    "            )\n",
    "        \n",
    "        # Check latency (lower is better)\n",
    "        latency_change_pct = (\n",
    "            (current_metrics.latency_p99_ms - self.baseline_metrics.latency_p99_ms) /\n",
    "            self.baseline_metrics.latency_p99_ms * 100\n",
    "        )\n",
    "        results[\"details\"][\"latency_change_pct\"] = latency_change_pct\n",
    "        \n",
    "        if latency_change_pct > self.allowed_regression_pct:\n",
    "            results[\"regressions\"].append(\n",
    "                f\"Latency regressed by {latency_change_pct:.1f}%\"\n",
    "            )\n",
    "        elif latency_change_pct < -self.allowed_regression_pct:\n",
    "            results[\"improvements\"].append(\n",
    "                f\"Latency improved by {abs(latency_change_pct):.1f}%\"\n",
    "            )\n",
    "        \n",
    "        has_regression = len(results[\"regressions\"]) > 0\n",
    "        return not has_regression, results\n",
    "\n",
    "\n",
    "# Example: Model Validation\n",
    "validator = ModelValidator(min_accuracy=0.85, min_precision=0.80)\n",
    "\n",
    "# Simulate metrics\n",
    "y_true = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\n",
    "y_pred = np.array([0, 1, 2, 0, 1, 1, 0, 1, 2, 0])  # One error\n",
    "latencies = np.random.exponential(10, 100).tolist()  # ms\n",
    "\n",
    "metrics = validator.compute_metrics(y_true, y_pred, latencies, memory_mb=256)\n",
    "passed, violations = validator.validate(metrics)\n",
    "\n",
    "print(f\"Model Metrics:\")\n",
    "print(f\"  Accuracy: {metrics.accuracy:.3f}\")\n",
    "print(f\"  Precision: {metrics.precision:.3f}\")\n",
    "print(f\"  Recall: {metrics.recall:.3f}\")\n",
    "print(f\"  F1 Score: {metrics.f1_score:.3f}\")\n",
    "print(f\"  Latency P99: {metrics.latency_p99_ms:.1f}ms\")\n",
    "print(f\"\\nValidation: {'PASSED' if passed else 'FAILED'}\")\n",
    "if violations:\n",
    "    print(f\"Violations: {violations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Testing\n",
    "\n",
    "Testing data quality and schema validation (inspired by Great Expectations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ColumnExpectation:\n",
    "    \"\"\"Expectation for a single column\"\"\"\n",
    "    column: str\n",
    "    expectation_type: str\n",
    "    kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class DataExpectationSuite:\n",
    "    \"\"\"\n",
    "    Suite of data quality expectations (Great Expectations style).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.expectations: List[ColumnExpectation] = []\n",
    "    \n",
    "    def expect_column_to_exist(self, column: str) -> 'DataExpectationSuite':\n",
    "        \"\"\"Expect column to exist in dataset\"\"\"\n",
    "        self.expectations.append(ColumnExpectation(\n",
    "            column=column,\n",
    "            expectation_type=\"column_exists\"\n",
    "        ))\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_to_not_be_null(self, column: str) -> 'DataExpectationSuite':\n",
    "        \"\"\"Expect no null values in column\"\"\"\n",
    "        self.expectations.append(ColumnExpectation(\n",
    "            column=column,\n",
    "            expectation_type=\"no_nulls\"\n",
    "        ))\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_in_range(\n",
    "        self,\n",
    "        column: str,\n",
    "        min_value: float,\n",
    "        max_value: float\n",
    "    ) -> 'DataExpectationSuite':\n",
    "        \"\"\"Expect values to be within range\"\"\"\n",
    "        self.expectations.append(ColumnExpectation(\n",
    "            column=column,\n",
    "            expectation_type=\"in_range\",\n",
    "            kwargs={\"min\": min_value, \"max\": max_value}\n",
    "        ))\n",
    "        return self\n",
    "    \n",
    "    def expect_column_values_in_set(\n",
    "        self,\n",
    "        column: str,\n",
    "        value_set: List[Any]\n",
    "    ) -> 'DataExpectationSuite':\n",
    "        \"\"\"Expect values to be in predefined set\"\"\"\n",
    "        self.expectations.append(ColumnExpectation(\n",
    "            column=column,\n",
    "            expectation_type=\"in_set\",\n",
    "            kwargs={\"values\": value_set}\n",
    "        ))\n",
    "        return self\n",
    "    \n",
    "    def expect_column_mean_in_range(\n",
    "        self,\n",
    "        column: str,\n",
    "        min_value: float,\n",
    "        max_value: float\n",
    "    ) -> 'DataExpectationSuite':\n",
    "        \"\"\"Expect column mean to be within range\"\"\"\n",
    "        self.expectations.append(ColumnExpectation(\n",
    "            column=column,\n",
    "            expectation_type=\"mean_in_range\",\n",
    "            kwargs={\"min\": min_value, \"max\": max_value}\n",
    "        ))\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Validates data against expectation suite.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suite: DataExpectationSuite):\n",
    "        self.suite = suite\n",
    "    \n",
    "    def validate(self, data: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate data against all expectations\"\"\"\n",
    "        results = {\n",
    "            \"success\": True,\n",
    "            \"expectations\": [],\n",
    "            \"failed_count\": 0,\n",
    "            \"passed_count\": 0\n",
    "        }\n",
    "        \n",
    "        for expectation in self.suite.expectations:\n",
    "            result = self._check_expectation(expectation, data)\n",
    "            results[\"expectations\"].append(result)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                results[\"passed_count\"] += 1\n",
    "            else:\n",
    "                results[\"failed_count\"] += 1\n",
    "                results[\"success\"] = False\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _check_expectation(\n",
    "        self,\n",
    "        exp: ColumnExpectation,\n",
    "        data: Dict[str, np.ndarray]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Check a single expectation\"\"\"\n",
    "        result = {\n",
    "            \"expectation_type\": exp.expectation_type,\n",
    "            \"column\": exp.column,\n",
    "            \"success\": False,\n",
    "            \"details\": {}\n",
    "        }\n",
    "        \n",
    "        if exp.expectation_type == \"column_exists\":\n",
    "            result[\"success\"] = exp.column in data\n",
    "            result[\"details\"][\"found\"] = exp.column in data\n",
    "        \n",
    "        elif exp.expectation_type == \"no_nulls\":\n",
    "            if exp.column in data:\n",
    "                null_count = np.sum(np.isnan(data[exp.column]))\n",
    "                result[\"success\"] = null_count == 0\n",
    "                result[\"details\"][\"null_count\"] = int(null_count)\n",
    "        \n",
    "        elif exp.expectation_type == \"in_range\":\n",
    "            if exp.column in data:\n",
    "                values = data[exp.column]\n",
    "                min_val, max_val = exp.kwargs[\"min\"], exp.kwargs[\"max\"]\n",
    "                in_range = np.all((values >= min_val) & (values <= max_val))\n",
    "                result[\"success\"] = bool(in_range)\n",
    "                result[\"details\"][\"min_observed\"] = float(np.min(values))\n",
    "                result[\"details\"][\"max_observed\"] = float(np.max(values))\n",
    "        \n",
    "        elif exp.expectation_type == \"in_set\":\n",
    "            if exp.column in data:\n",
    "                values = data[exp.column]\n",
    "                valid_set = set(exp.kwargs[\"values\"])\n",
    "                in_set = all(v in valid_set for v in values)\n",
    "                result[\"success\"] = in_set\n",
    "                result[\"details\"][\"unexpected_values\"] = list(\n",
    "                    set(values) - valid_set\n",
    "                )[:5]\n",
    "        \n",
    "        elif exp.expectation_type == \"mean_in_range\":\n",
    "            if exp.column in data:\n",
    "                mean = np.mean(data[exp.column])\n",
    "                min_val, max_val = exp.kwargs[\"min\"], exp.kwargs[\"max\"]\n",
    "                result[\"success\"] = min_val <= mean <= max_val\n",
    "                result[\"details\"][\"observed_mean\"] = float(mean)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Example: Data Quality Testing\n",
    "suite = DataExpectationSuite(\"training_data\")\n",
    "suite.expect_column_to_exist(\"feature_1\")\n",
    "suite.expect_column_to_exist(\"feature_2\")\n",
    "suite.expect_column_values_to_not_be_null(\"feature_1\")\n",
    "suite.expect_column_values_in_range(\"feature_1\", -10, 10)\n",
    "suite.expect_column_mean_in_range(\"feature_1\", -1, 1)\n",
    "suite.expect_column_values_in_set(\"label\", [0, 1, 2])\n",
    "\n",
    "# Test data\n",
    "test_data = {\n",
    "    \"feature_1\": np.random.randn(100),\n",
    "    \"feature_2\": np.random.randn(100),\n",
    "    \"label\": np.random.randint(0, 3, 100)\n",
    "}\n",
    "\n",
    "validator = DataValidator(suite)\n",
    "results = validator.validate(test_data)\n",
    "\n",
    "print(f\"Data Validation Results:\")\n",
    "print(f\"  Overall Success: {results['success']}\")\n",
    "print(f\"  Passed: {results['passed_count']}, Failed: {results['failed_count']}\")\n",
    "print(\"\\nExpectation Details:\")\n",
    "for exp_result in results['expectations']:\n",
    "    status = \"PASS\" if exp_result['success'] else \"FAIL\"\n",
    "    print(f\"  [{status}] {exp_result['column']}: {exp_result['expectation_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A/B Testing\n",
    "\n",
    "Statistical testing for ML experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentVariant:\n",
    "    \"\"\"A/B test variant\"\"\"\n",
    "    name: str\n",
    "    conversions: int\n",
    "    total: int\n",
    "    \n",
    "    @property\n",
    "    def conversion_rate(self) -> float:\n",
    "        return self.conversions / self.total if self.total > 0 else 0\n",
    "\n",
    "\n",
    "class ABTestAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes A/B test results with statistical rigor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.05,  # Significance level\n",
    "        power: float = 0.80,  # Statistical power\n",
    "        mde: float = 0.02    # Minimum detectable effect\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.power = power\n",
    "        self.mde = mde\n",
    "    \n",
    "    def calculate_sample_size(\n",
    "        self,\n",
    "        baseline_rate: float\n",
    "    ) -> int:\n",
    "        \"\"\"Calculate required sample size per variant\"\"\"\n",
    "        # Using normal approximation\n",
    "        z_alpha = stats.norm.ppf(1 - self.alpha / 2)\n",
    "        z_beta = stats.norm.ppf(self.power)\n",
    "        \n",
    "        p1 = baseline_rate\n",
    "        p2 = baseline_rate + self.mde\n",
    "        \n",
    "        pooled_p = (p1 + p2) / 2\n",
    "        \n",
    "        numerator = (z_alpha * np.sqrt(2 * pooled_p * (1 - pooled_p)) +\n",
    "                    z_beta * np.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2\n",
    "        denominator = (p2 - p1) ** 2\n",
    "        \n",
    "        return int(np.ceil(numerator / denominator))\n",
    "    \n",
    "    def z_test_proportions(\n",
    "        self,\n",
    "        control: ExperimentVariant,\n",
    "        treatment: ExperimentVariant\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Perform z-test for difference in proportions\"\"\"\n",
    "        p1 = control.conversion_rate\n",
    "        p2 = treatment.conversion_rate\n",
    "        n1 = control.total\n",
    "        n2 = treatment.total\n",
    "        \n",
    "        # Pooled proportion\n",
    "        p_pooled = (control.conversions + treatment.conversions) / (n1 + n2)\n",
    "        \n",
    "        # Standard error\n",
    "        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))\n",
    "        \n",
    "        # Z-statistic\n",
    "        z = (p2 - p1) / se if se > 0 else 0\n",
    "        \n",
    "        # Two-tailed p-value\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "        \n",
    "        # Confidence interval for difference\n",
    "        se_diff = np.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)\n",
    "        ci_lower = (p2 - p1) - 1.96 * se_diff\n",
    "        ci_upper = (p2 - p1) + 1.96 * se_diff\n",
    "        \n",
    "        return {\n",
    "            \"test\": \"z_test\",\n",
    "            \"z_statistic\": z,\n",
    "            \"p_value\": p_value,\n",
    "            \"significant\": p_value < self.alpha,\n",
    "            \"control_rate\": p1,\n",
    "            \"treatment_rate\": p2,\n",
    "            \"absolute_lift\": p2 - p1,\n",
    "            \"relative_lift\": (p2 - p1) / p1 if p1 > 0 else 0,\n",
    "            \"confidence_interval\": (ci_lower, ci_upper)\n",
    "        }\n",
    "\n",
    "\n",
    "class BayesianABTest:\n",
    "    \"\"\"\n",
    "    Bayesian A/B testing with Beta-Binomial model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prior_alpha: float = 1, prior_beta: float = 1):\n",
    "        self.prior_alpha = prior_alpha\n",
    "        self.prior_beta = prior_beta\n",
    "    \n",
    "    def analyze(\n",
    "        self,\n",
    "        control: ExperimentVariant,\n",
    "        treatment: ExperimentVariant,\n",
    "        n_samples: int = 10000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze using Bayesian inference\"\"\"\n",
    "        # Posterior distributions (Beta)\n",
    "        control_posterior = stats.beta(\n",
    "            self.prior_alpha + control.conversions,\n",
    "            self.prior_beta + control.total - control.conversions\n",
    "        )\n",
    "        \n",
    "        treatment_posterior = stats.beta(\n",
    "            self.prior_alpha + treatment.conversions,\n",
    "            self.prior_beta + treatment.total - treatment.conversions\n",
    "        )\n",
    "        \n",
    "        # Sample from posteriors\n",
    "        control_samples = control_posterior.rvs(n_samples)\n",
    "        treatment_samples = treatment_posterior.rvs(n_samples)\n",
    "        \n",
    "        # Probability that treatment > control\n",
    "        prob_treatment_better = np.mean(treatment_samples > control_samples)\n",
    "        \n",
    "        # Credible intervals (95%)\n",
    "        control_ci = control_posterior.ppf([0.025, 0.975])\n",
    "        treatment_ci = treatment_posterior.ppf([0.025, 0.975])\n",
    "        \n",
    "        # Expected lift\n",
    "        lift_samples = (treatment_samples - control_samples) / control_samples\n",
    "        expected_lift = np.mean(lift_samples)\n",
    "        lift_ci = np.percentile(lift_samples, [2.5, 97.5])\n",
    "        \n",
    "        return {\n",
    "            \"method\": \"bayesian\",\n",
    "            \"prob_treatment_better\": prob_treatment_better,\n",
    "            \"control_mean\": control_posterior.mean(),\n",
    "            \"control_ci\": tuple(control_ci),\n",
    "            \"treatment_mean\": treatment_posterior.mean(),\n",
    "            \"treatment_ci\": tuple(treatment_ci),\n",
    "            \"expected_lift\": expected_lift,\n",
    "            \"lift_ci\": tuple(lift_ci),\n",
    "            \"decision\": \"Treatment wins\" if prob_treatment_better > 0.95 else \"No clear winner\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: A/B Test Analysis\n",
    "control = ExperimentVariant(name=\"Model_v1\", conversions=500, total=5000)\n",
    "treatment = ExperimentVariant(name=\"Model_v2\", conversions=550, total=5000)\n",
    "\n",
    "# Frequentist analysis\n",
    "analyzer = ABTestAnalyzer(alpha=0.05)\n",
    "z_result = analyzer.z_test_proportions(control, treatment)\n",
    "\n",
    "print(\"Frequentist Analysis (Z-test):\")\n",
    "print(f\"  Control Rate: {z_result['control_rate']:.3f}\")\n",
    "print(f\"  Treatment Rate: {z_result['treatment_rate']:.3f}\")\n",
    "print(f\"  Relative Lift: {z_result['relative_lift']:.1%}\")\n",
    "print(f\"  P-value: {z_result['p_value']:.4f}\")\n",
    "print(f\"  Significant: {z_result['significant']}\")\n",
    "print(f\"  95% CI: ({z_result['confidence_interval'][0]:.4f}, {z_result['confidence_interval'][1]:.4f})\")\n",
    "\n",
    "# Bayesian analysis\n",
    "bayesian = BayesianABTest()\n",
    "bayes_result = bayesian.analyze(control, treatment)\n",
    "\n",
    "print(f\"\\nBayesian Analysis:\")\n",
    "print(f\"  P(Treatment > Control): {bayes_result['prob_treatment_better']:.1%}\")\n",
    "print(f\"  Expected Lift: {bayes_result['expected_lift']:.1%}\")\n",
    "print(f\"  Decision: {bayes_result['decision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load & Chaos Testing\n",
    "\n",
    "Testing ML service performance and resilience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoadTestResult:\n",
    "    \"\"\"Results from load test\"\"\"\n",
    "    total_requests: int\n",
    "    successful_requests: int\n",
    "    failed_requests: int\n",
    "    latency_p50_ms: float\n",
    "    latency_p95_ms: float\n",
    "    latency_p99_ms: float\n",
    "    throughput_rps: float\n",
    "    error_rate: float\n",
    "    duration_seconds: float\n",
    "\n",
    "\n",
    "class MLServiceLoadTester:\n",
    "    \"\"\"\n",
    "    Load testing for ML inference services.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        input_shape: Tuple[int, ...]\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.input_shape = input_shape\n",
    "        self.model.train(False)\n",
    "    \n",
    "    def _simulate_request(self) -> Tuple[bool, float]:\n",
    "        \"\"\"Simulate a single inference request\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                x = torch.randn(1, *self.input_shape)\n",
    "                _ = self.model(x)\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000  # ms\n",
    "            return True, latency\n",
    "        except Exception:\n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            return False, latency\n",
    "    \n",
    "    def run_load_test(\n",
    "        self,\n",
    "        num_requests: int = 1000,\n",
    "        target_rps: float = None\n",
    "    ) -> LoadTestResult:\n",
    "        \"\"\"Run load test\"\"\"\n",
    "        latencies = []\n",
    "        successes = 0\n",
    "        failures = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            success, latency = self._simulate_request()\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            if success:\n",
    "                successes += 1\n",
    "            else:\n",
    "                failures += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            if target_rps:\n",
    "                expected_time = (i + 1) / target_rps\n",
    "                elapsed = time.time() - start_time\n",
    "                if elapsed < expected_time:\n",
    "                    time.sleep(expected_time - elapsed)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        return LoadTestResult(\n",
    "            total_requests=num_requests,\n",
    "            successful_requests=successes,\n",
    "            failed_requests=failures,\n",
    "            latency_p50_ms=np.percentile(latencies, 50),\n",
    "            latency_p95_ms=np.percentile(latencies, 95),\n",
    "            latency_p99_ms=np.percentile(latencies, 99),\n",
    "            throughput_rps=num_requests / duration,\n",
    "            error_rate=failures / num_requests,\n",
    "            duration_seconds=duration\n",
    "        )\n",
    "\n",
    "\n",
    "class ChaosEngineer:\n",
    "    \"\"\"\n",
    "    Chaos engineering for ML systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments: List[Dict] = []\n",
    "    \n",
    "    def inject_latency(\n",
    "        self,\n",
    "        func: Callable,\n",
    "        latency_ms: float,\n",
    "        probability: float = 1.0\n",
    "    ) -> Callable:\n",
    "        \"\"\"Inject latency into function calls\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if np.random.random() < probability:\n",
    "                time.sleep(latency_ms / 1000)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    def inject_failure(\n",
    "        self,\n",
    "        func: Callable,\n",
    "        failure_rate: float = 0.1,\n",
    "        exception_type: type = RuntimeError\n",
    "    ) -> Callable:\n",
    "        \"\"\"Inject random failures\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if np.random.random() < failure_rate:\n",
    "                raise exception_type(\"Injected chaos failure\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    def run_experiment(\n",
    "        self,\n",
    "        name: str,\n",
    "        target_func: Callable,\n",
    "        chaos_func: Callable,\n",
    "        iterations: int = 100\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run chaos experiment\"\"\"\n",
    "        results = {\n",
    "            \"name\": name,\n",
    "            \"iterations\": iterations,\n",
    "            \"successes\": 0,\n",
    "            \"failures\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        # Wrap target with chaos\n",
    "        chaotic_func = chaos_func(target_func)\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            try:\n",
    "                chaotic_func()\n",
    "                results[\"successes\"] += 1\n",
    "            except Exception as e:\n",
    "                results[\"failures\"] += 1\n",
    "                results[\"errors\"].append(str(e))\n",
    "        \n",
    "        results[\"resilience_score\"] = results[\"successes\"] / iterations\n",
    "        self.experiments.append(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example: Load Testing\n",
    "model = SimpleClassifier(input_dim=10, hidden_dim=32, num_classes=3)\n",
    "load_tester = MLServiceLoadTester(model, input_shape=(10,))\n",
    "\n",
    "print(\"Running load test...\")\n",
    "result = load_tester.run_load_test(num_requests=500)\n",
    "\n",
    "print(f\"\\nLoad Test Results:\")\n",
    "print(f\"  Total Requests: {result.total_requests}\")\n",
    "print(f\"  Success Rate: {(1 - result.error_rate):.1%}\")\n",
    "print(f\"  Throughput: {result.throughput_rps:.1f} RPS\")\n",
    "print(f\"  Latency P50: {result.latency_p50_ms:.2f}ms\")\n",
    "print(f\"  Latency P95: {result.latency_p95_ms:.2f}ms\")\n",
    "print(f\"  Latency P99: {result.latency_p99_ms:.2f}ms\")\n",
    "\n",
    "# Example: Chaos Engineering\n",
    "chaos = ChaosEngineer()\n",
    "\n",
    "def inference_function():\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(1, 10)\n",
    "        return model(x)\n",
    "\n",
    "# Test with 10% failure rate\n",
    "chaos_result = chaos.run_experiment(\n",
    "    name=\"failure_injection\",\n",
    "    target_func=inference_function,\n",
    "    chaos_func=lambda f: chaos.inject_failure(f, failure_rate=0.1),\n",
    "    iterations=100\n",
    ")\n",
    "\n",
    "print(f\"\\nChaos Experiment Results:\")\n",
    "print(f\"  Experiment: {chaos_result['name']}\")\n",
    "print(f\"  Resilience Score: {chaos_result['resilience_score']:.1%}\")\n",
    "print(f\"  Failures: {chaos_result['failures']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAANG Interview Questions\n",
    "\n",
    "### Q1: How do you design a comprehensive test suite for an ML model before production?\n",
    "\n",
    "**Answer:**\n",
    "I design a multi-layered testing pyramid:\n",
    "\n",
    "1. **Unit Tests** (fastest, most numerous):\n",
    "   - Feature transformers: normalization, encoding, handling edge cases\n",
    "   - Model forward pass: output shapes, gradient flow, determinism\n",
    "   - Utility functions: data loading, preprocessing\n",
    "\n",
    "2. **Integration Tests**:\n",
    "   - End-to-end pipeline: training -> scoring -> prediction\n",
    "   - API contract testing: input/output schema validation\n",
    "   - Database/storage integration: model serialization/loading\n",
    "\n",
    "3. **Model Validation**:\n",
    "   - Performance thresholds: accuracy, precision, recall\n",
    "   - Regression tests against baseline\n",
    "   - Slice-based testing: performance on subgroups\n",
    "\n",
    "4. **Data Quality Tests**:\n",
    "   - Schema validation\n",
    "   - Distribution drift detection\n",
    "   - Feature completeness\n",
    "\n",
    "### Q2: How would you ensure statistical validity in A/B tests for ML models?\n",
    "\n",
    "**Answer:**\n",
    "Key practices:\n",
    "\n",
    "1. **Pre-registration**: Document hypothesis and analysis plan before experiment\n",
    "2. **Sample Size Calculation**: Compute required n based on MDE, power, and baseline rate\n",
    "3. **Sequential Testing**: Use alpha spending functions for early stopping without inflating false positives\n",
    "4. **Multiple Comparison Correction**: Apply Bonferroni or FDR when testing multiple metrics\n",
    "5. **Novelty Effects**: Wait sufficient time for effects to stabilize\n",
    "6. **Guardrail Metrics**: Monitor for negative side effects\n",
    "7. **Stratified Randomization**: Ensure balanced groups\n",
    "\n",
    "### Q3: What's your strategy for testing ML systems in production?\n",
    "\n",
    "**Answer:**\n",
    "Production testing strategy:\n",
    "\n",
    "1. **Shadow Mode**: Run new model alongside production without serving\n",
    "2. **Canary Deployment**: Gradual traffic shift (1% -> 5% -> 25% -> 100%)\n",
    "3. **Online Assessment**: Real-time metrics (latency, error rate, prediction distribution)\n",
    "4. **A/B Testing**: Statistically rigorous comparison of business metrics\n",
    "5. **Chaos Engineering**: Inject failures to test resilience\n",
    "6. **Load Testing**: Verify performance under peak traffic\n",
    "7. **Automated Rollback**: Trigger on SLO violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Unit Testing**: Testing individual ML components with proper fixtures\n",
    "2. **Integration Testing**: End-to-end pipeline validation\n",
    "3. **Model Validation**: Performance thresholds and regression testing\n",
    "4. **Data Quality**: Schema validation and expectation suites\n",
    "5. **A/B Testing**: Frequentist and Bayesian statistical analysis\n",
    "6. **Load & Chaos Testing**: Performance and resilience testing\n",
    "\n",
    "### Key Takeaways for FAANG Interviews:\n",
    "- ML systems require comprehensive testing beyond model accuracy\n",
    "- Data quality testing is as important as code testing\n",
    "- A/B tests need statistical rigor (power analysis, multiple comparisons)\n",
    "- Production testing includes shadow mode, canary, and chaos engineering\n",
    "- Automated testing enables continuous deployment with confidence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
