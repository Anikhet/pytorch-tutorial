{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c3a12f",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Training Your First Model\n",
    "\n",
    "We have the Data (Tensor). We have the Machine (Model). We have the Math (Autograd).\n",
    "\n",
    "Now we need to teach the machine. This is **Training**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is an \"Epoch\", \"Batch\", \"Loss\", and \"Optimizer\"?\n",
    "- **The Intuition**: Training as \"Learning to Ride a Bike\".\n",
    "- **The Loop**: The 5-step process that repeats millions of times.\n",
    "- **The Visual**: Watching the loss go down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860aa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06888d76",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Training a model is like training an athlete. Here are the terms:\n",
    "\n",
    "### 1. Epoch\n",
    "- One full pass through the entire dataset.\n",
    "- Example: If you have 1000 images and you look at all 1000, that's 1 Epoch.\n",
    "- Analogy: Reading the textbook cover-to-cover once.\n",
    "\n",
    "### 2. Batch\n",
    "- A small chunk of data processed at once.\n",
    "- We don't learn from 1 example at a time (too slow/noisy), nor all at once (too big for RAM).\n",
    "- Analogy: Studying one chapter at a time.\n",
    "\n",
    "### 3. Loss Function (The Scorecard)\n",
    "- Measures how bad the model's prediction is.\n",
    "- Example: MSE (Mean Squared Error) for numbers, CrossEntropy for categories.\n",
    "- Analogy: The grade on a practice test.\n",
    "\n",
    "### 4. Optimizer (The Coach)\n",
    "- The algorithm that updates the weights to reduce the loss.\n",
    "- Example: SGD (Stochastic Gradient Descent), Adam.\n",
    "- Analogy: The coach telling you \"Lean left!\" or \"Pedal harder!\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "krkd3tg5p9s",
   "source": "# Visualize Epochs, Batches, and Dataset relationship\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: Dataset division into batches\nax1.set_xlim(0, 10)\nax1.set_ylim(0, 10)\n\n# Simulate a dataset of 100 samples\ntotal_samples = 100\nbatch_size = 20\nnum_batches = total_samples // batch_size\n\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\ny_start = 8\n\nfor batch_idx in range(num_batches):\n    start_sample = batch_idx * batch_size\n    end_sample = (batch_idx + 1) * batch_size\n    \n    # Draw batch rectangle\n    rect = plt.Rectangle((1, y_start - batch_idx * 1.5), 8, 1, \n                         facecolor=colors[batch_idx], edgecolor='black', linewidth=2)\n    ax1.add_patch(rect)\n    \n    # Add label\n    ax1.text(5, y_start - batch_idx * 1.5 + 0.5, \n            f'Batch {batch_idx + 1}\\nSamples {start_sample}-{end_sample}',\n            ha='center', va='center', fontsize=11, fontweight='bold')\n\nax1.text(5, 9.5, 'Dataset (100 samples)', ha='center', fontsize=14, fontweight='bold',\n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nax1.text(5, 0.5, f'1 Epoch = Processing all {num_batches} batches', ha='center', \n        fontsize=12, fontweight='bold', color='red',\n        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\nax1.axis('off')\nax1.set_title('Dataset Division into Batches', fontsize=14, fontweight='bold')\n\n# Right: Multiple epochs\nax2.set_xlim(0, 6)\nax2.set_ylim(0, 10)\n\nepochs_to_show = 3\nfor epoch in range(epochs_to_show):\n    y_base = 8 - epoch * 3\n    \n    # Epoch label\n    ax2.text(0.5, y_base + 0.5, f'Epoch {epoch + 1}', fontsize=12, fontweight='bold',\n            rotation=0, va='center', bbox=dict(boxstyle='round', facecolor='lightgray'))\n    \n    # Mini batches\n    for batch in range(num_batches):\n        x_pos = 1.5 + batch * 0.8\n        rect = plt.Rectangle((x_pos, y_base), 0.6, 0.8, \n                           facecolor=colors[batch], edgecolor='black', linewidth=1.5)\n        ax2.add_patch(rect)\n        ax2.text(x_pos + 0.3, y_base + 0.4, f'B{batch+1}', \n                ha='center', va='center', fontsize=8, fontweight='bold')\n    \n    # Arrow to next epoch\n    if epoch < epochs_to_show - 1:\n        ax2.annotate('', xy=(0.5, y_base - 1), xytext=(0.5, y_base - 0.2),\n                   arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n\nax2.text(3, 0.5, 'Training continues...', ha='center', fontsize=11, \n        style='italic', color='gray')\nax2.axis('off')\nax2.set_title('Multiple Epochs of Training', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Concepts:\")\nprint(f\"â€¢ Dataset: {total_samples} total samples\")\nprint(f\"â€¢ Batch Size: {batch_size} samples per batch\")\nprint(f\"â€¢ Batches per Epoch: {num_batches}\")\nprint(f\"â€¢ 1 Epoch = Model sees each sample exactly once\")\nprint(f\"â€¢ Multiple Epochs = Model learns from the same data repeatedly\")\nprint(f\"â€¢ Total Updates (for 10 epochs) = {num_batches * 10} weight updates!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "skw91wvs00d",
   "source": "### Visualization: Understanding Epochs and Batches\n\nLet's visualize how data is divided into batches and processed over multiple epochs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "c372d1c9",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Learning to Ride a Bike)\n",
    "\n",
    "How do you learn to ride a bike?\n",
    "\n",
    "1. **Try**: You get on and pedal. (Forward Pass).\n",
    "2. **Fail**: You fall over. (Compute Loss).\n",
    "3. **Blame**: You realize you leaned too far left. (Compute Gradients).\n",
    "4. **Adjust**: You lean a bit to the right next time. (Update Parameters).\n",
    "5. **Repeat**: You do it again.\n",
    "\n",
    "This is exactly how Neural Networks learn."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a6f6x7g23g",
   "source": "import numpy as np\n\n# Create flowchart visualization of training loop\nfig, ax = plt.subplots(figsize=(12, 10))\n\nsteps = [\n    {\"text\": \"START\\n(Random Weights)\", \"y\": 0.95, \"color\": \"lightgray\"},\n    {\"text\": \"1. FORWARD PASS\\npred = model(X)\", \"y\": 0.80, \"color\": \"lightblue\"},\n    {\"text\": \"2. CALCULATE LOSS\\nloss = criterion(pred, y)\", \"y\": 0.65, \"color\": \"lightcoral\"},\n    {\"text\": \"3. ZERO GRADIENTS\\noptimizer.zero_grad()\", \"y\": 0.50, \"color\": \"lightyellow\"},\n    {\"text\": \"4. BACKPROPAGATION\\nloss.backward()\", \"y\": 0.35, \"color\": \"lightgreen\"},\n    {\"text\": \"5. UPDATE WEIGHTS\\noptimizer.step()\", \"y\": 0.20, \"color\": \"plum\"},\n    {\"text\": \"REPEAT\\n(Next Epoch)\", \"y\": 0.05, \"color\": \"lightgray\"}\n]\n\nfor i, step in enumerate(steps):\n    # Draw box\n    bbox = dict(boxstyle='round,pad=0.8', facecolor=step[\"color\"], \n                edgecolor='black', linewidth=2.5)\n    ax.text(0.5, step[\"y\"], step[\"text\"], ha='center', va='center',\n           fontsize=13, fontweight='bold', bbox=bbox)\n    \n    # Draw arrow to next step\n    if i < len(steps) - 1:\n        ax.annotate('', xy=(0.5, steps[i+1][\"y\"] + 0.04), \n                   xytext=(0.5, step[\"y\"] - 0.04),\n                   arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n\n# Draw loop-back arrow\nax.annotate('', xy=(0.72, 0.92), xytext=(0.72, 0.08),\n           arrowprops=dict(arrowstyle='->', lw=2.5, color='red', \n                         connectionstyle=\"arc3,rad=.5\"))\nax.text(0.85, 0.5, 'Training Loop\\n(Many Epochs)', fontsize=11, \n       color='red', fontweight='bold', rotation=90, va='center')\n\n# Add side annotations\nannotations = [\n    (0.05, 0.80, \"Compute predictions\\nfrom current weights\"),\n    (0.05, 0.65, \"How wrong are\\nwe? (Error)\"),\n    (0.05, 0.50, \"Clear old gradients\\n(Critical!)\"),\n    (0.05, 0.35, \"Calculate how to\\nimprove (âˆ‚Loss/âˆ‚W)\"),\n    (0.05, 0.20, \"Adjust weights:\\nW = W - lr Ã— âˆ‚Loss/âˆ‚W\")\n]\n\nfor x, y, text in annotations:\n    ax.text(x, y, text, fontsize=9, style='italic', \n           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('The 5-Step Training Loop\\n(Heart of Deep Learning)', \n            fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"This loop runs MILLIONS of times during training!\")\nprint(\"Each iteration makes the model slightly better.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "feefs34cln",
   "source": "### Visualization: The Training Process\n\nLet's visualize the 5-step training loop as a flowchart.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "346f317a",
   "metadata": {},
   "source": [
    "## Part 3: The Setup (Data, Model, Loss, Optimizer)\n",
    "\n",
    "Before the loop, we need 4 things:\n",
    "\n",
    "1. **Data**: $X$ (Inputs) and $y$ (Targets).\n",
    "2. **Model**: The network.\n",
    "3. **Loss Function**: The Scorecard.\n",
    "4. **Optimizer**: The Coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5be393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data (Linear Regression: y = 2x + 1)\n",
    "X = torch.linspace(0, 10, 100).view(-1, 1) # 100 inputs\n",
    "y = 2 * X + 1 + torch.randn(X.shape) * 0.5 # 100 targets (with noise)\n",
    "\n",
    "# 2. Model (Linear Layer)\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 3. Loss Function (MSE: Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. Optimizer (SGD: Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212befa5",
   "metadata": {},
   "source": [
    "## Part 4: The Training Loop (The 5 Steps)\n",
    "\n",
    "This loop is the heartbeat of Deep Learning. Memorize these 5 steps.\n",
    "\n",
    "1. **Forward Pass**: `pred = model(X)`\n",
    "2. **Calculate Loss**: `loss = criterion(pred, y)`\n",
    "3. **Zero Gradients**: `optimizer.zero_grad()` (Don't forget!)\n",
    "4. **Backpropagation**: `loss.backward()` (Compute gradients)\n",
    "5. **Step**: `optimizer.step()` (Update weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02377437",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced visualization of training results\nfig = plt.figure(figsize=(16, 10))\n\n# 1. Loss Curve with annotations\nax1 = plt.subplot(2, 3, 1)\nax1.plot(losses, linewidth=2.5, color='red', label='Training Loss')\nax1.fill_between(range(len(losses)), losses, alpha=0.3, color='red')\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('MSE Loss', fontsize=12)\nax1.set_title('Loss Curve\\n(Should Decrease!)', fontsize=13, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Mark key points\nmin_loss_idx = np.argmin(losses)\nax1.scatter([0, min_loss_idx, len(losses)-1], \n           [losses[0], losses[min_loss_idx], losses[-1]], \n           s=100, c=['red', 'green', 'blue'], zorder=5)\nax1.annotate(f'Start\\nLoss={losses[0]:.2f}', xy=(0, losses[0]), \n            xytext=(10, losses[0]+0.5), fontsize=9,\n            arrowprops=dict(arrowstyle='->', color='red'))\nax1.annotate(f'Best\\nLoss={losses[min_loss_idx]:.2f}', \n            xy=(min_loss_idx, losses[min_loss_idx]), \n            xytext=(min_loss_idx+10, losses[min_loss_idx]+0.5), fontsize=9,\n            arrowprops=dict(arrowstyle='->', color='green'))\n\n# 2. Model Fit\nax2 = plt.subplot(2, 3, 2)\nax2.scatter(X.numpy(), y.numpy(), alpha=0.6, s=30, label='True Data', color='blue')\nwith torch.no_grad():\n    predictions_final = model(X)\n    ax2.plot(X.numpy(), predictions_final.numpy(), color='red', \n            linewidth=3, label='Learned Line', linestyle='--')\nax2.set_xlabel('X (Input)', fontsize=12)\nax2.set_ylabel('y (Output)', fontsize=12)\nax2.set_title('Model Predictions vs True Data', fontsize=13, fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Residuals (Prediction Errors)\nax3 = plt.subplot(2, 3, 3)\nwith torch.no_grad():\n    residuals = (y - predictions_final).numpy()\nax3.scatter(X.numpy(), residuals, alpha=0.6, s=30, color='purple')\nax3.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Perfect Fit')\nax3.set_xlabel('X (Input)', fontsize=12)\nax3.set_ylabel('Residual (Error)', fontsize=12)\nax3.set_title('Prediction Errors\\n(Should be random noise)', fontsize=13, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Parameter Evolution (if we tracked it)\n# Let's retrain and track weight/bias changes\nmodel_track = nn.Linear(1, 1)\noptimizer_track = optim.SGD(model_track.parameters(), lr=0.01)\nweight_history = []\nbias_history = []\n\nfor epoch in range(epochs):\n    predictions = model_track(X)\n    loss = criterion(predictions, y)\n    optimizer_track.zero_grad()\n    loss.backward()\n    optimizer_track.step()\n    \n    weight_history.append(model_track.weight.item())\n    bias_history.append(model_track.bias.item())\n\nax4 = plt.subplot(2, 3, 4)\nax4.plot(weight_history, label='Weight', linewidth=2, color='blue')\nax4.axhline(y=2.0, color='blue', linestyle='--', linewidth=1, alpha=0.5, label='True Weight (2.0)')\nax4.set_xlabel('Epoch', fontsize=12)\nax4.set_ylabel('Weight Value', fontsize=12)\nax4.set_title('Weight Convergence', fontsize=13, fontweight='bold')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nax5 = plt.subplot(2, 3, 5)\nax5.plot(bias_history, label='Bias', linewidth=2, color='green')\nax5.axhline(y=1.0, color='green', linestyle='--', linewidth=1, alpha=0.5, label='True Bias (1.0)')\nax5.set_xlabel('Epoch', fontsize=12)\nax5.set_ylabel('Bias Value', fontsize=12)\nax5.set_title('Bias Convergence', fontsize=13, fontweight='bold')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# 6. Learning Rate Comparison\nax6 = plt.subplot(2, 3, 6)\nlearning_rates = [0.001, 0.01, 0.1]\ncolors = ['blue', 'green', 'red']\n\nfor lr, color in zip(learning_rates, colors):\n    model_lr = nn.Linear(1, 1)\n    optimizer_lr = optim.SGD(model_lr.parameters(), lr=lr)\n    losses_lr = []\n    \n    for epoch in range(50):\n        predictions = model_lr(X)\n        loss = criterion(predictions, y)\n        losses_lr.append(loss.item())\n        optimizer_lr.zero_grad()\n        loss.backward()\n        optimizer_lr.step()\n    \n    ax6.plot(losses_lr, label=f'LR={lr}', linewidth=2, color=color)\n\nax6.set_xlabel('Epoch', fontsize=12)\nax6.set_ylabel('Loss', fontsize=12)\nax6.set_title('Effect of Learning Rate', fontsize=13, fontweight='bold')\nax6.legend()\nax6.grid(True, alpha=0.3)\nax6.set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Results:\")\nprint(f\"â€¢ Learned Weight: {model.weight.item():.3f} (True: 2.000)\")\nprint(f\"â€¢ Learned Bias:   {model.bias.item():.3f} (True: 1.000)\")\nprint(f\"â€¢ Final Loss:     {losses[-1]:.4f}\")\nprint(f\"â€¢ Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "a4599e29",
   "metadata": {},
   "source": [
    "## Part 5: Visualization (Did it learn?)\n",
    "\n",
    "Let's see if the model learned the line $y = 2x + 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss Curve (Lower is Better)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "# Plot Predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "with torch.no_grad(): # Don't track gradients for plotting\n",
    "    plt.plot(X, model(X), color='red', label=\"Prediction\")\n",
    "plt.title(\"Model Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Check learned parameters\n",
    "print(f\"Learned Weight: {model.weight.item():.2f} (True: 2.0)\")\n",
    "print(f\"Learned Bias: {model.bias.item():.2f} (True: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 6: Advanced Training Concepts (FAANG Interview Essentials)\n\n### Optimizer Comparison - Know When to Use Each\n\nDifferent optimizers have different strengths. Know these for interviews."
  },
  {
   "cell_type": "code",
   "id": "db50330kcqh",
   "source": "# Optimizer Comparison - FAANG Interview Topic\n\n# Create a more challenging problem for optimizer comparison\ntorch.manual_seed(42)\nX_complex = torch.randn(200, 10)\ny_complex = (X_complex @ torch.randn(10, 1) + torch.randn(200, 1) * 0.1).squeeze()\n\ndef train_with_optimizer(optimizer_class, opt_kwargs, epochs=100):\n    \"\"\"Train a model with specified optimizer and return losses.\"\"\"\n    model = nn.Sequential(\n        nn.Linear(10, 32),\n        nn.ReLU(),\n        nn.Linear(32, 1)\n    )\n    optimizer = optimizer_class(model.parameters(), **opt_kwargs)\n    criterion = nn.MSELoss()\n    losses = []\n    \n    for _ in range(epochs):\n        pred = model(X_complex).squeeze()\n        loss = criterion(pred, y_complex)\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    return losses\n\n# Compare optimizers\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\noptimizers = {\n    'SGD (lr=0.01)': (optim.SGD, {'lr': 0.01}),\n    'SGD + Momentum': (optim.SGD, {'lr': 0.01, 'momentum': 0.9}),\n    'Adam (lr=0.01)': (optim.Adam, {'lr': 0.01}),\n    'AdamW (lr=0.01)': (optim.AdamW, {'lr': 0.01, 'weight_decay': 0.01}),\n}\n\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor (name, (opt_class, kwargs)), color in zip(optimizers.items(), colors):\n    losses = train_with_optimizer(opt_class, kwargs)\n    ax1.plot(losses, label=name, color=color, linewidth=2)\n\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Optimizer Comparison', fontsize=13, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_yscale('log')\n\n# Optimizer properties table\ntable_data = [\n    ['Optimizer', 'Momentum', 'Adaptive LR', 'Weight Decay', 'Best For'],\n    ['SGD', 'No', 'No', 'No', 'Simple problems'],\n    ['SGD+Momentum', 'Yes', 'No', 'No', 'Computer vision'],\n    ['Adam', 'Yes', 'Yes', 'No', 'General default'],\n    ['AdamW', 'Yes', 'Yes', 'Decoupled', 'Transformers/LLMs'],\n]\n\nax2.axis('off')\ntable = ax2.table(cellText=table_data, loc='center', cellLoc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.8)\n\n# Style header row\nfor j in range(5):\n    table[(0, j)].set_facecolor('#4472C4')\n    table[(0, j)].set_text_props(color='white', fontweight='bold')\n\nax2.set_title('Optimizer Properties', fontsize=13, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Optimizer Selection Guidelines:\")\nprint(\"â€¢ Adam: Best default choice - adaptive learning rate, fast convergence\")\nprint(\"â€¢ AdamW: Use for Transformers/LLMs (proper weight decay)\")\nprint(\"â€¢ SGD+Momentum: Often best for CNNs (better generalization)\")\nprint(\"â€¢ Pure SGD: Rarely used alone - too slow\")\nprint(\"\\nðŸ”‘ FAANG Tip: AdamW is the standard for LLMs, SGD+Momentum for vision\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0jnmoo7nmj58",
   "source": "### Production Training Loop - Best Practices\n\nA real training loop includes validation, logging, and checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o4a1npy3avg",
   "source": "# Production Training Loop Template\n\ndef production_training_loop(\n    model, \n    train_loader, \n    val_loader,\n    criterion,\n    optimizer,\n    epochs,\n    device='cpu',\n    scheduler=None,\n    early_stopping_patience=5\n):\n    \"\"\"\n    Production-ready training loop with all best practices.\n    \n    Features:\n    - Train/validation split\n    - Early stopping\n    - Learning rate scheduling\n    - Gradient clipping\n    - Best model checkpointing\n    - Proper train/eval modes\n    \"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    history = {'train_loss': [], 'val_loss': []}\n    \n    model = model.to(device)\n    \n    for epoch in range(epochs):\n        # === Training Phase ===\n        model.train()  # Set to training mode (enables dropout, etc.)\n        train_loss = 0.0\n        \n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            \n            # The 5 Steps\n            optimizer.zero_grad()                    # 1. Zero gradients\n            predictions = model(batch_x)             # 2. Forward pass\n            loss = criterion(predictions, batch_y)  # 3. Compute loss\n            loss.backward()                          # 4. Backward pass\n            \n            # Gradient clipping (prevents exploding gradients)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()                         # 5. Update weights\n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        \n        # === Validation Phase ===\n        model.eval()  # Set to evaluation mode (disables dropout, etc.)\n        val_loss = 0.0\n        \n        with torch.no_grad():  # No gradient computation needed\n            for batch_x, batch_y in val_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n                val_loss += loss.item()\n        \n        val_loss /= len(val_loader)\n        \n        # === Learning Rate Scheduling ===\n        if scheduler is not None:\n            scheduler.step(val_loss)  # ReduceLROnPlateau needs val_loss\n        \n        # === Early Stopping ===\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save best model (checkpoint)\n            best_model_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch + 1}\")\n                model.load_state_dict(best_model_state)  # Restore best\n                break\n        \n        # === Logging ===\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n    \n    return history, best_val_loss\n\n# Demo with simple data\nprint(\"Production Training Loop Template Features:\")\nprint(\"1. model.train() / model.eval() - Proper mode switching\")\nprint(\"2. torch.no_grad() - Disable gradients during validation\")\nprint(\"3. Gradient clipping - Prevent exploding gradients\")\nprint(\"4. Early stopping - Stop when validation stops improving\")\nprint(\"5. Checkpointing - Save and restore best model\")\nprint(\"\\nðŸ”‘ FAANG Tip: Missing model.eval() is the #1 bug in interview code!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9hfdibjd4ff",
   "source": "## Part 7: FAANG Interview Questions - Training\n\n### Question 1: \"Explain the difference between SGD and Adam\"\n\n**Answer**:\n\n**SGD (Stochastic Gradient Descent)**:\n```python\nw = w - lr * gradient  # Simple update\n```\n- Same learning rate for all parameters\n- No memory of past gradients\n- Often better generalization (less overfitting)\n- Needs careful learning rate tuning\n\n**Adam (Adaptive Moment Estimation)**:\n```python\nm = beta1 * m + (1 - beta1) * gradient      # Momentum (1st moment)\nv = beta2 * v + (1 - beta2) * gradient**2   # Velocity (2nd moment)\nw = w - lr * m / (sqrt(v) + eps)            # Adaptive update\n```\n- Adapts learning rate per-parameter\n- Remembers past gradients (momentum)\n- Faster convergence, less tuning\n- May generalize worse than SGD\n\n### Question 2: \"What is batch size and how does it affect training?\"\n\n**Answer**:\n| Batch Size | Pros | Cons |\n|------------|------|------|\n| Small (16-32) | Better generalization, less memory | Noisy gradients, slow |\n| Large (256+) | Stable gradients, GPU efficient | May overfit, needs LR tuning |\n\nThe key insight: **Effective batch size = batch_size Ã— gradient_accumulation_steps**\n\nFor large batch training, use:\n1. Learning rate warmup\n2. Linear scaling rule: `lr_new = lr_base * (batch_new / batch_base)`\n\n### Question 3: \"Why do we need to call optimizer.zero_grad()?\"\n\n**Answer**:\nPyTorch accumulates gradients by default (adds new gradients to existing `.grad`).\n\nWithout `zero_grad()`:\n```python\n# Iteration 1: grad = 2.0\n# Iteration 2: grad = 2.0 + 3.0 = 5.0  # WRONG! Gradients accumulate\n# Iteration 3: grad = 5.0 + 1.0 = 6.0  # WRONG!\n```\n\nThis design enables:\n1. Gradient accumulation for simulating large batches\n2. Multiple backward passes (multi-loss training)\n\n### Question 4: \"How do you detect and fix overfitting?\"\n\n**Answer**:\n\n**Detection**:\n```\nTraining Loss: decreasing â†“\nValidation Loss: decreasing then increasing â†‘ (gap widens)\n```\n\n**Fixes** (in order of preference):\n1. **More data**: Best solution\n2. **Data augmentation**: Free data\n3. **Regularization**: Dropout, weight decay\n4. **Early stopping**: Stop when val loss increases\n5. **Simpler model**: Reduce parameters\n\n### Question 5: \"Explain learning rate warmup and why it helps\"\n\n**Answer**:\nWarmup gradually increases learning rate from 0 to target over first N steps.\n\n```python\nwarmup_steps = 1000\nfor step in range(total_steps):\n    if step < warmup_steps:\n        lr = base_lr * (step / warmup_steps)  # Linear warmup\n    else:\n        lr = base_lr  # or cosine decay\n```\n\n**Why it helps**:\n1. Initial weights are random â†’ large gradients â†’ unstable\n2. Adam's running averages are zero â†’ inaccurate at start\n3. Prevents early \"bad updates\" that derail training\n\nStandard in modern training: warmup + cosine decay",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cyzuwkse05n",
   "source": "## Summary: Training Mastery Checklist\n\n### The 5-Step Loop (Memorize This)\n1. `optimizer.zero_grad()` - Clear old gradients\n2. `predictions = model(X)` - Forward pass\n3. `loss = criterion(predictions, y)` - Compute loss\n4. `loss.backward()` - Compute gradients\n5. `optimizer.step()` - Update weights\n\n### Key Vocabulary\n- [ ] Epoch = One full pass through dataset\n- [ ] Batch = Subset of data processed together\n- [ ] Loss = Error metric (MSE, CrossEntropy)\n- [ ] Optimizer = Weight update algorithm (Adam, SGD)\n\n### Optimizer Selection\n- [ ] Adam/AdamW - Default choice, works well out of the box\n- [ ] SGD + Momentum - Better generalization for CNNs\n- [ ] AdamW - Standard for Transformers/LLMs\n\n### Production Best Practices\n- [ ] `model.train()` / `model.eval()` - Mode switching\n- [ ] `torch.no_grad()` - Disable gradients for validation\n- [ ] Gradient clipping - Prevent exploding gradients\n- [ ] Early stopping - Stop when validation plateaus\n- [ ] Checkpointing - Save best model\n\n### Common Bugs\n- [ ] Forgetting `optimizer.zero_grad()` (gradient accumulation)\n- [ ] Forgetting `model.eval()` before inference\n- [ ] Forgetting `torch.no_grad()` during validation\n- [ ] Wrong loss function for task (MSE vs CrossEntropy)\n\n---\n**Next**: Notebook 04 - Practical Example: Regression",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}