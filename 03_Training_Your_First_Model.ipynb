{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c3a12f",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Training Your First Model\n",
    "\n",
    "We have the Data (Tensor). We have the Machine (Model). We have the Math (Autograd).\n",
    "\n",
    "Now we need to teach the machine. This is **Training**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is an \"Epoch\", \"Batch\", \"Loss\", and \"Optimizer\"?\n",
    "- **The Intuition**: Training as \"Learning to Ride a Bike\".\n",
    "- **The Loop**: The 5-step process that repeats millions of times.\n",
    "- **The Visual**: Watching the loss go down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860aa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06888d76",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Training a model is like training an athlete. Here are the terms:\n",
    "\n",
    "### 1. Epoch\n",
    "- One full pass through the entire dataset.\n",
    "- Example: If you have 1000 images and you look at all 1000, that's 1 Epoch.\n",
    "- Analogy: Reading the textbook cover-to-cover once.\n",
    "\n",
    "### 2. Batch\n",
    "- A small chunk of data processed at once.\n",
    "- We don't learn from 1 example at a time (too slow/noisy), nor all at once (too big for RAM).\n",
    "- Analogy: Studying one chapter at a time.\n",
    "\n",
    "### 3. Loss Function (The Scorecard)\n",
    "- Measures how bad the model's prediction is.\n",
    "- Example: MSE (Mean Squared Error) for numbers, CrossEntropy for categories.\n",
    "- Analogy: The grade on a practice test.\n",
    "\n",
    "### 4. Optimizer (The Coach)\n",
    "- The algorithm that updates the weights to reduce the loss.\n",
    "- Example: SGD (Stochastic Gradient Descent), Adam.\n",
    "- Analogy: The coach telling you \"Lean left!\" or \"Pedal harder!\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "krkd3tg5p9s",
   "source": "# Visualize Epochs, Batches, and Dataset relationship\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: Dataset division into batches\nax1.set_xlim(0, 10)\nax1.set_ylim(0, 10)\n\n# Simulate a dataset of 100 samples\ntotal_samples = 100\nbatch_size = 20\nnum_batches = total_samples // batch_size\n\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\ny_start = 8\n\nfor batch_idx in range(num_batches):\n    start_sample = batch_idx * batch_size\n    end_sample = (batch_idx + 1) * batch_size\n    \n    # Draw batch rectangle\n    rect = plt.Rectangle((1, y_start - batch_idx * 1.5), 8, 1, \n                         facecolor=colors[batch_idx], edgecolor='black', linewidth=2)\n    ax1.add_patch(rect)\n    \n    # Add label\n    ax1.text(5, y_start - batch_idx * 1.5 + 0.5, \n            f'Batch {batch_idx + 1}\\nSamples {start_sample}-{end_sample}',\n            ha='center', va='center', fontsize=11, fontweight='bold')\n\nax1.text(5, 9.5, 'Dataset (100 samples)', ha='center', fontsize=14, fontweight='bold',\n        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nax1.text(5, 0.5, f'1 Epoch = Processing all {num_batches} batches', ha='center', \n        fontsize=12, fontweight='bold', color='red',\n        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\nax1.axis('off')\nax1.set_title('Dataset Division into Batches', fontsize=14, fontweight='bold')\n\n# Right: Multiple epochs\nax2.set_xlim(0, 6)\nax2.set_ylim(0, 10)\n\nepochs_to_show = 3\nfor epoch in range(epochs_to_show):\n    y_base = 8 - epoch * 3\n    \n    # Epoch label\n    ax2.text(0.5, y_base + 0.5, f'Epoch {epoch + 1}', fontsize=12, fontweight='bold',\n            rotation=0, va='center', bbox=dict(boxstyle='round', facecolor='lightgray'))\n    \n    # Mini batches\n    for batch in range(num_batches):\n        x_pos = 1.5 + batch * 0.8\n        rect = plt.Rectangle((x_pos, y_base), 0.6, 0.8, \n                           facecolor=colors[batch], edgecolor='black', linewidth=1.5)\n        ax2.add_patch(rect)\n        ax2.text(x_pos + 0.3, y_base + 0.4, f'B{batch+1}', \n                ha='center', va='center', fontsize=8, fontweight='bold')\n    \n    # Arrow to next epoch\n    if epoch < epochs_to_show - 1:\n        ax2.annotate('', xy=(0.5, y_base - 1), xytext=(0.5, y_base - 0.2),\n                   arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n\nax2.text(3, 0.5, 'Training continues...', ha='center', fontsize=11, \n        style='italic', color='gray')\nax2.axis('off')\nax2.set_title('Multiple Epochs of Training', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Concepts:\")\nprint(f\"• Dataset: {total_samples} total samples\")\nprint(f\"• Batch Size: {batch_size} samples per batch\")\nprint(f\"• Batches per Epoch: {num_batches}\")\nprint(f\"• 1 Epoch = Model sees each sample exactly once\")\nprint(f\"• Multiple Epochs = Model learns from the same data repeatedly\")\nprint(f\"• Total Updates (for 10 epochs) = {num_batches * 10} weight updates!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "skw91wvs00d",
   "source": "### Visualization: Understanding Epochs and Batches\n\nLet's visualize how data is divided into batches and processed over multiple epochs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "c372d1c9",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Learning to Ride a Bike)\n",
    "\n",
    "How do you learn to ride a bike?\n",
    "\n",
    "1. **Try**: You get on and pedal. (Forward Pass).\n",
    "2. **Fail**: You fall over. (Compute Loss).\n",
    "3. **Blame**: You realize you leaned too far left. (Compute Gradients).\n",
    "4. **Adjust**: You lean a bit to the right next time. (Update Parameters).\n",
    "5. **Repeat**: You do it again.\n",
    "\n",
    "This is exactly how Neural Networks learn."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a6f6x7g23g",
   "source": "import numpy as np\n\n# Create flowchart visualization of training loop\nfig, ax = plt.subplots(figsize=(12, 10))\n\nsteps = [\n    {\"text\": \"START\\n(Random Weights)\", \"y\": 0.95, \"color\": \"lightgray\"},\n    {\"text\": \"1. FORWARD PASS\\npred = model(X)\", \"y\": 0.80, \"color\": \"lightblue\"},\n    {\"text\": \"2. CALCULATE LOSS\\nloss = criterion(pred, y)\", \"y\": 0.65, \"color\": \"lightcoral\"},\n    {\"text\": \"3. ZERO GRADIENTS\\noptimizer.zero_grad()\", \"y\": 0.50, \"color\": \"lightyellow\"},\n    {\"text\": \"4. BACKPROPAGATION\\nloss.backward()\", \"y\": 0.35, \"color\": \"lightgreen\"},\n    {\"text\": \"5. UPDATE WEIGHTS\\noptimizer.step()\", \"y\": 0.20, \"color\": \"plum\"},\n    {\"text\": \"REPEAT\\n(Next Epoch)\", \"y\": 0.05, \"color\": \"lightgray\"}\n]\n\nfor i, step in enumerate(steps):\n    # Draw box\n    bbox = dict(boxstyle='round,pad=0.8', facecolor=step[\"color\"], \n                edgecolor='black', linewidth=2.5)\n    ax.text(0.5, step[\"y\"], step[\"text\"], ha='center', va='center',\n           fontsize=13, fontweight='bold', bbox=bbox)\n    \n    # Draw arrow to next step\n    if i < len(steps) - 1:\n        ax.annotate('', xy=(0.5, steps[i+1][\"y\"] + 0.04), \n                   xytext=(0.5, step[\"y\"] - 0.04),\n                   arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n\n# Draw loop-back arrow\nax.annotate('', xy=(0.72, 0.92), xytext=(0.72, 0.08),\n           arrowprops=dict(arrowstyle='->', lw=2.5, color='red', \n                         connectionstyle=\"arc3,rad=.5\"))\nax.text(0.85, 0.5, 'Training Loop\\n(Many Epochs)', fontsize=11, \n       color='red', fontweight='bold', rotation=90, va='center')\n\n# Add side annotations\nannotations = [\n    (0.05, 0.80, \"Compute predictions\\nfrom current weights\"),\n    (0.05, 0.65, \"How wrong are\\nwe? (Error)\"),\n    (0.05, 0.50, \"Clear old gradients\\n(Critical!)\"),\n    (0.05, 0.35, \"Calculate how to\\nimprove (∂Loss/∂W)\"),\n    (0.05, 0.20, \"Adjust weights:\\nW = W - lr × ∂Loss/∂W\")\n]\n\nfor x, y, text in annotations:\n    ax.text(x, y, text, fontsize=9, style='italic', \n           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('The 5-Step Training Loop\\n(Heart of Deep Learning)', \n            fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"This loop runs MILLIONS of times during training!\")\nprint(\"Each iteration makes the model slightly better.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "feefs34cln",
   "source": "### Visualization: The Training Process\n\nLet's visualize the 5-step training loop as a flowchart.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "346f317a",
   "metadata": {},
   "source": [
    "## Part 3: The Setup (Data, Model, Loss, Optimizer)\n",
    "\n",
    "Before the loop, we need 4 things:\n",
    "\n",
    "1. **Data**: $X$ (Inputs) and $y$ (Targets).\n",
    "2. **Model**: The network.\n",
    "3. **Loss Function**: The Scorecard.\n",
    "4. **Optimizer**: The Coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5be393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data (Linear Regression: y = 2x + 1)\n",
    "X = torch.linspace(0, 10, 100).view(-1, 1) # 100 inputs\n",
    "y = 2 * X + 1 + torch.randn(X.shape) * 0.5 # 100 targets (with noise)\n",
    "\n",
    "# 2. Model (Linear Layer)\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 3. Loss Function (MSE: Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. Optimizer (SGD: Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212befa5",
   "metadata": {},
   "source": [
    "## Part 4: The Training Loop (The 5 Steps)\n",
    "\n",
    "This loop is the heartbeat of Deep Learning. Memorize these 5 steps.\n",
    "\n",
    "1. **Forward Pass**: `pred = model(X)`\n",
    "2. **Calculate Loss**: `loss = criterion(pred, y)`\n",
    "3. **Zero Gradients**: `optimizer.zero_grad()` (Don't forget!)\n",
    "4. **Backpropagation**: `loss.backward()` (Compute gradients)\n",
    "5. **Step**: `optimizer.step()` (Update weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02377437",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced visualization of training results\nfig = plt.figure(figsize=(16, 10))\n\n# 1. Loss Curve with annotations\nax1 = plt.subplot(2, 3, 1)\nax1.plot(losses, linewidth=2.5, color='red', label='Training Loss')\nax1.fill_between(range(len(losses)), losses, alpha=0.3, color='red')\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('MSE Loss', fontsize=12)\nax1.set_title('Loss Curve\\n(Should Decrease!)', fontsize=13, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Mark key points\nmin_loss_idx = np.argmin(losses)\nax1.scatter([0, min_loss_idx, len(losses)-1], \n           [losses[0], losses[min_loss_idx], losses[-1]], \n           s=100, c=['red', 'green', 'blue'], zorder=5)\nax1.annotate(f'Start\\nLoss={losses[0]:.2f}', xy=(0, losses[0]), \n            xytext=(10, losses[0]+0.5), fontsize=9,\n            arrowprops=dict(arrowstyle='->', color='red'))\nax1.annotate(f'Best\\nLoss={losses[min_loss_idx]:.2f}', \n            xy=(min_loss_idx, losses[min_loss_idx]), \n            xytext=(min_loss_idx+10, losses[min_loss_idx]+0.5), fontsize=9,\n            arrowprops=dict(arrowstyle='->', color='green'))\n\n# 2. Model Fit\nax2 = plt.subplot(2, 3, 2)\nax2.scatter(X.numpy(), y.numpy(), alpha=0.6, s=30, label='True Data', color='blue')\nwith torch.no_grad():\n    predictions_final = model(X)\n    ax2.plot(X.numpy(), predictions_final.numpy(), color='red', \n            linewidth=3, label='Learned Line', linestyle='--')\nax2.set_xlabel('X (Input)', fontsize=12)\nax2.set_ylabel('y (Output)', fontsize=12)\nax2.set_title('Model Predictions vs True Data', fontsize=13, fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Residuals (Prediction Errors)\nax3 = plt.subplot(2, 3, 3)\nwith torch.no_grad():\n    residuals = (y - predictions_final).numpy()\nax3.scatter(X.numpy(), residuals, alpha=0.6, s=30, color='purple')\nax3.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Perfect Fit')\nax3.set_xlabel('X (Input)', fontsize=12)\nax3.set_ylabel('Residual (Error)', fontsize=12)\nax3.set_title('Prediction Errors\\n(Should be random noise)', fontsize=13, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Parameter Evolution (if we tracked it)\n# Let's retrain and track weight/bias changes\nmodel_track = nn.Linear(1, 1)\noptimizer_track = optim.SGD(model_track.parameters(), lr=0.01)\nweight_history = []\nbias_history = []\n\nfor epoch in range(epochs):\n    predictions = model_track(X)\n    loss = criterion(predictions, y)\n    optimizer_track.zero_grad()\n    loss.backward()\n    optimizer_track.step()\n    \n    weight_history.append(model_track.weight.item())\n    bias_history.append(model_track.bias.item())\n\nax4 = plt.subplot(2, 3, 4)\nax4.plot(weight_history, label='Weight', linewidth=2, color='blue')\nax4.axhline(y=2.0, color='blue', linestyle='--', linewidth=1, alpha=0.5, label='True Weight (2.0)')\nax4.set_xlabel('Epoch', fontsize=12)\nax4.set_ylabel('Weight Value', fontsize=12)\nax4.set_title('Weight Convergence', fontsize=13, fontweight='bold')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nax5 = plt.subplot(2, 3, 5)\nax5.plot(bias_history, label='Bias', linewidth=2, color='green')\nax5.axhline(y=1.0, color='green', linestyle='--', linewidth=1, alpha=0.5, label='True Bias (1.0)')\nax5.set_xlabel('Epoch', fontsize=12)\nax5.set_ylabel('Bias Value', fontsize=12)\nax5.set_title('Bias Convergence', fontsize=13, fontweight='bold')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# 6. Learning Rate Comparison\nax6 = plt.subplot(2, 3, 6)\nlearning_rates = [0.001, 0.01, 0.1]\ncolors = ['blue', 'green', 'red']\n\nfor lr, color in zip(learning_rates, colors):\n    model_lr = nn.Linear(1, 1)\n    optimizer_lr = optim.SGD(model_lr.parameters(), lr=lr)\n    losses_lr = []\n    \n    for epoch in range(50):\n        predictions = model_lr(X)\n        loss = criterion(predictions, y)\n        losses_lr.append(loss.item())\n        optimizer_lr.zero_grad()\n        loss.backward()\n        optimizer_lr.step()\n    \n    ax6.plot(losses_lr, label=f'LR={lr}', linewidth=2, color=color)\n\nax6.set_xlabel('Epoch', fontsize=12)\nax6.set_ylabel('Loss', fontsize=12)\nax6.set_title('Effect of Learning Rate', fontsize=13, fontweight='bold')\nax6.legend()\nax6.grid(True, alpha=0.3)\nax6.set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Results:\")\nprint(f\"• Learned Weight: {model.weight.item():.3f} (True: 2.000)\")\nprint(f\"• Learned Bias:   {model.bias.item():.3f} (True: 1.000)\")\nprint(f\"• Final Loss:     {losses[-1]:.4f}\")\nprint(f\"• Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "a4599e29",
   "metadata": {},
   "source": [
    "## Part 5: Visualization (Did it learn?)\n",
    "\n",
    "Let's see if the model learned the line $y = 2x + 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss Curve (Lower is Better)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "# Plot Predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "with torch.no_grad(): # Don't track gradients for plotting\n",
    "    plt.plot(X, model(X), color='red', label=\"Prediction\")\n",
    "plt.title(\"Model Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Check learned parameters\n",
    "print(f\"Learned Weight: {model.weight.item():.2f} (True: 2.0)\")\n",
    "print(f\"Learned Bias: {model.bias.item():.2f} (True: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **Epoch** = One full pass through the dataset.\n",
    "2. **Loss** = The error metric we want to minimize.\n",
    "3. **Optimizer** = The algorithm (SGD, Adam) that updates weights.\n",
    "4. **The 5 Steps**: Forward -> Loss -> Zero -> Backward -> Step.\n",
    "\n",
    "You have now trained your first AI model from scratch. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}