{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Training Your First Model\n",
    "\n",
    "Now that you can build neural networks, it's time to train them! This notebook covers the complete training process.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the training loop\n",
    "- Learn about loss functions and optimizers\n",
    "- Implement a complete training loop\n",
    "- Understand validation and evaluation\n",
    "- Visualize training progress\n",
    "\n",
    "---\n",
    "\n",
    "## What is Training?\n",
    "\n",
    "**Training** is the process of teaching a neural network to make good predictions by:\n",
    "1. Making predictions on data\n",
    "2. Measuring how wrong they are (loss)\n",
    "3. Computing gradients\n",
    "4. Updating parameters to reduce the loss\n",
    "5. Repeating until the model learns\n",
    "\n",
    "This is exactly what gradient descent does, but applied to neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Dataset\n",
    "\n",
    "Let's create a simple dataset to train on. We'll predict y from x where y = 2x + 1 (with some noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: y = 2x + 1 + noise\n",
    "n_samples = 100\n",
    "x = torch.randn(n_samples, 1) * 5\n",
    "y_true = 2 * x + 1 + torch.randn(n_samples, 1) * 0.5\n",
    "\n",
    "print(f\"Dataset size: {n_samples} samples\")\n",
    "print(f\"X shape: {x.shape}, Y shape: {y_true.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x.numpy(), y_true.numpy(), alpha=0.6)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Training Data: y = 2x + 1 + noise', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleLinearModel()\n",
    "print(\"Model:\", model)\n",
    "print(\"\\nInitial parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimizers\n",
    "\n",
    "**Loss function** measures prediction error. **Optimizer** updates parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error for regression\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Stochastic Gradient Descent optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Loss function: MSE\")\n",
    "print(f\"Optimizer: SGD with lr={learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Training Loop\n",
    "\n",
    "This is the core pattern used in ALL neural network training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model = SimpleLinearModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    loss.backward()         # Compute gradients\n",
    "    optimizer.step()        # Update parameters\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "print(\"\\nLearned parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.item():.4f}\")\n",
    "print(\"Expected: weight â‰ˆ 2.0, bias â‰ˆ 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)\n",
    "plt.scatter(x.numpy(), y_true.numpy(), alpha=0.6, label='Actual')\n",
    "plt.scatter(x.numpy(), predictions.numpy(), alpha=0.6, label='Predicted')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split\n",
    "\n",
    "In practice, we split data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_size = int(0.8 * len(x))\n",
    "x_train, y_train = x[:train_size], y_true[:train_size]\n",
    "x_val, y_val = x[train_size:], y_true[train_size:]\n",
    "\n",
    "# New model\n",
    "model = SimpleLinearModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training\n",
    "    model.train()\n",
    "    pred = model(x_train)\n",
    "    train_loss = criterion(pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(x_val)\n",
    "        val_loss = criterion(val_pred, y_val)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Training Loop**: Forward â†’ Loss â†’ Backward â†’ Update\n",
    "2. **Loss Functions**: MSE for regression, CrossEntropy for classification\n",
    "3. **Optimizers**: SGD, Adam, etc. - update parameters using gradients\n",
    "4. **Epochs**: One complete pass through the dataset\n",
    "5. **Validation**: Test on unseen data to check generalization\n",
    "6. **model.train()** / **model.eval()**: Set model mode\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Next notebooks: Regression and Classification examples using this training loop!\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You can now train neural networks! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
