{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML System Design Patterns\n",
    "\n",
    "## Overview\n",
    "Core ML system design patterns tested in FAANG interviews:\n",
    "- **Recommendation Systems**: Two-tower, collaborative filtering, content-based\n",
    "- **Search Ranking**: Learning-to-rank, query understanding, relevance\n",
    "- **Fraud Detection**: Real-time scoring, anomaly detection, imbalanced data\n",
    "- **Ads/CTR Prediction**: Click models, calibration, explore-exploit\n",
    "- **Content Moderation**: Multi-label classification, active learning\n",
    "\n",
    "## FAANG Interview Framework\n",
    "1. **Clarify** (10%): Requirements, scale, constraints\n",
    "2. **High-Level Design** (20%): Components, data flow\n",
    "3. **Deep Dive** (50%): Model architecture, features, training\n",
    "4. **Evaluation** (20%): Metrics, A/B testing, monitoring\n",
    "\n",
    "## Common Questions\n",
    "- Design Instagram's Explore page (Meta)\n",
    "- Design YouTube search ranking (Google)\n",
    "- Design fraud detection for Stripe (Stripe/TikTok)\n",
    "- Design ad click prediction (Meta/Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "print(\"ML System Design Patterns - FAANG Interview Prep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Recommendation Systems\n",
    "\n",
    "## The Two-Tower Architecture\n",
    "Standard pattern for large-scale recommendations (YouTube, Instagram, TikTok)\n",
    "\n",
    "```\n",
    "User Tower          Item Tower\n",
    "    |                   |\n",
    "User Features      Item Features\n",
    "    |                   |\n",
    "  [MLP]               [MLP]\n",
    "    |                   |\n",
    "User Embedding     Item Embedding\n",
    "    \\                  /\n",
    "     \\                /\n",
    "      Dot Product Score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-Tower architecture for candidate generation.\n",
    "    \n",
    "    Used by: YouTube, Instagram Reels, TikTok\n",
    "    Scale: Millions of items, billions of users\n",
    "    Latency: ~10ms for retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_users: int,\n",
    "                 num_items: int,\n",
    "                 embedding_dim: int = 64,\n",
    "                 hidden_dims: List[int] = [128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # User tower\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.user_mlp = self._build_mlp(\n",
    "            embedding_dim, hidden_dims, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Item tower\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.item_mlp = self._build_mlp(\n",
    "            embedding_dim, hidden_dims, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Temperature for softmax\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def _build_mlp(self, input_dim: int, hidden_dims: List[int], \n",
    "                   output_dim: int) -> nn.Sequential:\n",
    "        \"\"\"Build MLP tower.\"\"\"\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def get_user_embedding(self, user_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get user embeddings (for serving).\"\"\"\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        user_vec = self.user_mlp(user_emb)\n",
    "        return F.normalize(user_vec, p=2, dim=-1)\n",
    "    \n",
    "    def get_item_embedding(self, item_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get item embeddings (pre-computed for ANN index).\"\"\"\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        item_vec = self.item_mlp(item_emb)\n",
    "        return F.normalize(item_vec, p=2, dim=-1)\n",
    "    \n",
    "    def forward(self, user_ids: torch.Tensor, \n",
    "                positive_ids: torch.Tensor,\n",
    "                negative_ids: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with in-batch negatives.\n",
    "        \n",
    "        Training strategy: Use other items in batch as negatives.\n",
    "        \"\"\"\n",
    "        user_vec = self.get_user_embedding(user_ids)\n",
    "        pos_vec = self.get_item_embedding(positive_ids)\n",
    "        \n",
    "        # Positive scores\n",
    "        pos_scores = torch.sum(user_vec * pos_vec, dim=-1)\n",
    "        \n",
    "        # In-batch negatives: all items in batch are potential negatives\n",
    "        # Shape: (batch_size, batch_size)\n",
    "        all_scores = torch.matmul(user_vec, pos_vec.T) / self.temperature\n",
    "        \n",
    "        # Labels: diagonal is positive (index i matches user i)\n",
    "        labels = torch.arange(len(user_ids), device=user_ids.device)\n",
    "        \n",
    "        # Cross-entropy loss (treats as multi-class classification)\n",
    "        loss = F.cross_entropy(all_scores, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'pos_scores': pos_scores,\n",
    "            'user_embeddings': user_vec,\n",
    "            'item_embeddings': pos_vec\n",
    "        }\n",
    "\n",
    "# Example\n",
    "print(\"\\n=== Two-Tower Recommendation Model ===\")\n",
    "model = TwoTowerModel(num_users=10000, num_items=100000)\n",
    "\n",
    "# Training batch\n",
    "user_ids = torch.randint(0, 10000, (32,))\n",
    "item_ids = torch.randint(0, 100000, (32,))\n",
    "\n",
    "output = model(user_ids, item_ids)\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"User embedding shape: {output['user_embeddings'].shape}\")\n",
    "print(f\"Item embedding shape: {output['item_embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateGeneration:\n",
    "    \"\"\"\n",
    "    Stage 1: Retrieve candidates from millions of items.\n",
    "    \n",
    "    Techniques:\n",
    "    - ANN search (FAISS, ScaNN)\n",
    "    - Collaborative filtering\n",
    "    - Content-based filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.item_index = None  # FAISS index in production\n",
    "        self.item_embeddings = None\n",
    "    \n",
    "    def build_index(self, item_embeddings: np.ndarray):\n",
    "        \"\"\"Build ANN index for fast retrieval.\"\"\"\n",
    "        # In production: Use FAISS or ScaNN\n",
    "        # faiss.IndexFlatIP or faiss.IndexIVFFlat\n",
    "        self.item_embeddings = item_embeddings\n",
    "        print(f\"Built index with {len(item_embeddings)} items\")\n",
    "    \n",
    "    def retrieve(self, user_embedding: np.ndarray, k: int = 100) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k candidates using ANN.\n",
    "        \n",
    "        Latency target: < 10ms\n",
    "        \"\"\"\n",
    "        if self.item_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Cosine similarity (dot product for normalized vectors)\n",
    "        scores = np.dot(self.item_embeddings, user_embedding)\n",
    "        \n",
    "        # Top-k\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        return [(idx, scores[idx]) for idx in top_indices]\n",
    "\n",
    "class Ranker(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 2: Rank candidates with a heavy model.\n",
    "    \n",
    "    Features:\n",
    "    - User features (demographics, history)\n",
    "    - Item features (content, popularity)\n",
    "    - Context features (time, device)\n",
    "    - Cross features (user-item interactions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, hidden_dims: List[int] = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = feature_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Multi-task heads\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.click_head = nn.Linear(prev_dim, 1)  # P(click)\n",
    "        self.engage_head = nn.Linear(prev_dim, 1)  # P(engagement)\n",
    "        self.share_head = nn.Linear(prev_dim, 1)  # P(share)\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Predict multiple objectives.\"\"\"\n",
    "        shared_rep = self.shared(features)\n",
    "        \n",
    "        return {\n",
    "            'p_click': torch.sigmoid(self.click_head(shared_rep)),\n",
    "            'p_engage': torch.sigmoid(self.engage_head(shared_rep)),\n",
    "            'p_share': torch.sigmoid(self.share_head(shared_rep))\n",
    "        }\n",
    "    \n",
    "    def compute_ranking_score(self, predictions: Dict[str, torch.Tensor],\n",
    "                              weights: Dict[str, float] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Combine predictions into final ranking score.\n",
    "        \n",
    "        Score = w1 * P(click) + w2 * P(engage) + w3 * P(share)\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'p_click': 1.0, 'p_engage': 2.0, 'p_share': 3.0}\n",
    "        \n",
    "        score = sum(weights[k] * v for k, v in predictions.items())\n",
    "        return score\n",
    "\n",
    "print(\"\\n=== Candidate Generation + Ranking ===\")\n",
    "print(\"Stage 1 (Retrieval): 1M items -> 100 candidates (<10ms)\")\n",
    "print(\"Stage 2 (Ranking): 100 candidates -> 20 ranked (<50ms)\")\n",
    "print(\"Stage 3 (Re-ranking): Business rules, diversity (<10ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Search Ranking (Learning-to-Rank)\n",
    "\n",
    "## The Search Pipeline\n",
    "```\n",
    "Query -> Query Understanding -> Retrieval -> Ranking -> Re-ranking -> Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryUnderstanding:\n",
    "    \"\"\"\n",
    "    Parse and enrich user queries.\n",
    "    \n",
    "    Components:\n",
    "    - Query classification (navigational, informational, transactional)\n",
    "    - Entity recognition (brands, categories)\n",
    "    - Query expansion (synonyms, spelling correction)\n",
    "    - Intent detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {'the', 'a', 'an', 'is', 'are', 'for', 'to'}\n",
    "    \n",
    "    def parse(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse and enrich query.\"\"\"\n",
    "        tokens = query.lower().split()\n",
    "        \n",
    "        return {\n",
    "            'original': query,\n",
    "            'tokens': tokens,\n",
    "            'filtered_tokens': [t for t in tokens if t not in self.stop_words],\n",
    "            'intent': self._detect_intent(query),\n",
    "            'entities': self._extract_entities(query),\n",
    "            'query_type': self._classify_query(query)\n",
    "        }\n",
    "    \n",
    "    def _detect_intent(self, query: str) -> str:\n",
    "        \"\"\"Detect user intent.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(w in query_lower for w in ['buy', 'price', 'cheap', 'deal']):\n",
    "            return 'transactional'\n",
    "        elif any(w in query_lower for w in ['how', 'what', 'why', 'tutorial']):\n",
    "            return 'informational'\n",
    "        else:\n",
    "            return 'navigational'\n",
    "    \n",
    "    def _extract_entities(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Extract named entities.\"\"\"\n",
    "        # In production: Use NER model\n",
    "        entities = []\n",
    "        \n",
    "        # Simple brand detection\n",
    "        brands = ['apple', 'samsung', 'nike', 'sony']\n",
    "        for brand in brands:\n",
    "            if brand in query.lower():\n",
    "                entities.append({'type': 'brand', 'value': brand})\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _classify_query(self, query: str) -> str:\n",
    "        \"\"\"Classify query type.\"\"\"\n",
    "        if len(query.split()) == 1:\n",
    "            return 'head'  # Popular, short\n",
    "        elif len(query.split()) <= 3:\n",
    "            return 'torso'  # Medium frequency\n",
    "        else:\n",
    "            return 'tail'  # Long, specific\n",
    "\n",
    "class LearningToRank(nn.Module):\n",
    "    \"\"\"\n",
    "    Learning-to-Rank model for search.\n",
    "    \n",
    "    Approaches:\n",
    "    - Pointwise: Predict relevance score\n",
    "    - Pairwise: Predict which doc is more relevant (RankNet)\n",
    "    - Listwise: Optimize list-level metric (LambdaRank, ListNet)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict relevance score.\"\"\"\n",
    "        return self.model(features).squeeze(-1)\n",
    "    \n",
    "    def pairwise_loss(self, scores_i: torch.Tensor, scores_j: torch.Tensor,\n",
    "                      labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RankNet pairwise loss.\n",
    "        \n",
    "        labels: 1 if doc_i > doc_j, 0 otherwise\n",
    "        \"\"\"\n",
    "        diff = scores_i - scores_j\n",
    "        return F.binary_cross_entropy_with_logits(diff, labels.float())\n",
    "    \n",
    "    def listwise_loss(self, scores: torch.Tensor, \n",
    "                      relevance: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ListNet loss (softmax cross-entropy).\n",
    "        \n",
    "        scores: (batch_size, num_docs)\n",
    "        relevance: (batch_size, num_docs) - ground truth relevance\n",
    "        \"\"\"\n",
    "        # Convert to probability distributions\n",
    "        pred_dist = F.softmax(scores, dim=-1)\n",
    "        true_dist = F.softmax(relevance.float(), dim=-1)\n",
    "        \n",
    "        # Cross-entropy\n",
    "        loss = -torch.sum(true_dist * torch.log(pred_dist + 1e-10), dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "def compute_ndcg(relevance: List[int], k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    Primary metric for search ranking.\n",
    "    \"\"\"\n",
    "    def dcg(rel: List[int], k: int) -> float:\n",
    "        rel = rel[:k]\n",
    "        return sum((2**r - 1) / np.log2(i + 2) for i, r in enumerate(rel))\n",
    "    \n",
    "    # DCG of predicted ranking\n",
    "    actual_dcg = dcg(relevance, k)\n",
    "    \n",
    "    # Ideal DCG (sorted by relevance)\n",
    "    ideal_dcg = dcg(sorted(relevance, reverse=True), k)\n",
    "    \n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return actual_dcg / ideal_dcg\n",
    "\n",
    "# Example\n",
    "print(\"\\n=== Search Ranking Example ===\")\n",
    "qu = QueryUnderstanding()\n",
    "parsed = qu.parse(\"buy cheap apple iphone\")\n",
    "print(f\"Query: {parsed['original']}\")\n",
    "print(f\"Intent: {parsed['intent']}\")\n",
    "print(f\"Entities: {parsed['entities']}\")\n",
    "print(f\"Type: {parsed['query_type']}\")\n",
    "\n",
    "# NDCG example\n",
    "relevance = [3, 2, 3, 0, 1, 2]  # Graded relevance (0-3)\n",
    "print(f\"\\nNDCG@5: {compute_ndcg(relevance, k=5):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Fraud Detection\n",
    "\n",
    "## Real-Time Fraud Scoring System\n",
    "Requirements:\n",
    "- Latency: < 100ms per transaction\n",
    "- Scale: Millions of transactions/day\n",
    "- Imbalanced: ~0.1% fraud rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Transaction:\n",
    "    \"\"\"Transaction data for fraud detection.\"\"\"\n",
    "    transaction_id: str\n",
    "    user_id: str\n",
    "    amount: float\n",
    "    merchant_id: str\n",
    "    timestamp: float\n",
    "    device_id: str\n",
    "    ip_address: str\n",
    "    location: Tuple[float, float]  # lat, lon\n",
    "    card_type: str\n",
    "    is_international: bool\n",
    "\n",
    "class FraudFeatureEngine:\n",
    "    \"\"\"\n",
    "    Feature engineering for fraud detection.\n",
    "    \n",
    "    Feature categories:\n",
    "    1. Transaction features (amount, time, location)\n",
    "    2. User behavior features (velocity, patterns)\n",
    "    3. Device/network features (device fingerprint, IP)\n",
    "    4. Merchant features (risk score, category)\n",
    "    5. Graph features (connections, clusters)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_history = defaultdict(list)\n",
    "        self.device_history = defaultdict(list)\n",
    "    \n",
    "    def extract_features(self, txn: Transaction) -> Dict[str, float]:\n",
    "        \"\"\"Extract all features for a transaction.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Transaction features\n",
    "        features['amount'] = txn.amount\n",
    "        features['amount_log'] = np.log1p(txn.amount)\n",
    "        features['hour_of_day'] = (txn.timestamp % 86400) / 3600\n",
    "        features['is_night'] = 1 if features['hour_of_day'] < 6 or features['hour_of_day'] > 22 else 0\n",
    "        features['is_international'] = float(txn.is_international)\n",
    "        \n",
    "        # User velocity features\n",
    "        user_txns = self.user_history[txn.user_id]\n",
    "        features['txn_count_1h'] = self._count_recent(user_txns, txn.timestamp, 3600)\n",
    "        features['txn_count_24h'] = self._count_recent(user_txns, txn.timestamp, 86400)\n",
    "        features['amount_sum_24h'] = self._sum_recent_amounts(user_txns, txn.timestamp, 86400)\n",
    "        \n",
    "        # Deviation from user's normal behavior\n",
    "        if user_txns:\n",
    "            avg_amount = np.mean([t['amount'] for t in user_txns[-100:]])\n",
    "            features['amount_deviation'] = (txn.amount - avg_amount) / (avg_amount + 1)\n",
    "        else:\n",
    "            features['amount_deviation'] = 0\n",
    "        \n",
    "        # Device features\n",
    "        device_txns = self.device_history[txn.device_id]\n",
    "        features['device_user_count'] = len(set(t['user_id'] for t in device_txns))\n",
    "        features['new_device'] = 1 if txn.device_id not in self.device_history else 0\n",
    "        \n",
    "        # Update history\n",
    "        self._update_history(txn)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _count_recent(self, history: List[Dict], timestamp: float, \n",
    "                      window_seconds: float) -> int:\n",
    "        \"\"\"Count transactions in time window.\"\"\"\n",
    "        cutoff = timestamp - window_seconds\n",
    "        return sum(1 for t in history if t['timestamp'] > cutoff)\n",
    "    \n",
    "    def _sum_recent_amounts(self, history: List[Dict], timestamp: float,\n",
    "                           window_seconds: float) -> float:\n",
    "        \"\"\"Sum transaction amounts in time window.\"\"\"\n",
    "        cutoff = timestamp - window_seconds\n",
    "        return sum(t['amount'] for t in history if t['timestamp'] > cutoff)\n",
    "    \n",
    "    def _update_history(self, txn: Transaction):\n",
    "        \"\"\"Update user and device history.\"\"\"\n",
    "        record = {\n",
    "            'timestamp': txn.timestamp,\n",
    "            'amount': txn.amount,\n",
    "            'user_id': txn.user_id\n",
    "        }\n",
    "        self.user_history[txn.user_id].append(record)\n",
    "        self.device_history[txn.device_id].append(record)\n",
    "\n",
    "class FraudDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural fraud detection model.\n",
    "    \n",
    "    Handles imbalanced data with:\n",
    "    - Focal loss\n",
    "    - Class weights\n",
    "    - SMOTE/oversampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, hidden_dims: List[int] = [128, 64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = feature_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(prev_dim, 1)\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict fraud probability.\"\"\"\n",
    "        hidden = self.network(features)\n",
    "        logits = self.classifier(hidden)\n",
    "        return torch.sigmoid(logits).squeeze(-1)\n",
    "    \n",
    "    def focal_loss(self, predictions: torch.Tensor, targets: torch.Tensor,\n",
    "                   gamma: float = 2.0, alpha: float = 0.75) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Focal Loss for imbalanced classification.\n",
    "        \n",
    "        Down-weights easy examples, focuses on hard ones.\n",
    "        \"\"\"\n",
    "        bce = F.binary_cross_entropy(predictions, targets.float(), reduction='none')\n",
    "        \n",
    "        # Focal weight\n",
    "        pt = torch.where(targets == 1, predictions, 1 - predictions)\n",
    "        focal_weight = (1 - pt) ** gamma\n",
    "        \n",
    "        # Class weight (alpha for positive class)\n",
    "        class_weight = torch.where(targets == 1, alpha, 1 - alpha)\n",
    "        \n",
    "        loss = focal_weight * class_weight * bce\n",
    "        return loss.mean()\n",
    "\n",
    "def compute_fraud_metrics(y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                          threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute fraud detection metrics.\n",
    "    \n",
    "    Key metrics:\n",
    "    - Precision: Of predicted frauds, how many are real?\n",
    "    - Recall: Of real frauds, how many did we catch?\n",
    "    - False Positive Rate: Good transactions flagged as fraud\n",
    "    \"\"\"\n",
    "    y_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((y_binary == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_binary == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_binary == 0) & (y_true == 1))\n",
    "    tn = np.sum((y_binary == 0) & (y_true == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    # For imbalanced data, use PR-AUC not ROC-AUC\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0,\n",
    "        'false_positive_rate': fpr,\n",
    "        'fraud_caught': recall,\n",
    "        'good_txn_blocked': fpr\n",
    "    }\n",
    "\n",
    "# Example\n",
    "print(\"\\n=== Fraud Detection System ===\")\n",
    "feature_engine = FraudFeatureEngine()\n",
    "\n",
    "txn = Transaction(\n",
    "    transaction_id=\"txn_001\",\n",
    "    user_id=\"user_123\",\n",
    "    amount=999.99,\n",
    "    merchant_id=\"merchant_456\",\n",
    "    timestamp=time.time(),\n",
    "    device_id=\"device_789\",\n",
    "    ip_address=\"192.168.1.1\",\n",
    "    location=(37.7749, -122.4194),\n",
    "    card_type=\"visa\",\n",
    "    is_international=False\n",
    ")\n",
    "\n",
    "features = feature_engine.extract_features(txn)\n",
    "print(f\"Extracted {len(features)} features\")\n",
    "print(f\"Sample features: amount={features['amount']}, amount_deviation={features['amount_deviation']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Ads / CTR Prediction\n",
    "\n",
    "## Click-Through Rate Prediction\n",
    "Core problem for Meta, Google, TikTok ads systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepFM: Deep Factorization Machine for CTR prediction.\n",
    "    \n",
    "    Combines:\n",
    "    - FM (Factorization Machine): Captures 2nd-order feature interactions\n",
    "    - DNN: Captures high-order feature interactions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_fields: int,\n",
    "                 embedding_dim: int = 8,\n",
    "                 hidden_dims: List[int] = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_fields = num_fields\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embeddings for each field\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(10000, embedding_dim)  # Assume max 10k values per field\n",
    "            for _ in range(num_fields)\n",
    "        ])\n",
    "        \n",
    "        # Linear part (1st order)\n",
    "        self.linear = nn.Linear(num_fields * embedding_dim, 1)\n",
    "        \n",
    "        # DNN part (high-order)\n",
    "        dnn_input_dim = num_fields * embedding_dim\n",
    "        layers = []\n",
    "        prev_dim = dnn_input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.dnn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, field_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        field_indices: (batch_size, num_fields) - indices for each field\n",
    "        \"\"\"\n",
    "        # Get embeddings for each field\n",
    "        embedded = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            embedded.append(emb(field_indices[:, i]))\n",
    "        \n",
    "        # Stack: (batch_size, num_fields, embedding_dim)\n",
    "        embedded = torch.stack(embedded, dim=1)\n",
    "        \n",
    "        # FM: 2nd order interactions\n",
    "        # sum_square - square_sum trick\n",
    "        sum_embed = torch.sum(embedded, dim=1)\n",
    "        sum_square = torch.sum(sum_embed ** 2, dim=-1, keepdim=True)\n",
    "        square_sum = torch.sum(torch.sum(embedded ** 2, dim=-1), dim=-1, keepdim=True)\n",
    "        fm_out = 0.5 * (sum_square - square_sum)\n",
    "        \n",
    "        # Flatten for linear and DNN\n",
    "        flat = embedded.view(embedded.size(0), -1)\n",
    "        \n",
    "        # Linear (1st order)\n",
    "        linear_out = self.linear(flat)\n",
    "        \n",
    "        # DNN (high-order)\n",
    "        dnn_out = self.dnn(flat)\n",
    "        \n",
    "        # Combine\n",
    "        logits = linear_out + fm_out + dnn_out\n",
    "        return torch.sigmoid(logits).squeeze(-1)\n",
    "\n",
    "class CTRCalibration:\n",
    "    \"\"\"\n",
    "    Calibrate CTR predictions for accurate bidding.\n",
    "    \n",
    "    Predicted CTR should match actual CTR.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_bins: int = 10):\n",
    "        self.num_bins = num_bins\n",
    "        self.calibration_map = {}\n",
    "    \n",
    "    def fit(self, predictions: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Fit calibration using Platt scaling or isotonic regression.\"\"\"\n",
    "        # Bin predictions\n",
    "        bins = np.linspace(0, 1, self.num_bins + 1)\n",
    "        \n",
    "        for i in range(self.num_bins):\n",
    "            mask = (predictions >= bins[i]) & (predictions < bins[i + 1])\n",
    "            if np.sum(mask) > 0:\n",
    "                predicted_ctr = predictions[mask].mean()\n",
    "                actual_ctr = labels[mask].mean()\n",
    "                self.calibration_map[i] = actual_ctr / (predicted_ctr + 1e-10)\n",
    "    \n",
    "    def calibrate(self, predictions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply calibration.\"\"\"\n",
    "        bins = np.linspace(0, 1, self.num_bins + 1)\n",
    "        calibrated = predictions.copy()\n",
    "        \n",
    "        for i in range(self.num_bins):\n",
    "            mask = (predictions >= bins[i]) & (predictions < bins[i + 1])\n",
    "            if i in self.calibration_map:\n",
    "                calibrated[mask] *= self.calibration_map[i]\n",
    "        \n",
    "        return np.clip(calibrated, 0, 1)\n",
    "\n",
    "print(\"\\n=== CTR Prediction (DeepFM) ===\")\n",
    "model = DeepFM(num_fields=10, embedding_dim=8)\n",
    "\n",
    "# Sample batch\n",
    "batch = torch.randint(0, 1000, (32, 10))  # 10 categorical fields\n",
    "ctr_pred = model(batch)\n",
    "print(f\"Predicted CTR: mean={ctr_pred.mean().item():.4f}, std={ctr_pred.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: System Design Interview Framework\n",
    "\n",
    "## Template for ML System Design Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLSystemDesignTemplate:\n",
    "    \"\"\"\n",
    "    Framework for answering ML system design questions.\n",
    "    \n",
    "    Time allocation (45 min interview):\n",
    "    - Clarification: 5 min (10%)\n",
    "    - High-level design: 10 min (20%)\n",
    "    - Deep dive: 20 min (50%)\n",
    "    - Evaluation: 10 min (20%)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def step1_clarify() -> Dict[str, List[str]]:\n",
    "        \"\"\"Questions to ask the interviewer.\"\"\"\n",
    "        return {\n",
    "            'business': [\n",
    "                'What is the primary business objective?',\n",
    "                'What metrics matter most? (engagement, revenue, safety)',\n",
    "                'Are there any constraints? (latency, cost, fairness)'\n",
    "            ],\n",
    "            'scale': [\n",
    "                'How many users/items/requests?',\n",
    "                'What is the expected QPS?',\n",
    "                'Real-time or batch predictions?'\n",
    "            ],\n",
    "            'data': [\n",
    "                'What data is available?',\n",
    "                'How much labeled data do we have?',\n",
    "                'What is the label distribution? (imbalanced?)'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def step2_high_level_design() -> Dict[str, str]:\n",
    "        \"\"\"Components of the system.\"\"\"\n",
    "        return {\n",
    "            'data_pipeline': 'Data collection -> Processing -> Feature store',\n",
    "            'training_pipeline': 'Feature engineering -> Model training -> Evaluation',\n",
    "            'serving_pipeline': 'Feature retrieval -> Inference -> Post-processing',\n",
    "            'monitoring': 'Metrics collection -> Drift detection -> Alerting'\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def step3_deep_dive() -> Dict[str, List[str]]:\n",
    "        \"\"\"Deep dive topics.\"\"\"\n",
    "        return {\n",
    "            'features': [\n",
    "                'User features (demographics, history, preferences)',\n",
    "                'Item features (content, popularity, recency)',\n",
    "                'Context features (time, device, location)',\n",
    "                'Cross features (user-item interactions)'\n",
    "            ],\n",
    "            'model': [\n",
    "                'Model architecture (two-tower, transformer, GNN)',\n",
    "                'Loss function (cross-entropy, focal, pairwise)',\n",
    "                'Multi-task learning (multiple objectives)',\n",
    "                'Training strategy (in-batch negatives, hard negatives)'\n",
    "            ],\n",
    "            'serving': [\n",
    "                'Candidate generation (ANN, collaborative filtering)',\n",
    "                'Ranking (heavy model on top-k candidates)',\n",
    "                'Re-ranking (diversity, business rules)',\n",
    "                'Caching (user/item embeddings, frequent predictions)'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def step4_evaluation() -> Dict[str, List[str]]:\n",
    "        \"\"\"Evaluation metrics and testing.\"\"\"\n",
    "        return {\n",
    "            'offline_metrics': [\n",
    "                'AUC-ROC, PR-AUC (classification)',\n",
    "                'NDCG, MRR (ranking)',\n",
    "                'Precision@k, Recall@k (retrieval)',\n",
    "                'Calibration (predicted vs actual CTR)'\n",
    "            ],\n",
    "            'online_testing': [\n",
    "                'A/B testing with statistical significance',\n",
    "                'Interleaving experiments for ranking',\n",
    "                'Holdout groups for long-term effects',\n",
    "                'Guardrail metrics (safety, latency)'\n",
    "            ],\n",
    "            'monitoring': [\n",
    "                'Feature drift (PSI, KS test)',\n",
    "                'Prediction drift (distribution shift)',\n",
    "                'Performance degradation (accuracy over time)',\n",
    "                'Latency (P50, P95, P99)'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Print the framework\n",
    "print(\"\\n=== ML System Design Interview Framework ===\")\n",
    "template = MLSystemDesignTemplate()\n",
    "\n",
    "print(\"\\n1. CLARIFY (5 min)\")\n",
    "for category, questions in template.step1_clarify().items():\n",
    "    print(f\"  {category.upper()}:\")\n",
    "    for q in questions[:2]:\n",
    "        print(f\"    - {q}\")\n",
    "\n",
    "print(\"\\n2. HIGH-LEVEL DESIGN (10 min)\")\n",
    "for component, description in template.step2_high_level_design().items():\n",
    "    print(f\"  {component}: {description}\")\n",
    "\n",
    "print(\"\\n3. DEEP DIVE (20 min)\")\n",
    "for topic, points in template.step3_deep_dive().items():\n",
    "    print(f\"  {topic.upper()}: {points[0]}...\")\n",
    "\n",
    "print(\"\\n4. EVALUATION (10 min)\")\n",
    "for category, metrics in template.step4_evaluation().items():\n",
    "    print(f\"  {category}: {metrics[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Recommendation Systems:\n",
    "- Two-tower for candidate generation (embeddings + ANN)\n",
    "- Multi-task ranking for final scoring\n",
    "- In-batch negatives for efficient training\n",
    "\n",
    "### Search Ranking:\n",
    "- Query understanding is critical\n",
    "- Learning-to-rank (pointwise, pairwise, listwise)\n",
    "- NDCG is the primary metric\n",
    "\n",
    "### Fraud Detection:\n",
    "- Real-time feature engineering (velocity, device)\n",
    "- Handle imbalanced data (focal loss, class weights)\n",
    "- Precision-recall trade-off (not accuracy)\n",
    "\n",
    "### CTR Prediction:\n",
    "- Feature interactions matter (FM, DeepFM)\n",
    "- Calibration for accurate bidding\n",
    "- Multi-task learning for multiple objectives\n",
    "\n",
    "## FAANG Interview Questions\n",
    "\n",
    "**Q1: Design Instagram's Explore page.**\n",
    "- Two-tower for candidate generation (user-post embeddings)\n",
    "- Multi-task ranker (P(like), P(comment), P(share))\n",
    "- Diversity injection (different content types)\n",
    "- Cold-start for new users (popularity, demographics)\n",
    "\n",
    "**Q2: Design YouTube search ranking.**\n",
    "- Query understanding (intent, entities)\n",
    "- Two-stage retrieval (inverted index + semantic)\n",
    "- Learning-to-rank with NDCG optimization\n",
    "- Personalization (watch history, preferences)\n",
    "\n",
    "**Q3: Design fraud detection for Stripe.**\n",
    "- Real-time features (velocity, device fingerprint)\n",
    "- Graph features (fraud rings, connected accounts)\n",
    "- Focal loss for imbalanced data\n",
    "- Human-in-the-loop for edge cases\n",
    "\n",
    "**Q4: What metrics would you use for a recommendation system?**\n",
    "- Offline: Precision@k, Recall@k, NDCG, Hit Rate\n",
    "- Online: CTR, watch time, engagement rate\n",
    "- Business: Revenue, retention, DAU\n",
    "- Guardrails: Diversity, freshness, safety"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
