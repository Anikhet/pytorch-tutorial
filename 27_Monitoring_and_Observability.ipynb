{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and Observability for ML Systems\n",
    "\n",
    "## Overview\n",
    "Production monitoring for ML services with:\n",
    "- **Prometheus + Grafana**: Metrics collection and visualization\n",
    "- **Model Drift Detection**: Data and concept drift monitoring\n",
    "- **Structured Logging**: ELK stack, CloudWatch integration\n",
    "- **Alerting**: PagerDuty, Slack notifications\n",
    "- **Distributed Tracing**: OpenTelemetry for request flows\n",
    "\n",
    "## Why Monitoring Matters\n",
    "- **Detect Issues Early**: Catch drift before it impacts users\n",
    "- **SLA Compliance**: Track uptime, latency, throughput\n",
    "- **Cost Optimization**: Identify inefficient resource usage\n",
    "- **Root Cause Analysis**: Debug production incidents\n",
    "\n",
    "## Interview Focus\n",
    "- ML-specific metrics (drift, prediction distribution)\n",
    "- SLI/SLO/SLA definitions\n",
    "- Alerting best practices\n",
    "- Observability vs monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation\n",
    "# pip install prometheus-client evidently alibi-detect sentry-sdk opentelemetry-api\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "import json\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prometheus Metrics\n",
    "\n",
    "### Four Golden Signals for ML Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from prometheus_client import Counter, Histogram, Gauge, Summary, Info, generate_latest\n",
    "\n",
    "class MLMetrics:\n",
    "    \"\"\"Production metrics for ML inference service.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 1. Latency (how long requests take)\n",
    "        self.request_latency = Histogram(\n",
    "            'ml_request_latency_seconds',\n",
    "            'Request latency in seconds',\n",
    "            ['endpoint', 'model_version'],\n",
    "            buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]\n",
    "        )\n",
    "        \n",
    "        self.inference_latency = Histogram(\n",
    "            'ml_inference_latency_seconds',\n",
    "            'Model inference latency',\n",
    "            ['model_name', 'model_version'],\n",
    "            buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5]\n",
    "        )\n",
    "        \n",
    "        # 2. Traffic (requests per second)\n",
    "        self.request_count = Counter(\n",
    "            'ml_requests_total',\n",
    "            'Total number of requests',\n",
    "            ['endpoint', 'method', 'status_code', 'model_version']\n",
    "        )\n",
    "        \n",
    "        self.predictions_total = Counter(\n",
    "            'ml_predictions_total',\n",
    "            'Total predictions made',\n",
    "            ['model_name', 'model_version', 'prediction_class']\n",
    "        )\n",
    "        \n",
    "        # 3. Errors (failed requests)\n",
    "        self.error_count = Counter(\n",
    "            'ml_errors_total',\n",
    "            'Total errors',\n",
    "            ['error_type', 'endpoint']\n",
    "        )\n",
    "        \n",
    "        self.model_errors = Counter(\n",
    "            'ml_model_errors_total',\n",
    "            'Model-specific errors',\n",
    "            ['model_name', 'error_type']\n",
    "        )\n",
    "        \n",
    "        # 4. Saturation (resource utilization)\n",
    "        self.active_requests = Gauge(\n",
    "            'ml_active_requests',\n",
    "            'Number of active requests'\n",
    "        )\n",
    "        \n",
    "        self.queue_size = Gauge(\n",
    "            'ml_queue_size',\n",
    "            'Inference queue size'\n",
    "        )\n",
    "        \n",
    "        self.gpu_utilization = Gauge(\n",
    "            'ml_gpu_utilization_percent',\n",
    "            'GPU utilization percentage',\n",
    "            ['gpu_id']\n",
    "        )\n",
    "        \n",
    "        self.memory_usage = Gauge(\n",
    "            'ml_memory_usage_bytes',\n",
    "            'Memory usage in bytes',\n",
    "            ['type']  # 'model', 'cache', 'system'\n",
    "        )\n",
    "        \n",
    "        # ML-specific metrics\n",
    "        self.prediction_confidence = Histogram(\n",
    "            'ml_prediction_confidence',\n",
    "            'Prediction confidence scores',\n",
    "            ['model_name'],\n",
    "            buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]\n",
    "        )\n",
    "        \n",
    "        self.feature_values = Histogram(\n",
    "            'ml_feature_values',\n",
    "            'Feature value distributions',\n",
    "            ['feature_name'],\n",
    "            buckets=list(np.linspace(0, 100, 20))\n",
    "        )\n",
    "        \n",
    "        self.model_info = Info(\n",
    "            'ml_model_info',\n",
    "            'Model metadata'\n",
    "        )\n",
    "        \n",
    "        # Set model info\n",
    "        self.model_info.info({\n",
    "            'name': 'churn_predictor',\n",
    "            'version': '2.1.0',\n",
    "            'framework': 'pytorch',\n",
    "            'deployed_at': datetime.utcnow().isoformat()\n",
    "        })\n",
    "    \n",
    "    def record_request(self, endpoint: str, method: str, status_code: int, \n",
    "                      latency: float, model_version: str):\n",
    "        \"\"\"Record a request with all relevant metrics.\"\"\"\n",
    "        self.request_count.labels(\n",
    "            endpoint=endpoint,\n",
    "            method=method,\n",
    "            status_code=status_code,\n",
    "            model_version=model_version\n",
    "        ).inc()\n",
    "        \n",
    "        self.request_latency.labels(\n",
    "            endpoint=endpoint,\n",
    "            model_version=model_version\n",
    "        ).observe(latency)\n",
    "    \n",
    "    def record_prediction(self, model_name: str, model_version: str, \n",
    "                         prediction: int, confidence: float, \n",
    "                         inference_time: float):\n",
    "        \"\"\"Record a prediction with metadata.\"\"\"\n",
    "        self.predictions_total.labels(\n",
    "            model_name=model_name,\n",
    "            model_version=model_version,\n",
    "            prediction_class=str(prediction)\n",
    "        ).inc()\n",
    "        \n",
    "        self.prediction_confidence.labels(\n",
    "            model_name=model_name\n",
    "        ).observe(confidence)\n",
    "        \n",
    "        self.inference_latency.labels(\n",
    "            model_name=model_name,\n",
    "            model_version=model_version\n",
    "        ).observe(inference_time)\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = MLMetrics()\n",
    "\n",
    "# Simulate some requests\n",
    "print(\"=== Recording Simulated Metrics ===\")\n",
    "for i in range(100):\n",
    "    metrics.record_request(\n",
    "        endpoint='/predict',\n",
    "        method='POST',\n",
    "        status_code=200 if i < 95 else 500,\n",
    "        latency=np.random.uniform(0.01, 0.1),\n",
    "        model_version='v2.1'\n",
    "    )\n",
    "    \n",
    "    metrics.record_prediction(\n",
    "        model_name='churn_predictor',\n",
    "        model_version='v2.1',\n",
    "        prediction=np.random.choice([0, 1]),\n",
    "        confidence=np.random.uniform(0.7, 0.99),\n",
    "        inference_time=np.random.uniform(0.001, 0.01)\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Recorded 100 requests\")\n",
    "print(\"\\nMetrics available at /metrics endpoint\")\n",
    "print(\"Prometheus scrape config:\")\n",
    "print(\"\"\"scrape_configs:\n",
    "  - job_name: 'ml_service'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:8000']\n",
    "    scrape_interval: 15s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Drift Detection\n",
    "\n",
    "### Data Drift and Concept Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"Detect data and concept drift in production.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data: pd.DataFrame, \n",
    "                 threshold_psi: float = 0.2,\n",
    "                 threshold_ks: float = 0.05):\n",
    "        self.reference_data = reference_data\n",
    "        self.threshold_psi = threshold_psi  # Population Stability Index\n",
    "        self.threshold_ks = threshold_ks     # Kolmogorov-Smirnov p-value\n",
    "        \n",
    "        # Store reference statistics\n",
    "        self.reference_stats = self._compute_statistics(reference_data)\n",
    "    \n",
    "    def _compute_statistics(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compute reference statistics for each feature.\"\"\"\n",
    "        stats = {}\n",
    "        for col in data.columns:\n",
    "            if pd.api.types.is_numeric_dtype(data[col]):\n",
    "                stats[col] = {\n",
    "                    'mean': data[col].mean(),\n",
    "                    'std': data[col].std(),\n",
    "                    'min': data[col].min(),\n",
    "                    'max': data[col].max(),\n",
    "                    'quantiles': data[col].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def calculate_psi(self, reference: np.ndarray, current: np.ndarray, \n",
    "                     bins: int = 10) -> float:\n",
    "        \"\"\"Population Stability Index for distribution drift.\"\"\"\n",
    "        # Create bins based on reference distribution\n",
    "        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
    "        breakpoints = np.unique(breakpoints)  # Remove duplicates\n",
    "        \n",
    "        # Calculate distribution for each dataset\n",
    "        ref_percents = np.histogram(reference, bins=breakpoints)[0] / len(reference)\n",
    "        cur_percents = np.histogram(current, bins=breakpoints)[0] / len(current)\n",
    "        \n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        ref_percents = np.where(ref_percents == 0, epsilon, ref_percents)\n",
    "        cur_percents = np.where(cur_percents == 0, epsilon, cur_percents)\n",
    "        \n",
    "        # PSI formula: sum((actual% - expected%) * ln(actual% / expected%))\n",
    "        psi = np.sum((cur_percents - ref_percents) * np.log(cur_percents / ref_percents))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def calculate_ks_statistic(self, reference: np.ndarray, current: np.ndarray):\n",
    "        \"\"\"Kolmogorov-Smirnov test for distribution similarity.\"\"\"\n",
    "        statistic, p_value = stats.ks_2samp(reference, current)\n",
    "        return {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < self.threshold_ks\n",
    "        }\n",
    "    \n",
    "    def detect_feature_drift(self, current_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Detect drift for each feature.\"\"\"\n",
    "        drift_report = {\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'features': {},\n",
    "            'overall_drift': False\n",
    "        }\n",
    "        \n",
    "        for col in self.reference_data.columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.reference_data[col]):\n",
    "                ref_values = self.reference_data[col].dropna().values\n",
    "                cur_values = current_data[col].dropna().values\n",
    "                \n",
    "                # PSI\n",
    "                psi = self.calculate_psi(ref_values, cur_values)\n",
    "                \n",
    "                # KS test\n",
    "                ks_result = self.calculate_ks_statistic(ref_values, cur_values)\n",
    "                \n",
    "                # Statistical comparison\n",
    "                ref_mean = ref_values.mean()\n",
    "                cur_mean = cur_values.mean()\n",
    "                mean_shift = abs(cur_mean - ref_mean) / (ref_values.std() + 1e-10)\n",
    "                \n",
    "                drift_detected = (\n",
    "                    psi > self.threshold_psi or \n",
    "                    ks_result['drift_detected'] or\n",
    "                    mean_shift > 3  # 3 sigma shift\n",
    "                )\n",
    "                \n",
    "                drift_report['features'][col] = {\n",
    "                    'psi': float(psi),\n",
    "                    'ks_statistic': float(ks_result['statistic']),\n",
    "                    'ks_p_value': float(ks_result['p_value']),\n",
    "                    'mean_shift_sigma': float(mean_shift),\n",
    "                    'drift_detected': drift_detected,\n",
    "                    'severity': self._get_drift_severity(psi)\n",
    "                }\n",
    "                \n",
    "                if drift_detected:\n",
    "                    drift_report['overall_drift'] = True\n",
    "        \n",
    "        return drift_report\n",
    "    \n",
    "    def _get_drift_severity(self, psi: float) -> str:\n",
    "        \"\"\"Classify drift severity based on PSI.\"\"\"\n",
    "        if psi < 0.1:\n",
    "            return \"none\"\n",
    "        elif psi < 0.2:\n",
    "            return \"low\"\n",
    "        elif psi < 0.5:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"high\"\n",
    "    \n",
    "    def detect_prediction_drift(self, reference_predictions: np.ndarray,\n",
    "                               current_predictions: np.ndarray) -> Dict:\n",
    "        \"\"\"Detect drift in prediction distributions.\"\"\"\n",
    "        # For classification: compare class distributions\n",
    "        ref_dist = np.bincount(reference_predictions) / len(reference_predictions)\n",
    "        cur_dist = np.bincount(current_predictions) / len(current_predictions)\n",
    "        \n",
    "        # Ensure same length\n",
    "        max_len = max(len(ref_dist), len(cur_dist))\n",
    "        ref_dist = np.pad(ref_dist, (0, max_len - len(ref_dist)))\n",
    "        cur_dist = np.pad(cur_dist, (0, max_len - len(cur_dist)))\n",
    "        \n",
    "        # Jensen-Shannon divergence\n",
    "        js_divergence = jensenshannon(ref_dist, cur_dist)\n",
    "        \n",
    "        return {\n",
    "            'js_divergence': float(js_divergence),\n",
    "            'drift_detected': js_divergence > 0.1,\n",
    "            'reference_distribution': ref_dist.tolist(),\n",
    "            'current_distribution': cur_dist.tolist()\n",
    "        }\n",
    "\n",
    "# Example: Detect drift\n",
    "print(\"\\n=== Drift Detection Example ===\")\n",
    "\n",
    "# Reference data (training distribution)\n",
    "reference_data = pd.DataFrame({\n",
    "    'feature1': np.random.normal(50, 10, 1000),\n",
    "    'feature2': np.random.normal(100, 20, 1000),\n",
    "    'feature3': np.random.exponential(5, 1000)\n",
    "})\n",
    "\n",
    "# Current data (production with drift)\n",
    "current_data = pd.DataFrame({\n",
    "    'feature1': np.random.normal(55, 12, 500),  # Mean shift\n",
    "    'feature2': np.random.normal(100, 20, 500),  # No drift\n",
    "    'feature3': np.random.exponential(8, 500)    # Distribution change\n",
    "})\n",
    "\n",
    "detector = DriftDetector(reference_data)\n",
    "drift_report = detector.detect_feature_drift(current_data)\n",
    "\n",
    "print(f\"\\nOverall drift detected: {drift_report['overall_drift']}\")\n",
    "print(\"\\nFeature-level drift:\")\n",
    "for feature, stats in drift_report['features'].items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  PSI: {stats['psi']:.4f} ({stats['severity']})\")\n",
    "    print(f\"  KS p-value: {stats['ks_p_value']:.4f}\")\n",
    "    print(f\"  Drift: {'‚ö†Ô∏è YES' if stats['drift_detected'] else '‚úÖ NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Structured Logging\n",
    "\n",
    "### Production-Grade Logging System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "class StructuredLogger:\n",
    "    \"\"\"JSON-formatted structured logging for ML services.\"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str, environment: str = \"production\"):\n",
    "        self.service_name = service_name\n",
    "        self.environment = environment\n",
    "        self.logger = self._setup_logger()\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        \"\"\"Configure structured JSON logger.\"\"\"\n",
    "        logger = logging.getLogger(self.service_name)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # JSON formatter\n",
    "        log_handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = jsonlogger.JsonFormatter(\n",
    "            '%(timestamp)s %(level)s %(name)s %(message)s',\n",
    "            timestamp=True\n",
    "        )\n",
    "        log_handler.setFormatter(formatter)\n",
    "        logger.addHandler(log_handler)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    def log_request(self, request_id: str, endpoint: str, method: str, \n",
    "                   user_id: Optional[str] = None, **kwargs):\n",
    "        \"\"\"Log API request.\"\"\"\n",
    "        self.logger.info(\n",
    "            \"API request received\",\n",
    "            extra={\n",
    "                'request_id': request_id,\n",
    "                'endpoint': endpoint,\n",
    "                'method': method,\n",
    "                'user_id': user_id,\n",
    "                'service': self.service_name,\n",
    "                'environment': self.environment,\n",
    "                **kwargs\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def log_prediction(self, request_id: str, model_name: str, model_version: str,\n",
    "                      input_hash: str, prediction: Any, confidence: float,\n",
    "                      latency_ms: float):\n",
    "        \"\"\"Log model prediction with full context.\"\"\"\n",
    "        self.logger.info(\n",
    "            \"Model prediction\",\n",
    "            extra={\n",
    "                'request_id': request_id,\n",
    "                'model_name': model_name,\n",
    "                'model_version': model_version,\n",
    "                'input_hash': input_hash,\n",
    "                'prediction': str(prediction),\n",
    "                'confidence': confidence,\n",
    "                'latency_ms': latency_ms,\n",
    "                'service': self.service_name,\n",
    "                'environment': self.environment\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def log_error(self, request_id: str, error_type: str, error_message: str,\n",
    "                 stack_trace: Optional[str] = None, **kwargs):\n",
    "        \"\"\"Log error with context.\"\"\"\n",
    "        self.logger.error(\n",
    "            \"Error occurred\",\n",
    "            extra={\n",
    "                'request_id': request_id,\n",
    "                'error_type': error_type,\n",
    "                'error_message': error_message,\n",
    "                'stack_trace': stack_trace,\n",
    "                'service': self.service_name,\n",
    "                'environment': self.environment,\n",
    "                **kwargs\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def log_drift_alert(self, feature_name: str, psi: float, severity: str,\n",
    "                       drift_report: Dict):\n",
    "        \"\"\"Log drift detection alert.\"\"\"\n",
    "        self.logger.warning(\n",
    "            \"Data drift detected\",\n",
    "            extra={\n",
    "                'alert_type': 'drift_detection',\n",
    "                'feature_name': feature_name,\n",
    "                'psi': psi,\n",
    "                'severity': severity,\n",
    "                'drift_report': drift_report,\n",
    "                'service': self.service_name,\n",
    "                'environment': self.environment\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def log_model_update(self, old_version: str, new_version: str,\n",
    "                        deployment_strategy: str, metrics: Dict):\n",
    "        \"\"\"Log model deployment.\"\"\"\n",
    "        self.logger.info(\n",
    "            \"Model updated\",\n",
    "            extra={\n",
    "                'event_type': 'model_deployment',\n",
    "                'old_version': old_version,\n",
    "                'new_version': new_version,\n",
    "                'deployment_strategy': deployment_strategy,\n",
    "                'metrics': metrics,\n",
    "                'service': self.service_name,\n",
    "                'environment': self.environment\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n=== Structured Logging Example ===\")\n",
    "log = StructuredLogger('ml_inference_service', 'production')\n",
    "\n",
    "# Log request\n",
    "log.log_request(\n",
    "    request_id='req-12345',\n",
    "    endpoint='/predict',\n",
    "    method='POST',\n",
    "    user_id='user-789'\n",
    ")\n",
    "\n",
    "# Log prediction\n",
    "log.log_prediction(\n",
    "    request_id='req-12345',\n",
    "    model_name='churn_predictor',\n",
    "    model_version='v2.1',\n",
    "    input_hash='abc123',\n",
    "    prediction=1,\n",
    "    confidence=0.92,\n",
    "    latency_ms=15.5\n",
    ")\n",
    "\n",
    "# Log drift alert\n",
    "log.log_drift_alert(\n",
    "    feature_name='user_age',\n",
    "    psi=0.25,\n",
    "    severity='medium',\n",
    "    drift_report={'details': 'Mean shifted 2.5 sigma'}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Structured logs sent to stdout (ship to ELK/CloudWatch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Alerting System\n",
    "\n",
    "### Smart Alerts with Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class AlertSeverity(Enum):\n",
    "    INFO = \"info\"\n",
    "    WARNING = \"warning\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    \"\"\"Alert definition.\"\"\"\n",
    "    name: str\n",
    "    severity: AlertSeverity\n",
    "    message: str\n",
    "    metric: str\n",
    "    threshold: float\n",
    "    current_value: float\n",
    "    timestamp: datetime\n",
    "    runbook_url: Optional[str] = None\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"Production alerting system with deduplication.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_alerts = {}\n",
    "        self.alert_history = deque(maxlen=1000)\n",
    "        self.rules = self._define_alert_rules()\n",
    "    \n",
    "    def _define_alert_rules(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Define alerting rules and thresholds.\"\"\"\n",
    "        return {\n",
    "            'high_error_rate': {\n",
    "                'metric': 'error_rate',\n",
    "                'threshold': 0.05,  # 5% error rate\n",
    "                'severity': AlertSeverity.CRITICAL,\n",
    "                'message': 'Error rate exceeded 5%',\n",
    "                'runbook': 'https://runbooks.company.com/high-error-rate'\n",
    "            },\n",
    "            'high_latency': {\n",
    "                'metric': 'latency_p95',\n",
    "                'threshold': 100,  # 100ms\n",
    "                'severity': AlertSeverity.WARNING,\n",
    "                'message': 'P95 latency exceeded 100ms',\n",
    "                'runbook': 'https://runbooks.company.com/high-latency'\n",
    "            },\n",
    "            'data_drift': {\n",
    "                'metric': 'psi',\n",
    "                'threshold': 0.2,\n",
    "                'severity': AlertSeverity.WARNING,\n",
    "                'message': 'Significant data drift detected',\n",
    "                'runbook': 'https://runbooks.company.com/data-drift'\n",
    "            },\n",
    "            'prediction_drift': {\n",
    "                'metric': 'prediction_distribution_shift',\n",
    "                'threshold': 0.15,\n",
    "                'severity': AlertSeverity.WARNING,\n",
    "                'message': 'Prediction distribution shifted',\n",
    "                'runbook': 'https://runbooks.company.com/prediction-drift'\n",
    "            },\n",
    "            'low_confidence': {\n",
    "                'metric': 'avg_confidence',\n",
    "                'threshold': 0.7,\n",
    "                'severity': AlertSeverity.WARNING,\n",
    "                'message': 'Average prediction confidence below 70%',\n",
    "                'runbook': 'https://runbooks.company.com/low-confidence'\n",
    "            },\n",
    "            'model_failure': {\n",
    "                'metric': 'model_load_failures',\n",
    "                'threshold': 1,\n",
    "                'severity': AlertSeverity.CRITICAL,\n",
    "                'message': 'Model failed to load',\n",
    "                'runbook': 'https://runbooks.company.com/model-failure'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def check_threshold(self, rule_name: str, current_value: float) -> Optional[Alert]:\n",
    "        \"\"\"Check if metric exceeds threshold.\"\"\"\n",
    "        if rule_name not in self.rules:\n",
    "            return None\n",
    "        \n",
    "        rule = self.rules[rule_name]\n",
    "        \n",
    "        # Check threshold based on metric type\n",
    "        threshold_exceeded = False\n",
    "        if rule['metric'] in ['error_rate', 'latency_p95', 'psi', 'prediction_distribution_shift']:\n",
    "            threshold_exceeded = current_value > rule['threshold']\n",
    "        elif rule['metric'] == 'avg_confidence':\n",
    "            threshold_exceeded = current_value < rule['threshold']\n",
    "        elif rule['metric'] == 'model_load_failures':\n",
    "            threshold_exceeded = current_value >= rule['threshold']\n",
    "        \n",
    "        if threshold_exceeded:\n",
    "            alert = Alert(\n",
    "                name=rule_name,\n",
    "                severity=rule['severity'],\n",
    "                message=rule['message'],\n",
    "                metric=rule['metric'],\n",
    "                threshold=rule['threshold'],\n",
    "                current_value=current_value,\n",
    "                timestamp=datetime.utcnow(),\n",
    "                runbook_url=rule.get('runbook')\n",
    "            )\n",
    "            \n",
    "            # Deduplication: only fire if not already active\n",
    "            if rule_name not in self.active_alerts:\n",
    "                self.active_alerts[rule_name] = alert\n",
    "                self.alert_history.append(alert)\n",
    "                self._send_alert(alert)\n",
    "                return alert\n",
    "        else:\n",
    "            # Clear alert if it was active\n",
    "            if rule_name in self.active_alerts:\n",
    "                del self.active_alerts[rule_name]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _send_alert(self, alert: Alert):\n",
    "        \"\"\"Send alert to notification channels.\"\"\"\n",
    "        # In production: send to PagerDuty, Slack, email, etc.\n",
    "        logger.warning(\n",
    "            f\"üö® ALERT [{alert.severity.value.upper()}]: {alert.message}\",\n",
    "            extra={\n",
    "                'alert_name': alert.name,\n",
    "                'metric': alert.metric,\n",
    "                'threshold': alert.threshold,\n",
    "                'current_value': alert.current_value,\n",
    "                'runbook': alert.runbook_url\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def get_active_alerts(self) -> List[Alert]:\n",
    "        \"\"\"Get all currently active alerts.\"\"\"\n",
    "        return list(self.active_alerts.values())\n",
    "    \n",
    "    def get_alert_summary(self) -> Dict:\n",
    "        \"\"\"Get alert summary for dashboards.\"\"\"\n",
    "        return {\n",
    "            'active_alerts': len(self.active_alerts),\n",
    "            'critical_count': sum(1 for a in self.active_alerts.values() \n",
    "                                 if a.severity == AlertSeverity.CRITICAL),\n",
    "            'warning_count': sum(1 for a in self.active_alerts.values() \n",
    "                                if a.severity == AlertSeverity.WARNING),\n",
    "            'total_alerts_24h': len(self.alert_history)\n",
    "        }\n",
    "\n",
    "# Example: Alerting\n",
    "print(\"\\n=== Alerting System Example ===\")\n",
    "alert_manager = AlertManager()\n",
    "\n",
    "# Simulate metrics\n",
    "test_metrics = [\n",
    "    ('high_error_rate', 0.08),      # Triggers alert\n",
    "    ('high_latency', 45),            # No alert\n",
    "    ('data_drift', 0.25),            # Triggers alert\n",
    "    ('low_confidence', 0.65),        # Triggers alert\n",
    "]\n",
    "\n",
    "for rule_name, value in test_metrics:\n",
    "    alert = alert_manager.check_threshold(rule_name, value)\n",
    "    if alert:\n",
    "        print(f\"\\n‚ö†Ô∏è  Alert fired: {alert.name}\")\n",
    "        print(f\"   Severity: {alert.severity.value}\")\n",
    "        print(f\"   Message: {alert.message}\")\n",
    "        print(f\"   Current: {alert.current_value}, Threshold: {alert.threshold}\")\n",
    "        print(f\"   Runbook: {alert.runbook_url}\")\n",
    "\n",
    "summary = alert_manager.get_alert_summary()\n",
    "print(f\"\\nAlert summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: SLI/SLO/SLA Framework\n",
    "\n",
    "### Service Level Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SLOMonitor:\n",
    "    \"\"\"Track SLIs and SLOs for ML service.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # SLOs (Service Level Objectives)\n",
    "        self.slos = {\n",
    "            'availability': {\n",
    "                'target': 99.9,  # 99.9% uptime\n",
    "                'description': 'Service available for requests',\n",
    "                'error_budget_minutes': (100 - 99.9) * 365 * 24 * 60 / 100  # ~43.8 min/month\n",
    "            },\n",
    "            'latency_p95': {\n",
    "                'target': 50,  # 50ms P95 latency\n",
    "                'description': '95th percentile response time',\n",
    "                'unit': 'milliseconds'\n",
    "            },\n",
    "            'latency_p99': {\n",
    "                'target': 100,  # 100ms P99 latency\n",
    "                'description': '99th percentile response time',\n",
    "                'unit': 'milliseconds'\n",
    "            },\n",
    "            'accuracy': {\n",
    "                'target': 90.0,  # 90% accuracy\n",
    "                'description': 'Model prediction accuracy',\n",
    "                'unit': 'percent'\n",
    "            },\n",
    "            'throughput': {\n",
    "                'target': 1000,  # 1000 QPS\n",
    "                'description': 'Queries per second',\n",
    "                'unit': 'qps'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Track SLIs (Service Level Indicators)\n",
    "        self.measurements = defaultdict(list)\n",
    "    \n",
    "    def record_sli(self, metric: str, value: float):\n",
    "        \"\"\"Record an SLI measurement.\"\"\"\n",
    "        self.measurements[metric].append({\n",
    "            'value': value,\n",
    "            'timestamp': datetime.utcnow()\n",
    "        })\n",
    "    \n",
    "    def calculate_slo_compliance(self, metric: str, window_hours: int = 24) -> Dict:\n",
    "        \"\"\"Calculate SLO compliance over time window.\"\"\"\n",
    "        if metric not in self.slos:\n",
    "            return {'error': 'Unknown metric'}\n",
    "        \n",
    "        slo = self.slos[metric]\n",
    "        recent_measurements = [\n",
    "            m for m in self.measurements[metric]\n",
    "            if m['timestamp'] > datetime.utcnow() - timedelta(hours=window_hours)\n",
    "        ]\n",
    "        \n",
    "        if not recent_measurements:\n",
    "            return {'error': 'No measurements'}\n",
    "        \n",
    "        values = [m['value'] for m in recent_measurements]\n",
    "        \n",
    "        # Calculate compliance\n",
    "        if metric == 'availability':\n",
    "            # Availability: % of successful requests\n",
    "            compliance = (sum(values) / len(values))\n",
    "            met_slo = compliance >= slo['target']\n",
    "        elif 'latency' in metric:\n",
    "            # Latency: % of requests under threshold\n",
    "            under_threshold = sum(1 for v in values if v <= slo['target'])\n",
    "            compliance = (under_threshold / len(values)) * 100\n",
    "            met_slo = compliance >= 95  # 95% of requests should meet SLO\n",
    "        else:\n",
    "            # Other metrics: average should meet target\n",
    "            compliance = sum(values) / len(values)\n",
    "            met_slo = compliance >= slo['target']\n",
    "        \n",
    "        return {\n",
    "            'metric': metric,\n",
    "            'target': slo['target'],\n",
    "            'current': compliance,\n",
    "            'met_slo': met_slo,\n",
    "            'measurements': len(values),\n",
    "            'window_hours': window_hours\n",
    "        }\n",
    "    \n",
    "    def get_error_budget_remaining(self, metric: str = 'availability') -> Dict:\n",
    "        \"\"\"Calculate remaining error budget.\"\"\"\n",
    "        compliance = self.calculate_slo_compliance(metric)\n",
    "        \n",
    "        if 'error' in compliance:\n",
    "            return compliance\n",
    "        \n",
    "        slo = self.slos[metric]\n",
    "        budget_minutes = slo.get('error_budget_minutes', 0)\n",
    "        \n",
    "        # Calculate used budget\n",
    "        if metric == 'availability':\n",
    "            used_pct = 100 - compliance['current']\n",
    "            budget_used_pct = (used_pct / (100 - slo['target'])) * 100\n",
    "        else:\n",
    "            budget_used_pct = 0\n",
    "        \n",
    "        return {\n",
    "            'metric': metric,\n",
    "            'total_budget_minutes': budget_minutes,\n",
    "            'budget_used_pct': budget_used_pct,\n",
    "            'budget_remaining_pct': 100 - budget_used_pct,\n",
    "            'status': 'healthy' if budget_used_pct < 50 else 'warning' if budget_used_pct < 90 else 'critical'\n",
    "        }\n",
    "\n",
    "# Example: SLO monitoring\n",
    "print(\"\\n=== SLO Monitoring Example ===\")\n",
    "slo_monitor = SLOMonitor()\n",
    "\n",
    "# Simulate measurements\n",
    "for _ in range(1000):\n",
    "    slo_monitor.record_sli('availability', 1 if np.random.random() > 0.001 else 0)\n",
    "    slo_monitor.record_sli('latency_p95', np.random.uniform(30, 80))\n",
    "    slo_monitor.record_sli('accuracy', np.random.uniform(88, 94))\n",
    "\n",
    "# Check SLO compliance\n",
    "print(\"\\nSLO Compliance Report:\")\n",
    "for metric in ['availability', 'latency_p95', 'accuracy']:\n",
    "    compliance = slo_monitor.calculate_slo_compliance(metric)\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Target: {compliance['target']}\")\n",
    "    print(f\"  Current: {compliance['current']:.2f}\")\n",
    "    print(f\"  Status: {'‚úÖ PASS' if compliance['met_slo'] else '‚ùå FAIL'}\")\n",
    "\n",
    "# Error budget\n",
    "budget = slo_monitor.get_error_budget_remaining('availability')\n",
    "print(f\"\\nError Budget:\")\n",
    "print(f\"  Remaining: {budget['budget_remaining_pct']:.1f}%\")\n",
    "print(f\"  Status: {budget['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Monitoring Best Practices:\n",
    "1. ‚úÖ **Four Golden Signals**: Latency, Traffic, Errors, Saturation\n",
    "2. ‚úÖ **ML-Specific Metrics**: Drift, prediction distribution, confidence\n",
    "3. ‚úÖ **Structured Logging**: JSON logs for easy parsing and analysis\n",
    "4. ‚úÖ **Smart Alerting**: Thresholds, deduplication, runbooks\n",
    "5. ‚úÖ **SLO Tracking**: Define, measure, and enforce service levels\n",
    "6. ‚úÖ **Error Budgets**: Balance innovation and reliability\n",
    "7. ‚úÖ **Drift Detection**: Continuous monitoring of data quality\n",
    "8. ‚úÖ **Observability**: Understand system behavior from outputs\n",
    "\n",
    "### Production Checklist:\n",
    "- **Metrics**: Prometheus exporters on all services\n",
    "- **Dashboards**: Grafana with SLO tracking\n",
    "- **Logging**: Centralized (ELK, CloudWatch, Datadog)\n",
    "- **Tracing**: Distributed tracing for request flows\n",
    "- **Alerting**: PagerDuty/Slack with runbooks\n",
    "- **Drift**: Automated detection and alerts\n",
    "- **Feedback Loop**: Capture predictions and outcomes\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "1. **What's the difference between monitoring and observability?**\n",
    "   - Monitoring: Known unknowns (predefined metrics, alerts)\n",
    "   - Observability: Unknown unknowns (explore system behavior)\n",
    "   - Three pillars: Metrics, Logs, Traces\n",
    "\n",
    "2. **How do you detect model drift in production?**\n",
    "   - Data drift: PSI, KS test on input features\n",
    "   - Prediction drift: JS divergence on output distribution\n",
    "   - Concept drift: Track actual vs predicted outcomes\n",
    "   - Performance monitoring: Track accuracy, precision, recall over time\n",
    "\n",
    "3. **What are SLIs, SLOs, and SLAs?**\n",
    "   - SLI: Service Level Indicator (metric you measure)\n",
    "   - SLO: Service Level Objective (internal target, e.g., 99.9% uptime)\n",
    "   - SLA: Service Level Agreement (contract with users, penalty for breach)\n",
    "   - Error budget: (100% - SLO) = room for innovation/downtime\n",
    "\n",
    "4. **How do you set alert thresholds?**\n",
    "   - Based on SLOs and user impact\n",
    "   - Use percentiles (P95, P99) not averages\n",
    "   - Consider seasonality and trends\n",
    "   - Avoid alert fatigue (deduplication, grouping)\n",
    "   - Include runbooks for every alert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
