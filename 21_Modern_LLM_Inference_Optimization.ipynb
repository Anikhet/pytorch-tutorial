{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 21: Modern LLM Inference Optimization (2025)\n",
    "\n",
    "**The Reality**: Inference costs account for **90% of production ML expenses**. For LLMs serving millions of users, a 2x speedup can save millions of dollars annually.\n",
    "\n",
    "In 2024-2025, several breakthrough techniques emerged that fundamentally changed how we deploy LLMs:\n",
    "- **KV Cache Optimization**: Reducing memory bottlenecks\n",
    "- **Speculative Decoding**: 2-3x speedup with zero accuracy loss\n",
    "- **PagedAttention**: Treating GPU memory like OS virtual memory\n",
    "- **Continuous Batching**: No more waiting for slow requests\n",
    "\n",
    "This notebook teaches you the **state-of-the-art** techniques used by OpenAI, Anthropic, and Meta in production.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand why LLM inference is **memory-bandwidth-bound**\n",
    "2. Implement **KV caching** from scratch\n",
    "3. Learn **speculative decoding** (the technique behind GPT-4 Turbo's speed)\n",
    "4. Understand **PagedAttention** and **continuous batching**\n",
    "5. Deploy models with **vLLM** and compare to naive implementations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem - Why is LLM Inference Slow?\n",
    "\n",
    "### Vocabulary First\n",
    "\n",
    "- **Autoregressive Generation**: Generating one token at a time, where each new token depends on all previous tokens\n",
    "- **Memory-Bandwidth-Bound**: Performance limited by how fast we can load data from memory, not by computation speed\n",
    "- **KV Cache**: Storing Key and Value tensors from previous tokens to avoid recomputing them\n",
    "- **Time-to-First-Token (TTFT)**: Latency before the first output token appears\n",
    "- **Throughput**: Total tokens generated per second across all requests\n",
    "\n",
    "### The Core Issue\n",
    "\n",
    "**Problem**: In autoregressive decoding, we must load **ALL model weights and KV cache** just to generate **ONE token**.\n",
    "\n",
    "For a 7B parameter model:\n",
    "- Model weights: ~14GB (FP16)\n",
    "- KV cache per request (2048 tokens): ~2GB\n",
    "- Loading this to compute 1 token = huge waste!\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "Latency per token = (Model Size + KV Cache Size) / Memory Bandwidth\n",
    "```\n",
    "\n",
    "This is why optimizing memory access is more important than optimizing compute for LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Bandwidth: ~{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: KV Caching - The Foundation\n",
    "\n",
    "### Without KV Cache (Naive)\n",
    "For each new token, recompute attention for ALL previous tokens.\n",
    "\n",
    "### With KV Cache\n",
    "Store the Key and Value projections of past tokens and reuse them.\n",
    "\n",
    "**Speedup**: From O(nÂ²) to O(n) for sequence length n!\n",
    "\n",
    "Let's implement this from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttentionWithKVCache(nn.Module):\n",
    "    \"\"\"Attention mechanism with KV caching for efficient autoregressive generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.out = nn.Linear(dim, dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, dim]\n",
    "            kv_cache: Tuple of (k_cache, v_cache) from previous steps\n",
    "            use_cache: Whether to return updated cache\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output\n",
    "            new_cache: Updated (k, v) cache (if use_cache=True)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, T, head_dim)\n",
    "        \n",
    "        # If we have cached K, V, concatenate with new ones\n",
    "        if kv_cache is not None:\n",
    "            k_cache, v_cache = kv_cache\n",
    "            k = torch.cat([k_cache, k], dim=2)  # Concat along seq dimension\n",
    "            v = torch.cat([v_cache, v], dim=2)\n",
    "        \n",
    "        # Attention: Q @ K^T / sqrt(d)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Output: Attention @ V\n",
    "        out = attn @ v  # (B, heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.out(out)\n",
    "        \n",
    "        if use_cache:\n",
    "            return out, (k, v)\n",
    "        return out\n",
    "\n",
    "# Test it\n",
    "attn = SimpleAttentionWithKVCache(dim=512, num_heads=8).to(device)\n",
    "\n",
    "# Simulate autoregressive generation\n",
    "print(\"Simulating autoregressive generation with KV cache:\\n\")\n",
    "\n",
    "x = torch.randn(1, 1, 512).to(device)  # First token\n",
    "kv_cache = None\n",
    "\n",
    "for step in range(5):\n",
    "    with torch.no_grad():\n",
    "        out, kv_cache = attn(x, kv_cache=kv_cache, use_cache=True)\n",
    "    \n",
    "    k_cache, v_cache = kv_cache\n",
    "    print(f\"Step {step+1}: K cache shape = {k_cache.shape}\")\n",
    "    \n",
    "    # Next token (in reality, this would be from the model output)\n",
    "    x = torch.randn(1, 1, 512).to(device)\n",
    "\n",
    "print(\"\\nâœ… KV cache grows with each token, avoiding recomputation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache Optimization Techniques (2025)\n",
    "\n",
    "#### 1. **Quantization**\n",
    "Reduce KV cache from FP16 to INT8 or even INT4.\n",
    "- **Benefit**: 2-4x memory savings\n",
    "- **Cost**: Minimal accuracy loss (<1% in most cases)\n",
    "\n",
    "#### 2. **Entropy-Based Allocation**\n",
    "Some layers have more \"dispersed\" attention (high entropy) and need more cache.\n",
    "- Allocate larger KV budgets to high-entropy layers\n",
    "- Can save 4-5% memory with same performance\n",
    "\n",
    "#### 3. **PagedAttention** (vLLM)\n",
    "Treat KV cache like OS virtual memory:\n",
    "- Split cache into fixed-size \"pages\" (e.g., 64 tokens)\n",
    "- Store non-contiguously in GPU memory\n",
    "- Share pages across requests (e.g., system prompts)\n",
    "\n",
    "**Impact**: 2-4x higher throughput, near-zero memory waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual example: KV Cache Quantization\n",
    "\n",
    "def quantize_kv_cache(k, v, bits=8):\n",
    "    \"\"\"Quantize KV cache to save memory.\"\"\"\n",
    "    def quantize_tensor(x, bits):\n",
    "        # Find min/max\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        \n",
    "        # Quantize to N bits\n",
    "        scale = (x_max - x_min) / (2**bits - 1)\n",
    "        x_quant = ((x - x_min) / scale).round()\n",
    "        \n",
    "        return x_quant.to(torch.int8), scale, x_min\n",
    "    \n",
    "    k_quant, k_scale, k_min = quantize_tensor(k, bits)\n",
    "    v_quant, v_scale, v_min = quantize_tensor(v, bits)\n",
    "    \n",
    "    return (k_quant, k_scale, k_min), (v_quant, v_scale, v_min)\n",
    "\n",
    "def dequantize_tensor(x_quant, scale, x_min):\n",
    "    \"\"\"Dequantize back to FP16.\"\"\"\n",
    "    return x_quant.float() * scale + x_min\n",
    "\n",
    "# Test\n",
    "k = torch.randn(1, 8, 100, 64)  # (batch, heads, seq, dim)\n",
    "v = torch.randn(1, 8, 100, 64)\n",
    "\n",
    "print(f\"Original K memory: {k.element_size() * k.nelement() / 1e6:.2f} MB\")\n",
    "\n",
    "k_quant, v_quant = quantize_kv_cache(k, v, bits=8)\n",
    "k_q, k_scale, k_min = k_quant\n",
    "\n",
    "print(f\"Quantized K memory: {k_q.element_size() * k_q.nelement() / 1e6:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - k_q.element_size() / k.element_size()) * 100:.1f}%\")\n",
    "\n",
    "# Verify we can reconstruct\n",
    "k_reconstructed = dequantize_tensor(k_q, k_scale, k_min)\n",
    "error = (k - k_reconstructed).abs().mean()\n",
    "print(f\"Reconstruction error: {error:.6f} (very small!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Speculative Decoding - The 2-3x Speedup Trick\n",
    "\n",
    "### The Idea\n",
    "Use a **small, fast \"draft\" model** to predict multiple tokens ahead, then **verify** with the large target model in parallel.\n",
    "\n",
    "### How it Works\n",
    "1. Draft model generates K candidate tokens (fast!)\n",
    "2. Target model scores all K tokens in **one forward pass** (parallel!)\n",
    "3. Accept tokens while predictions match, reject the rest\n",
    "4. Repeat\n",
    "\n",
    "### Why it Works\n",
    "- Draft model is wrong sometimes, but when it's right, we get K tokens for ~1 forward pass cost\n",
    "- No accuracy loss - output is identical to standard decoding!\n",
    "- OpenAI's GPT-4 Turbo uses this technique\n",
    "\n",
    "**Typical Speedup**: 2-3x when draft model has >60% token agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Speculative Decoding Example\n",
    "\n",
    "class DraftModel:\n",
    "    \"\"\"Simulates a small, fast model.\"\"\"\n",
    "    def generate_k_tokens(self, context, k=5):\n",
    "        # In reality, this would be a small LM (e.g., 1B params)\n",
    "        # For demo, we'll just return random predictions\n",
    "        return torch.randint(0, 50000, (k,))\n",
    "\n",
    "class TargetModel:\n",
    "    \"\"\"Simulates the large, accurate model.\"\"\"\n",
    "    def verify_tokens(self, context, draft_tokens):\n",
    "        # In reality, this scores draft_tokens in parallel\n",
    "        # For demo, we'll randomly accept ~70% of tokens\n",
    "        acceptance_mask = torch.rand(len(draft_tokens)) > 0.3\n",
    "        \n",
    "        # Find first rejection\n",
    "        if acceptance_mask.all():\n",
    "            return draft_tokens, len(draft_tokens)\n",
    "        else:\n",
    "            first_reject = (~acceptance_mask).nonzero()[0].item()\n",
    "            return draft_tokens[:first_reject], first_reject\n",
    "\n",
    "# Simulate speculative decoding\n",
    "draft = DraftModel()\n",
    "target = TargetModel()\n",
    "\n",
    "total_tokens = 0\n",
    "forward_passes = 0\n",
    "context = \"Once upon a time\"  # Initial prompt\n",
    "\n",
    "print(\"Simulating Speculative Decoding:\\n\")\n",
    "for step in range(10):\n",
    "    # Draft model generates K tokens\n",
    "    k = 5\n",
    "    draft_tokens = draft.generate_k_tokens(context, k=k)\n",
    "    \n",
    "    # Target model verifies in ONE forward pass\n",
    "    accepted_tokens, num_accepted = target.verify_tokens(context, draft_tokens)\n",
    "    forward_passes += 1\n",
    "    \n",
    "    total_tokens += num_accepted\n",
    "    \n",
    "    print(f\"Step {step+1}: Drafted {k} tokens, accepted {num_accepted}\")\n",
    "\n",
    "speedup = total_tokens / forward_passes\n",
    "print(f\"\\nâœ… Generated {total_tokens} tokens in {forward_passes} forward passes\")\n",
    "print(f\"Effective speedup: {speedup:.2f}x (vs 1.0x for standard decoding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Continuous Batching - Never Wait for Slow Requests\n",
    "\n",
    "### The Old Way (Static Batching)\n",
    "- Batch 8 requests together\n",
    "- Wait for ALL 8 to finish before starting new requests\n",
    "- **Problem**: If one request generates 1000 tokens and others finish in 50 tokens, 7 GPUs sit idle!\n",
    "\n",
    "### The New Way (Continuous Batching)\n",
    "- As soon as a request finishes, immediately inject a new one into the batch\n",
    "- GPU never sits idle\n",
    "- **Benefit**: 2-10x higher throughput!\n",
    "\n",
    "### Implementation\n",
    "vLLM and TensorRT-LLM do this automatically. The key insight:\n",
    "- Each request has its own KV cache\n",
    "- Batch size can change dynamically\n",
    "- Use PagedAttention to handle variable-length caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Putting It All Together - Using vLLM\n",
    "\n",
    "vLLM combines all these techniques:\n",
    "- âœ… KV caching with PagedAttention\n",
    "- âœ… Continuous batching\n",
    "- âœ… Optional speculative decoding\n",
    "- âœ… Quantization support\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using vLLM (commented out since it requires GPU + installation)\n",
    "\n",
    "code_example = '''\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize LLM with optimizations\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-3-8B\",\n",
    "    tensor_parallel_size=1,           # Use 1 GPU\n",
    "    gpu_memory_utilization=0.95,      # Use 95% of GPU memory\n",
    "    enable_prefix_caching=True,       # Cache common prefixes (system prompts)\n",
    "    max_model_len=4096,               # Max sequence length\n",
    "    enforce_eager=False,              # Use CUDA graphs for speed\n",
    ")\n",
    "\n",
    "# Sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Batch requests (continuous batching happens automatically!)\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a Python function to reverse a string.\",\n",
    "    \"What are the key benefits of PyTorch?\"\n",
    "]\n",
    "\n",
    "# Generate (vLLM handles batching, KV cache, everything!)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Prompt: {output.prompt}\")\n",
    "    print(f\"Output: {output.outputs[0].text}\")\n",
    "    print(\"-\" * 80)\n",
    "'''\n",
    "\n",
    "print(\"vLLM Usage Example:\")\n",
    "print(code_example)\n",
    "print(\"\\nâš¡ With vLLM, you get 2-4x higher throughput compared to Hugging Face Transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance Comparison\n",
    "\n",
    "### Typical Benchmarks (Llama-3 8B on A100 GPU)\n",
    "\n",
    "| Method | Throughput (tokens/sec) | Latency (TTFT) | Memory Usage |\n",
    "|--------|------------------------|----------------|---------------|\n",
    "| **Naive PyTorch** | 50 | 200ms | 24GB |\n",
    "| **HuggingFace + KV Cache** | 150 | 100ms | 20GB |\n",
    "| **vLLM (PagedAttention + Batching)** | 450 | 60ms | 14GB |\n",
    "| **TensorRT-LLM (FP8 + All Tricks)** | 800 | 35ms | 10GB |\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. KV caching alone gives ~3x speedup\n",
    "2. PagedAttention + continuous batching â†’ another 3x\n",
    "3. Hardware-specific optimizations (TensorRT) â†’ another 1.8x\n",
    "\n",
    "**Total**: 16x faster inference with modern techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The 2025 LLM Inference Stack\n",
    "\n",
    "### Essential Techniques\n",
    "1. **KV Caching** - Foundation (3x speedup)\n",
    "2. **KV Cache Quantization** - INT8/INT4 (2-4x memory savings)\n",
    "3. **PagedAttention** - Efficient memory management (2x higher batch size)\n",
    "4. **Continuous Batching** - No GPU idle time (2-10x throughput)\n",
    "5. **Speculative Decoding** - Multiple tokens per pass (2-3x speedup)\n",
    "\n",
    "### Production Frameworks\n",
    "- **vLLM**: Best for most use cases, easy integration\n",
    "- **TensorRT-LLM**: Maximum performance on NVIDIA GPUs\n",
    "- **SGLang**: Emerging, focuses on structured generation\n",
    "\n",
    "### What FAANG Expects You to Know\n",
    "âœ… Why LLM inference is memory-bandwidth-bound\n",
    "âœ… How KV caching works and its memory/compute trade-offs\n",
    "âœ… What speculative decoding is and when to use it\n",
    "âœ… Difference between static and continuous batching\n",
    "âœ… How to deploy with vLLM or TensorRT-LLM\n",
    "âœ… Cost optimization strategies (inference = 90% of costs!)\n",
    "\n",
    "### Further Reading\n",
    "- [vLLM Paper: Efficient Memory Management for LLM Serving](https://arxiv.org/abs/2309.06180)\n",
    "- [Speculative Decoding Blog](https://arxiv.org/abs/2211.17192)\n",
    "- [FlashAttention-2](https://arxiv.org/abs/2307.08691)\n",
    "\n",
    "**You now understand cutting-edge LLM inference optimization! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
