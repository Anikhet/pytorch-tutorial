{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Tutorial: Building Neural Networks\n",
        "\n",
        "Now that you understand tensors and gradients, it's time to build your first neural network! This notebook covers the building blocks of neural networks in PyTorch.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand what neural networks are and how they work\n",
        "- Learn to use `nn.Module` to create custom networks\n",
        "- Understand different types of layers (Linear, activation functions, etc.)\n",
        "- Build your first neural network from scratch\n",
        "- Understand the forward pass through a network\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Neural Network?\n",
        "\n",
        "A **neural network** is a series of connected layers that transform input data into output predictions. Think of it as a pipeline:\n",
        "\n",
        "**Input â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Layer N â†’ Output**\n",
        "\n",
        "Each layer:\n",
        "1. Takes input data\n",
        "2. Applies a transformation (usually: multiply by weights, add bias, apply activation)\n",
        "3. Produces output that becomes input to the next layer\n",
        "\n",
        "### Key Components:\n",
        "- **Weights**: Parameters that the network learns\n",
        "- **Biases**: Additional parameters for each layer\n",
        "- **Activation Functions**: Non-linear functions that allow networks to learn complex patterns\n",
        "- **Layers**: Building blocks that perform specific operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up\n",
        "\n",
        "Let's import the necessary modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn  # nn module contains all neural network components\n",
        "import torch.nn.functional as F  # Functional interface for operations\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding nn.Module\n",
        "\n",
        "`nn.Module` is the base class for all neural network components in PyTorch. It provides:\n",
        "- Automatic gradient tracking for all parameters\n",
        "- Easy parameter management\n",
        "- Device management (CPU/GPU)\n",
        "- Model saving and loading capabilities\n",
        "\n",
        "Let's create our first simple network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple neural network class\n",
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple neural network with one hidden layer.\n",
        "    \n",
        "    Architecture:\n",
        "    Input (2 features) â†’ Hidden Layer (3 neurons) â†’ Output (1 value)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Call parent class constructor\n",
        "        super(SimpleNet, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        # nn.Linear(input_size, output_size)\n",
        "        # This creates: output = input @ weight.T + bias\n",
        "        self.hidden = nn.Linear(2, 3)  # 2 inputs â†’ 3 outputs\n",
        "        self.output = nn.Linear(3, 1)   # 3 inputs â†’ 1 output\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass (how data flows through the network).\n",
        "        This is called automatically when you do: model(input)\n",
        "        \"\"\"\n",
        "        # Pass through hidden layer\n",
        "        x = self.hidden(x)  # Shape: (batch, 2) â†’ (batch, 3)\n",
        "        \n",
        "        # Apply activation function (we'll learn about these next)\n",
        "        x = torch.relu(x)  # ReLU: max(0, x)\n",
        "        \n",
        "        # Pass through output layer\n",
        "        x = self.output(x)  # Shape: (batch, 3) â†’ (batch, 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create an instance of our network\n",
        "model = SimpleNet()\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Model structure:\")\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "# Let's see what parameters the model has\n",
        "print(\"Model parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: shape {param.shape}, requires_grad={param.requires_grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Our Network\n",
        "\n",
        "Let's pass some data through our network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create some sample input data\n",
        "# Shape: (batch_size, input_features)\n",
        "# batch_size = 3 (3 samples), input_features = 2\n",
        "x = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0],\n",
        "                  [5.0, 6.0]])\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Input data:\")\n",
        "print(x)\n",
        "print()\n",
        "\n",
        "# Forward pass through the network\n",
        "# This automatically calls the forward() method\n",
        "output = model(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output data:\")\n",
        "print(output)\n",
        "print()\n",
        "\n",
        "# Note: The network hasn't been trained yet, so outputs are random!\n",
        "# We'll learn how to train networks in the next notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Layers\n",
        "\n",
        "Let's explore the most common types of layers in PyTorch:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Linear Layer (Fully Connected / Dense Layer)\n",
        "\n",
        "A Linear layer performs: `output = input @ weight.T + bias`\n",
        "\n",
        "This is the most common layer type in neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a linear layer\n",
        "# nn.Linear(in_features, out_features)\n",
        "linear = nn.Linear(in_features=4, out_features=3)\n",
        "\n",
        "print(\"Linear layer:\")\n",
        "print(linear)\n",
        "print()\n",
        "\n",
        "# Check the parameters\n",
        "print(\"Weight shape:\", linear.weight.shape)  # (out_features, in_features) = (3, 4)\n",
        "print(\"Bias shape:\", linear.bias.shape)      # (out_features,) = (3,)\n",
        "print()\n",
        "\n",
        "# Create some input data\n",
        "x = torch.randn(2, 4)  # batch_size=2, features=4\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Input:\")\n",
        "print(x)\n",
        "print()\n",
        "\n",
        "# Forward pass\n",
        "output = linear(x)\n",
        "print(\"Output shape:\", output.shape)  # (batch_size, out_features) = (2, 3)\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print()\n",
        "\n",
        "# Manual calculation (for understanding):\n",
        "# output = x @ weight.T + bias\n",
        "manual_output = x @ linear.weight.T + linear.bias\n",
        "print(\"Manual calculation matches:\", torch.allclose(output, manual_output))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Activation Functions\n",
        "\n",
        "Activation functions introduce **non-linearity** into neural networks. Without them, multiple layers would be equivalent to a single layer!\n",
        "\n",
        "Common activation functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create some input values\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "\n",
        "# ReLU (Rectified Linear Unit): max(0, x)\n",
        "# Most common activation function\n",
        "relu = torch.relu(x)\n",
        "\n",
        "# Sigmoid: 1 / (1 + exp(-x))\n",
        "# Outputs between 0 and 1\n",
        "sigmoid = torch.sigmoid(x)\n",
        "\n",
        "# Tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "# Outputs between -1 and 1\n",
        "tanh = torch.tanh(x)\n",
        "\n",
        "# Visualize activation functions\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(x.numpy(), relu.numpy(), 'b-', linewidth=2)\n",
        "plt.title('ReLU: max(0, x)', fontsize=12)\n",
        "plt.xlabel('x', fontsize=10)\n",
        "plt.ylabel('y', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(x.numpy(), sigmoid.numpy(), 'r-', linewidth=2)\n",
        "plt.title('Sigmoid: 1/(1+exp(-x))', fontsize=12)\n",
        "plt.xlabel('x', fontsize=10)\n",
        "plt.ylabel('y', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0.5, color='k', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(x.numpy(), tanh.numpy(), 'g-', linewidth=2)\n",
        "plt.title('Tanh', fontsize=12)\n",
        "plt.xlabel('x', fontsize=10)\n",
        "plt.ylabel('y', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key differences:\")\n",
        "print(\"- ReLU: Simple, fast, most common. Outputs 0 for negative inputs.\")\n",
        "print(\"- Sigmoid: Smooth, outputs 0-1. Good for probabilities.\")\n",
        "print(\"- Tanh: Smooth, outputs -1 to 1. Similar to sigmoid but centered at 0.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Sequential: Building Networks Easily\n",
        "\n",
        "`nn.Sequential` allows you to build networks without writing a class. It's great for simple, linear architectures:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a network using Sequential\n",
        "# This is equivalent to our SimpleNet but simpler to write\n",
        "sequential_model = nn.Sequential(\n",
        "    nn.Linear(2, 3),      # Input layer: 2 â†’ 3\n",
        "    nn.ReLU(),            # Activation\n",
        "    nn.Linear(3, 1)       # Output layer: 3 â†’ 1\n",
        ")\n",
        "\n",
        "print(\"Sequential model:\")\n",
        "print(sequential_model)\n",
        "print()\n",
        "\n",
        "# Test it\n",
        "x = torch.tensor([[1.0, 2.0]])\n",
        "output = sequential_model(x)\n",
        "print(\"Input:\", x)\n",
        "print(\"Output:\", output)\n",
        "print()\n",
        "\n",
        "# Access individual layers\n",
        "print(\"First layer (index 0):\", sequential_model[0])\n",
        "print(\"First layer weight shape:\", sequential_model[0].weight.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a More Complex Network\n",
        "\n",
        "Let's build a network for a classification problem (e.g., classifying images):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassificationNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network for classification.\n",
        "    \n",
        "    Architecture:\n",
        "    Input (784 features, e.g., flattened 28x28 image)\n",
        "    â†’ Hidden Layer 1 (128 neurons) + ReLU\n",
        "    â†’ Hidden Layer 2 (64 neurons) + ReLU\n",
        "    â†’ Output Layer (10 classes, e.g., digits 0-9) + Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size=784, hidden1=128, hidden2=64, num_classes=10):\n",
        "        super(ClassificationNet, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden1)  # Fully connected layer 1\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)       # Fully connected layer 2\n",
        "        self.fc3 = nn.Linear(hidden2, num_classes)   # Output layer\n",
        "        \n",
        "        # Note: We don't define ReLU here because we'll use F.relu in forward()\n",
        "        # This is a common pattern\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Flatten input if needed (for images)\n",
        "        # x shape: (batch, height, width) â†’ (batch, height*width)\n",
        "        x = x.view(x.size(0), -1)  # -1 means \"figure out this dimension\"\n",
        "        \n",
        "        # Layer 1\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)  # Apply ReLU activation\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Output layer (no activation here - we'll apply softmax during loss calculation)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = ClassificationNet()\n",
        "print(\"Classification Network:\")\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print()\n",
        "\n",
        "# Test with dummy data\n",
        "dummy_input = torch.randn(5, 784)  # 5 samples, 784 features\n",
        "output = model(dummy_input)\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")  # (5, 10) - 5 samples, 10 class scores\n",
        "print(f\"Output (class scores):\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Forward Pass\n",
        "\n",
        "Let's trace through what happens during a forward pass:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple example to trace through\n",
        "simple_model = nn.Sequential(\n",
        "    nn.Linear(3, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 2)\n",
        ")\n",
        "\n",
        "# Create input\n",
        "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
        "print(\"Input:\", x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print()\n",
        "\n",
        "# Trace through each layer\n",
        "print(\"Forward pass:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# After first linear layer\n",
        "x1 = simple_model[0](x)\n",
        "print(f\"After Linear(3â†’4): {x1}\")\n",
        "print(f\"  Shape: {x1.shape}\")\n",
        "\n",
        "# After ReLU\n",
        "x2 = simple_model[1](x1)\n",
        "print(f\"After ReLU: {x2}\")\n",
        "print(f\"  Shape: {x2.shape}\")\n",
        "\n",
        "# After second linear layer\n",
        "x3 = simple_model[2](x2)\n",
        "print(f\"After Linear(4â†’2): {x3}\")\n",
        "print(f\"  Shape: {x3.shape}\")\n",
        "print()\n",
        "\n",
        "# Compare with direct forward pass\n",
        "direct_output = simple_model(x)\n",
        "print(\"Direct forward pass output:\", direct_output)\n",
        "print(\"Matches:\", torch.allclose(x3, direct_output))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Layer Types (Reference)\n",
        "\n",
        "Here's a quick reference of other common layers you'll encounter:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropout: Randomly sets some neurons to zero during training (prevents overfitting)\n",
        "dropout = nn.Dropout(p=0.5)  # 50% dropout rate\n",
        "x = torch.randn(3, 4)\n",
        "print(\"Before dropout:\", x)\n",
        "print(\"After dropout (training):\", dropout(x))  # Some values become 0\n",
        "print()\n",
        "\n",
        "# Batch Normalization: Normalizes inputs to each layer (helps training)\n",
        "batch_norm = nn.BatchNorm1d(4)  # For 1D data with 4 features\n",
        "x = torch.randn(3, 4)  # (batch, features)\n",
        "print(\"Before batch norm:\", x)\n",
        "print(\"After batch norm:\", batch_norm(x))\n",
        "print()\n",
        "\n",
        "# Note: We'll learn about Conv2d (for images) and other layers in later notebooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercises\n",
        "\n",
        "### Exercise 1: Build a Simple Network\n",
        "Create a network with:\n",
        "- Input: 10 features\n",
        "- Hidden layer: 20 neurons with ReLU\n",
        "- Output: 5 classes\n",
        "\n",
        "### Exercise 2: Count Parameters\n",
        "For the network in Exercise 1, calculate:\n",
        "- Number of weights in the first layer\n",
        "- Number of biases in the first layer\n",
        "- Total parameters\n",
        "\n",
        "### Exercise 3: Forward Pass\n",
        "Create input data of shape (3, 10) and pass it through your network. What's the output shape?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions to Exercises\n",
        "\n",
        "### Exercise 1 Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1 Solution\n",
        "class ExerciseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExerciseNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.fc2 = nn.Linear(20, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = ExerciseNet()\n",
        "print(\"Network:\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2 Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2 Solution\n",
        "print(\"First layer (fc1):\")\n",
        "print(f\"  Weight shape: {model.fc1.weight.shape}\")  # (20, 10)\n",
        "print(f\"  Number of weights: {model.fc1.weight.numel()}\")  # 20 * 10 = 200\n",
        "print(f\"  Bias shape: {model.fc1.bias.shape}\")  # (20,)\n",
        "print(f\"  Number of biases: {model.fc1.bias.numel()}\")  # 20\n",
        "print()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"  Breakdown: (10*20 + 20) + (20*5 + 5) = 200 + 20 + 100 + 5 = 325\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3 Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3 Solution\n",
        "x = torch.randn(3, 10)  # 3 samples, 10 features\n",
        "print(\"Input shape:\", x.shape)\n",
        "\n",
        "output = model(x)\n",
        "print(\"Output shape:\", output.shape)  # (3, 5) - 3 samples, 5 class scores\n",
        "print(\"Output:\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **nn.Module**: Base class for all neural networks in PyTorch\n",
        "2. **forward()**: Defines how data flows through the network\n",
        "3. **nn.Linear**: Fully connected layer (most common layer type)\n",
        "4. **Activation Functions**: Introduce non-linearity (ReLU, sigmoid, tanh)\n",
        "5. **nn.Sequential**: Easy way to build simple linear networks\n",
        "6. **Parameters**: Weights and biases are automatically tracked for gradient computation\n",
        "7. **Forward Pass**: Simply call `model(input)` to get predictions\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In the next notebook, we'll learn about:\n",
        "- **Training Neural Networks**: How to train your models\n",
        "- **Loss Functions**: Measuring how wrong predictions are\n",
        "- **Optimizers**: Algorithms that update network parameters\n",
        "- **Training Loop**: The complete process of training a model\n",
        "\n",
        "Now that you can build networks, it's time to teach them to learn!\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work! You can now build neural networks! ðŸŽ‰**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
