{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f28f4e",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Building Neural Networks\n",
    "\n",
    "You know Tensors (Data). You know Autograd (Math). Now let's build the **Machine**.\n",
    "\n",
    "Before we start assembling, we need to define the **Parts**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Layer\", \"Module\", \"Parameter\", and \"Activation\"?\n",
    "- **The Intuition**: Layers as \"Filters\" or \"Assembly Line Stations\".\n",
    "- **The Mechanism**: `nn.Module` and the `forward()` method.\n",
    "- **The Deep Dive**: What are \"Parameters\" and where do they live?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d3e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ea443b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855108a8",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "A Neural Network is just a big function made of smaller functions. Here are the names of the parts:\n",
    "\n",
    "### 1. Layer\n",
    "- A reusable block of math that transforms data.\n",
    "- Example: `nn.Linear`, `nn.Conv2d`.\n",
    "- Analogy: A single machine in a factory.\n",
    "\n",
    "### 2. Module (`nn.Module`)\n",
    "- The base class for all neural network parts in PyTorch.\n",
    "- Your entire model is a Module. A single layer is also a Module.\n",
    "- Analogy: The \"Blueprint\" for the machine.\n",
    "\n",
    "### 3. Parameter\n",
    "- The internal numbers (Weights and Biases) that the model learns.\n",
    "- These are the \"knobs\" the optimizer turns.\n",
    "- Analogy: The settings on the machine.\n",
    "\n",
    "### 4. Activation Function\n",
    "- A non-linear function applied after a layer.\n",
    "- Without this, a neural network is just one big linear regression.\n",
    "- Example: ReLU, Sigmoid.\n",
    "- Analogy: The \"Spark\" or \"Decision\" to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ad394",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (The Assembly Line)\n",
    "\n",
    "Imagine a car factory assembly line.\n",
    "\n",
    "1. **Input**: Raw Steel (Data).\n",
    "2. **Station 1 (Layer 1)**: Stamps steel into doors. (Transforms shape).\n",
    "3. **Station 2 (Layer 2)**: Welds doors to frame. (Combines features).\n",
    "4. **Station 3 (Layer 3)**: Paints the car. (Final polish).\n",
    "5. **Output**: Finished Car (Prediction).\n",
    "\n",
    "In PyTorch, this factory is a `nn.Module`. The stations are Layers. The conveyor belt is the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "lf0b4elswj",
   "source": "import numpy as np\n\n# Create a simple visualization of network architecture\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Layer positions\nlayers = [\n    {\"name\": \"Input\\nLayer\", \"neurons\": 4, \"x\": 0.1, \"color\": \"lightblue\"},\n    {\"name\": \"Hidden\\nLayer 1\", \"neurons\": 6, \"x\": 0.35, \"color\": \"lightgreen\"},\n    {\"name\": \"Hidden\\nLayer 2\", \"neurons\": 4, \"x\": 0.6, \"color\": \"lightyellow\"},\n    {\"name\": \"Output\\nLayer\", \"neurons\": 2, \"x\": 0.85, \"color\": \"lightcoral\"}\n]\n\n# Draw neurons and connections\nneuron_positions = {}\n\nfor layer_idx, layer in enumerate(layers):\n    x_pos = layer[\"x\"]\n    n_neurons = layer[\"neurons\"]\n    y_positions = np.linspace(0.2, 0.8, n_neurons)\n    \n    neuron_positions[layer_idx] = []\n    \n    for neuron_idx, y_pos in enumerate(y_positions):\n        # Draw neuron\n        circle = plt.Circle((x_pos, y_pos), 0.03, color=layer[\"color\"], \n                          ec='black', linewidth=2, zorder=3)\n        ax.add_patch(circle)\n        neuron_positions[layer_idx].append((x_pos, y_pos))\n    \n    # Add layer label\n    ax.text(x_pos, 0.95, layer[\"name\"], ha='center', fontsize=12, \n            fontweight='bold', bbox=dict(boxstyle='round', facecolor=layer[\"color\"], alpha=0.8))\n    \n    # Draw connections to next layer\n    if layer_idx < len(layers) - 1:\n        for pos1 in neuron_positions[layer_idx]:\n            for pos2 in neuron_positions[layer_idx + 1]:\n                ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], \n                       'gray', alpha=0.3, linewidth=0.5, zorder=1)\n\n# Add arrows between layers\nfor i in range(len(layers) - 1):\n    mid_x = (layers[i][\"x\"] + layers[i+1][\"x\"]) / 2\n    ax.annotate('', xy=(layers[i+1][\"x\"] - 0.05, 0.5), \n               xytext=(layers[i][\"x\"] + 0.05, 0.5),\n               arrowprops=dict(arrowstyle='->', lw=3, color='blue'))\n    \n    # Add operation label\n    if i == 0:\n        operation = \"Linear + ReLU\"\n    elif i == 1:\n        operation = \"Linear + ReLU\"\n    else:\n        operation = \"Linear\"\n    \n    ax.text(mid_x, 0.05, operation, ha='center', fontsize=10,\n           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('Neural Network Architecture: Data Flow', fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Data Flow:\")\nprint(\"1. Input (4 features) → Linear transformation → 6 hidden neurons → ReLU activation\")\nprint(\"2. Hidden (6) → Linear transformation → 4 hidden neurons → ReLU activation\")\nprint(\"3. Hidden (4) → Linear transformation → 2 output predictions\")\nprint(\"\\nEach connection represents a learnable weight parameter!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nybkl7x661",
   "source": "### Visualization: Neural Network as Assembly Line\n\nLet's visualize how data flows through layers in a neural network.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9c63a7da",
   "metadata": {},
   "source": [
    "## Part 3: The Linear Layer (The Workhorse)\n",
    "\n",
    "The most basic layer is `nn.Linear`. It performs the equation of a line (in N dimensions):\n",
    "\n",
    "$$ y = xA^T + b $$\n",
    "\n",
    "Where:\n",
    "- $x$: Input features.\n",
    "- $A$: Weights (The \"Slope\").\n",
    "- $b$: Bias (The \"Intercept\").\n",
    "\n",
    "It simply maps input points to output points via rotation and stretching."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3us0cx68d",
   "source": "# Visualize linear transformation in 2D\ntorch.manual_seed(42)\n\n# Create sample 2D points (spiral pattern)\ntheta = torch.linspace(0, 4*np.pi, 100)\nr = torch.linspace(0.5, 2, 100)\nx_input = torch.stack([r * torch.cos(theta), r * torch.sin(theta)], dim=1)\n\n# Create a linear layer (2D -> 2D transformation)\nlinear_layer = nn.Linear(2, 2)\nwith torch.no_grad():\n    # Set specific weights for a clear transformation\n    linear_layer.weight = nn.Parameter(torch.tensor([[1.5, 0.5], [-0.3, 1.2]]))\n    linear_layer.bias = nn.Parameter(torch.tensor([0.5, -0.3]))\n\n# Transform the points\nx_output = linear_layer(x_input)\n\n# Visualize\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n\n# Original points\nax1.scatter(x_input[:, 0].numpy(), x_input[:, 1].numpy(), \n           c=range(len(x_input)), cmap='viridis', s=30, alpha=0.7)\nax1.set_xlabel('Feature 1', fontsize=12)\nax1.set_ylabel('Feature 2', fontsize=12)\nax1.set_title('Input Data (Original)', fontsize=13, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.axis('equal')\nax1.set_xlim(-3, 3)\nax1.set_ylim(-3, 3)\n\n# Show the transformation\nax2.scatter(x_input[:, 0].numpy(), x_input[:, 1].numpy(), \n           c=range(len(x_input)), cmap='viridis', s=30, alpha=0.3, label='Original')\nax2.scatter(x_output[:, 0].detach().numpy(), x_output[:, 1].detach().numpy(), \n           c=range(len(x_output)), cmap='plasma', s=30, alpha=0.7, label='Transformed')\nax2.set_xlabel('Feature 1', fontsize=12)\nax2.set_ylabel('Feature 2', fontsize=12)\nax2.set_title('Linear Transformation', fontsize=13, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.legend()\nax2.axis('equal')\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\n\n# Transformed points\nax3.scatter(x_output[:, 0].detach().numpy(), x_output[:, 1].detach().numpy(), \n           c=range(len(x_output)), cmap='plasma', s=30, alpha=0.7)\nax3.set_xlabel('Feature 1', fontsize=12)\nax3.set_ylabel('Feature 2', fontsize=12)\nax3.set_title('Output Data (Transformed)', fontsize=13, fontweight='bold')\nax3.grid(True, alpha=0.3)\nax3.axis('equal')\nax3.set_xlim(-3, 3)\nax3.set_ylim(-3, 3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"What Linear Layers Do:\")\nprint(\"• Rotation: Change the orientation of data\")\nprint(\"• Scaling: Stretch or compress along axes\")\nprint(\"• Translation: Shift the data (via bias)\")\nprint(f\"\\nWeight matrix:\\n{linear_layer.weight.data}\")\nprint(f\"Bias vector: {linear_layer.bias.data}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "44csvuxay1o",
   "source": "### Visualization: How Linear Layers Transform Data\n\nLet's visualize what a linear layer does geometrically - it rotates, scales, and shifts points in space.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Layer\n",
    "# Input: 3 features (e.g., Age, Height, Weight)\n",
    "# Output: 1 feature (e.g., Life Expectancy)\n",
    "layer = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "print(\"Weights (A):\", layer.weight)\n",
    "print(\"Bias (b):\", layer.bias)\n",
    "\n",
    "# Pass data through it\n",
    "input_data = torch.tensor([[1.0, 2.0, 3.0]]) # Batch of 1 sample\n",
    "output = layer(input_data)\n",
    "\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80163e2d",
   "metadata": {},
   "source": [
    "## Part 4: Activation Functions (The Spark)\n",
    "\n",
    "Linear layers can only learn straight lines. But the world is curved.\n",
    "\n",
    "To learn curves, we need **Non-Linearity**. We call these \"Activation Functions\".\n",
    "\n",
    "Think of a biological neuron. It gathers signals. If the signal is strong enough, it **FIRES** (Action Potential). If not, it stays silent.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: The most common. If x > 0, return x. If x < 0, return 0. (Like a switch).\n",
    "- **Sigmoid**: Squashes numbers between 0 and 1. (Like a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046079f",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive visualization of activation functions\nx = torch.linspace(-5, 5, 200)\nrelu = nn.ReLU()\nsigmoid = nn.Sigmoid()\ntanh = nn.Tanh()\nleaky_relu = nn.LeakyReLU(0.1)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# 1. ReLU\nax = axes[0, 0]\nax.plot(x, relu(x), linewidth=3, color='blue', label='ReLU')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax.fill_between(x.numpy(), 0, relu(x).numpy(), alpha=0.2, color='blue')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('ReLU: max(0, x)\\n\"The Switch\" - Dead below 0', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 2. Sigmoid\nax = axes[0, 1]\nax.plot(x, sigmoid(x), linewidth=3, color='green', label='Sigmoid')\nax.axhline(y=0.5, color='k', linestyle='--', alpha=0.3, label='Midpoint')\nax.axhline(y=0, color='r', linestyle=':', alpha=0.3)\nax.axhline(y=1, color='r', linestyle=':', alpha=0.3)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Sigmoid: 1/(1+e⁻ˣ)\\n\"The Probability\" - Output [0,1]', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 3. Tanh\nax = axes[0, 2]\nax.plot(x, tanh(x), linewidth=3, color='orange', label='Tanh')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axhline(y=-1, color='r', linestyle=':', alpha=0.3)\nax.axhline(y=1, color='r', linestyle=':', alpha=0.3)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Tanh: (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ)\\n\"Centered\" - Output [-1,1]', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 4. Leaky ReLU\nax = axes[1, 0]\nax.plot(x, leaky_relu(x), linewidth=3, color='purple', label='Leaky ReLU')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax.fill_between(x.numpy(), 0, leaky_relu(x).numpy(), alpha=0.2, color='purple')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Leaky ReLU: max(0.1x, x)\\n\"Allows small negative values\"', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 5. Comparison of all\nax = axes[1, 1]\nax.plot(x, relu(x), linewidth=2, label='ReLU', alpha=0.8)\nax.plot(x, sigmoid(x), linewidth=2, label='Sigmoid', alpha=0.8)\nax.plot(x, tanh(x), linewidth=2, label='Tanh', alpha=0.8)\nax.plot(x, leaky_relu(x), linewidth=2, label='Leaky ReLU', alpha=0.8)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Comparison of All Activations', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 6. Why non-linearity matters\nax = axes[1, 2]\nx_demo = torch.linspace(-2, 2, 100)\n# Linear only (no activation)\nlinear_output = 2 * x_demo + 1\n# With ReLU activation\nnonlinear_output = relu(2 * x_demo + 1)\n\nax.plot(x_demo, linear_output, linewidth=2.5, label='Linear only (boring!)', color='gray', linestyle='--')\nax.plot(x_demo, nonlinear_output, linewidth=2.5, label='Linear + ReLU (learns curves!)', color='red')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Why We Need Activation Functions', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\nax.text(0, -1.5, 'Without activation: just a straight line!\\nWith activation: can learn complex patterns', \n        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Activation Function Properties:\")\nprint(\"• ReLU: Fast, but can 'die' (always output 0)\")\nprint(\"• Sigmoid: Smooth, but gradients vanish for large |x|\")\nprint(\"• Tanh: Zero-centered, but also suffers from vanishing gradients\")\nprint(\"• Leaky ReLU: Fixes dying ReLU problem with small negative slope\")"
  },
  {
   "cell_type": "code",
   "id": "wdnlkoazzpk",
   "source": "# Visualize the parameters of our network\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Get parameters from the model\nparams_list = list(model.named_parameters())\n\n# Plot Layer 1 weights\nax = axes[0, 0]\nweights_layer1 = params_list[0][1].detach().numpy()\nim1 = ax.imshow(weights_layer1, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\nax.set_xlabel('Input Features (10)', fontsize=11)\nax.set_ylabel('Hidden Neurons (5)', fontsize=11)\nax.set_title('Layer 1 Weights (5×10)\\nEach row = one neuron\\'s weights', fontsize=12, fontweight='bold')\nplt.colorbar(im1, ax=ax, label='Weight Value')\nfor i in range(weights_layer1.shape[0]):\n    for j in range(weights_layer1.shape[1]):\n        text = ax.text(j, i, f'{weights_layer1[i, j]:.2f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n\n# Plot Layer 1 bias\nax = axes[0, 1]\nbias_layer1 = params_list[1][1].detach().numpy()\nax.barh(range(len(bias_layer1)), bias_layer1, color='steelblue', edgecolor='black')\nax.set_ylabel('Hidden Neurons (5)', fontsize=11)\nax.set_xlabel('Bias Value', fontsize=11)\nax.set_title('Layer 1 Biases (5)\\nShift each neuron\\'s activation', fontsize=12, fontweight='bold')\nax.axvline(x=0, color='red', linestyle='--', linewidth=1)\nax.grid(True, alpha=0.3, axis='x')\nfor i, v in enumerate(bias_layer1):\n    ax.text(v + 0.02, i, f'{v:.3f}', va='center', fontsize=9)\n\n# Plot Layer 2 weights\nax = axes[1, 0]\nweights_layer2 = params_list[2][1].detach().numpy()\nim2 = ax.imshow(weights_layer2, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\nax.set_xlabel('Hidden Features (5)', fontsize=11)\nax.set_ylabel('Output Neurons (1)', fontsize=11)\nax.set_title('Layer 2 Weights (1×5)\\nCombines hidden features', fontsize=12, fontweight='bold')\nplt.colorbar(im2, ax=ax, label='Weight Value')\nfor i in range(weights_layer2.shape[0]):\n    for j in range(weights_layer2.shape[1]):\n        text = ax.text(j, i, f'{weights_layer2[i, j]:.2f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n\n# Plot parameter histogram\nax = axes[1, 1]\nall_weights = torch.cat([p.flatten() for name, p in model.named_parameters() if 'weight' in name])\nall_biases = torch.cat([p.flatten() for name, p in model.named_parameters() if 'bias' in name])\n\nax.hist(all_weights.detach().numpy(), bins=30, alpha=0.6, label='Weights', color='blue', edgecolor='black')\nax.hist(all_biases.detach().numpy(), bins=15, alpha=0.6, label='Biases', color='orange', edgecolor='black')\nax.set_xlabel('Parameter Value', fontsize=11)\nax.set_ylabel('Count', fontsize=11)\nax.set_title('Distribution of All Parameters', fontsize=12, fontweight='bold')\nax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\nprint(f\"• Layer 1: {weights_layer1.size} weights + {bias_layer1.size} biases = {weights_layer1.size + bias_layer1.size}\")\nprint(f\"• Layer 2: {weights_layer2.size} weights + {params_list[3][1].numel()} biases = {weights_layer2.size + params_list[3][1].numel()}\")\nprint(\"\\nThese are the 'knobs' gradient descent will tune during training!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n86the9lxz",
   "source": "### Visualization: Network Parameters\n\nLet's visualize the actual parameters (weights and biases) that the network learns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8c262b25",
   "metadata": {},
   "source": [
    "## Part 5: Building the Factory (nn.Module)\n",
    "\n",
    "To build a full network, we subclass `nn.Module`. We must define two things:\n",
    "\n",
    "1. `__init__`: **Define the stations**. (Create the layers).\n",
    "2. `forward`: **Define the conveyor belt**. (Connect the layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c962b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Station 1: 10 inputs -> 5 hidden features\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        # Station 2: 5 hidden -> 1 output\n",
    "        self.layer2 = nn.Linear(5, 1)\n",
    "        # The Spark\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The Conveyor Belt\n",
    "        x = self.layer1(x)      # Step 1\n",
    "        x = self.activation(x)  # Step 2 (Non-linearity)\n",
    "        x = self.layer2(x)      # Step 3\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64072644",
   "metadata": {},
   "source": [
    "## Part 6: The Deep Dive (Parameters)\n",
    "\n",
    "Where does the \"Knowledge\" live?\n",
    "\n",
    "It lives in the **Parameters** (Weights and Biases). PyTorch automatically tracks these for you because you used `nn.Linear`.\n",
    "\n",
    "Let's inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal Learnable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **nn.Module** = The Blueprint for your network.\n",
    "2. **Layers** = The transformation stations (Linear, Conv2d).\n",
    "3. **Activation** = The non-linear spark (ReLU, Sigmoid).\n",
    "4. **forward()** = The path data takes through the network.\n",
    "5. **Parameters** = The learnable weights that hold the knowledge.\n",
    "\n",
    "Next, we will learn how to **Train** this machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}