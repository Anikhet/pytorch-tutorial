{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f28f4e",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Building Neural Networks\n",
    "\n",
    "You know Tensors (Data). You know Autograd (Math). Now let's build the **Machine**.\n",
    "\n",
    "Before we start assembling, we need to define the **Parts**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Layer\", \"Module\", \"Parameter\", and \"Activation\"?\n",
    "- **The Intuition**: Layers as \"Filters\" or \"Assembly Line Stations\".\n",
    "- **The Mechanism**: `nn.Module` and the `forward()` method.\n",
    "- **The Deep Dive**: What are \"Parameters\" and where do they live?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855108a8",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "A Neural Network is just a big function made of smaller functions. Here are the names of the parts:\n",
    "\n",
    "### 1. Layer\n",
    "- A reusable block of math that transforms data.\n",
    "- Example: `nn.Linear`, `nn.Conv2d`.\n",
    "- Analogy: A single machine in a factory.\n",
    "\n",
    "### 2. Module (`nn.Module`)\n",
    "- The base class for all neural network parts in PyTorch.\n",
    "- Your entire model is a Module. A single layer is also a Module.\n",
    "- Analogy: The \"Blueprint\" for the machine.\n",
    "\n",
    "### 3. Parameter\n",
    "- The internal numbers (Weights and Biases) that the model learns.\n",
    "- These are the \"knobs\" the optimizer turns.\n",
    "- Analogy: The settings on the machine.\n",
    "\n",
    "### 4. Activation Function\n",
    "- A non-linear function applied after a layer.\n",
    "- Without this, a neural network is just one big linear regression.\n",
    "- Example: ReLU, Sigmoid.\n",
    "- Analogy: The \"Spark\" or \"Decision\" to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ad394",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (The Assembly Line)\n",
    "\n",
    "Imagine a car factory assembly line.\n",
    "\n",
    "1. **Input**: Raw Steel (Data).\n",
    "2. **Station 1 (Layer 1)**: Stamps steel into doors. (Transforms shape).\n",
    "3. **Station 2 (Layer 2)**: Welds doors to frame. (Combines features).\n",
    "4. **Station 3 (Layer 3)**: Paints the car. (Final polish).\n",
    "5. **Output**: Finished Car (Prediction).\n",
    "\n",
    "In PyTorch, this factory is a `nn.Module`. The stations are Layers. The conveyor belt is the `forward()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63a7da",
   "metadata": {},
   "source": [
    "## Part 3: The Linear Layer (The Workhorse)\n",
    "\n",
    "The most basic layer is `nn.Linear`. It performs the equation of a line (in N dimensions):\n",
    "\n",
    "$$ y = xA^T + b $$\n",
    "\n",
    "Where:\n",
    "- $x$: Input features.\n",
    "- $A$: Weights (The \"Slope\").\n",
    "- $b$: Bias (The \"Intercept\").\n",
    "\n",
    "It simply maps input points to output points via rotation and stretching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Layer\n",
    "# Input: 3 features (e.g., Age, Height, Weight)\n",
    "# Output: 1 feature (e.g., Life Expectancy)\n",
    "layer = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "print(\"Weights (A):\", layer.weight)\n",
    "print(\"Bias (b):\", layer.bias)\n",
    "\n",
    "# Pass data through it\n",
    "input_data = torch.tensor([[1.0, 2.0, 3.0]]) # Batch of 1 sample\n",
    "output = layer(input_data)\n",
    "\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80163e2d",
   "metadata": {},
   "source": [
    "## Part 4: Activation Functions (The Spark)\n",
    "\n",
    "Linear layers can only learn straight lines. But the world is curved.\n",
    "\n",
    "To learn curves, we need **Non-Linearity**. We call these \"Activation Functions\".\n",
    "\n",
    "Think of a biological neuron. It gathers signals. If the signal is strong enough, it **FIRES** (Action Potential). If not, it stays silent.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: The most common. If x > 0, return x. If x < 0, return 0. (Like a switch).\n",
    "- **Sigmoid**: Squashes numbers between 0 and 1. (Like a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5, 100)\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title(\"ReLU (The Switch)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title(\"Sigmoid (The Probability)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c262b25",
   "metadata": {},
   "source": [
    "## Part 5: Building the Factory (nn.Module)\n",
    "\n",
    "To build a full network, we subclass `nn.Module`. We must define two things:\n",
    "\n",
    "1. `__init__`: **Define the stations**. (Create the layers).\n",
    "2. `forward`: **Define the conveyor belt**. (Connect the layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c962b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Station 1: 10 inputs -> 5 hidden features\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        # Station 2: 5 hidden -> 1 output\n",
    "        self.layer2 = nn.Linear(5, 1)\n",
    "        # The Spark\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The Conveyor Belt\n",
    "        x = self.layer1(x)      # Step 1\n",
    "        x = self.activation(x)  # Step 2 (Non-linearity)\n",
    "        x = self.layer2(x)      # Step 3\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64072644",
   "metadata": {},
   "source": [
    "## Part 6: The Deep Dive (Parameters)\n",
    "\n",
    "Where does the \"Knowledge\" live?\n",
    "\n",
    "It lives in the **Parameters** (Weights and Biases). PyTorch automatically tracks these for you because you used `nn.Linear`.\n",
    "\n",
    "Let's inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal Learnable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **nn.Module** = The Blueprint for your network.\n",
    "2. **Layers** = The transformation stations (Linear, Conv2d).\n",
    "3. **Activation** = The non-linear spark (ReLU, Sigmoid).\n",
    "4. **forward()** = The path data takes through the network.\n",
    "5. **Parameters** = The learnable weights that hold the knowledge.\n",
    "\n",
    "Next, we will learn how to **Train** this machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
