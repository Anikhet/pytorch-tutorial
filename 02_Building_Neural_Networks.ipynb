{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f28f4e",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Building Neural Networks\n",
    "\n",
    "You know Tensors (Data). You know Autograd (Math). Now let's build the **Machine**.\n",
    "\n",
    "Before we start assembling, we need to define the **Parts**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Layer\", \"Module\", \"Parameter\", and \"Activation\"?\n",
    "- **The Intuition**: Layers as \"Filters\" or \"Assembly Line Stations\".\n",
    "- **The Mechanism**: `nn.Module` and the `forward()` method.\n",
    "- **The Deep Dive**: What are \"Parameters\" and where do they live?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d3e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ea443b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855108a8",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "A Neural Network is just a big function made of smaller functions. Here are the names of the parts:\n",
    "\n",
    "### 1. Layer\n",
    "- A reusable block of math that transforms data.\n",
    "- Example: `nn.Linear`, `nn.Conv2d`.\n",
    "- Analogy: A single machine in a factory.\n",
    "\n",
    "### 2. Module (`nn.Module`)\n",
    "- The base class for all neural network parts in PyTorch.\n",
    "- Your entire model is a Module. A single layer is also a Module.\n",
    "- Analogy: The \"Blueprint\" for the machine.\n",
    "\n",
    "### 3. Parameter\n",
    "- The internal numbers (Weights and Biases) that the model learns.\n",
    "- These are the \"knobs\" the optimizer turns.\n",
    "- Analogy: The settings on the machine.\n",
    "\n",
    "### 4. Activation Function\n",
    "- A non-linear function applied after a layer.\n",
    "- Without this, a neural network is just one big linear regression.\n",
    "- Example: ReLU, Sigmoid.\n",
    "- Analogy: The \"Spark\" or \"Decision\" to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ad394",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (The Assembly Line)\n",
    "\n",
    "Imagine a car factory assembly line.\n",
    "\n",
    "1. **Input**: Raw Steel (Data).\n",
    "2. **Station 1 (Layer 1)**: Stamps steel into doors. (Transforms shape).\n",
    "3. **Station 2 (Layer 2)**: Welds doors to frame. (Combines features).\n",
    "4. **Station 3 (Layer 3)**: Paints the car. (Final polish).\n",
    "5. **Output**: Finished Car (Prediction).\n",
    "\n",
    "In PyTorch, this factory is a `nn.Module`. The stations are Layers. The conveyor belt is the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "lf0b4elswj",
   "source": "import numpy as np\n\n# Create a simple visualization of network architecture\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Layer positions\nlayers = [\n    {\"name\": \"Input\\nLayer\", \"neurons\": 4, \"x\": 0.1, \"color\": \"lightblue\"},\n    {\"name\": \"Hidden\\nLayer 1\", \"neurons\": 6, \"x\": 0.35, \"color\": \"lightgreen\"},\n    {\"name\": \"Hidden\\nLayer 2\", \"neurons\": 4, \"x\": 0.6, \"color\": \"lightyellow\"},\n    {\"name\": \"Output\\nLayer\", \"neurons\": 2, \"x\": 0.85, \"color\": \"lightcoral\"}\n]\n\n# Draw neurons and connections\nneuron_positions = {}\n\nfor layer_idx, layer in enumerate(layers):\n    x_pos = layer[\"x\"]\n    n_neurons = layer[\"neurons\"]\n    y_positions = np.linspace(0.2, 0.8, n_neurons)\n    \n    neuron_positions[layer_idx] = []\n    \n    for neuron_idx, y_pos in enumerate(y_positions):\n        # Draw neuron\n        circle = plt.Circle((x_pos, y_pos), 0.03, color=layer[\"color\"], \n                          ec='black', linewidth=2, zorder=3)\n        ax.add_patch(circle)\n        neuron_positions[layer_idx].append((x_pos, y_pos))\n    \n    # Add layer label\n    ax.text(x_pos, 0.95, layer[\"name\"], ha='center', fontsize=12, \n            fontweight='bold', bbox=dict(boxstyle='round', facecolor=layer[\"color\"], alpha=0.8))\n    \n    # Draw connections to next layer\n    if layer_idx < len(layers) - 1:\n        for pos1 in neuron_positions[layer_idx]:\n            for pos2 in neuron_positions[layer_idx + 1]:\n                ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], \n                       'gray', alpha=0.3, linewidth=0.5, zorder=1)\n\n# Add arrows between layers\nfor i in range(len(layers) - 1):\n    mid_x = (layers[i][\"x\"] + layers[i+1][\"x\"]) / 2\n    ax.annotate('', xy=(layers[i+1][\"x\"] - 0.05, 0.5), \n               xytext=(layers[i][\"x\"] + 0.05, 0.5),\n               arrowprops=dict(arrowstyle='->', lw=3, color='blue'))\n    \n    # Add operation label\n    if i == 0:\n        operation = \"Linear + ReLU\"\n    elif i == 1:\n        operation = \"Linear + ReLU\"\n    else:\n        operation = \"Linear\"\n    \n    ax.text(mid_x, 0.05, operation, ha='center', fontsize=10,\n           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nax.set_title('Neural Network Architecture: Data Flow', fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Data Flow:\")\nprint(\"1. Input (4 features) ‚Üí Linear transformation ‚Üí 6 hidden neurons ‚Üí ReLU activation\")\nprint(\"2. Hidden (6) ‚Üí Linear transformation ‚Üí 4 hidden neurons ‚Üí ReLU activation\")\nprint(\"3. Hidden (4) ‚Üí Linear transformation ‚Üí 2 output predictions\")\nprint(\"\\nEach connection represents a learnable weight parameter!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nybkl7x661",
   "source": "### Visualization: Neural Network as Assembly Line\n\nLet's visualize how data flows through layers in a neural network.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9c63a7da",
   "metadata": {},
   "source": [
    "## Part 3: The Linear Layer (The Workhorse)\n",
    "\n",
    "The most basic layer is `nn.Linear`. It performs the equation of a line (in N dimensions):\n",
    "\n",
    "$$ y = xA^T + b $$\n",
    "\n",
    "Where:\n",
    "- $x$: Input features.\n",
    "- $A$: Weights (The \"Slope\").\n",
    "- $b$: Bias (The \"Intercept\").\n",
    "\n",
    "It simply maps input points to output points via rotation and stretching."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3us0cx68d",
   "source": "# Visualize linear transformation in 2D\ntorch.manual_seed(42)\n\n# Create sample 2D points (spiral pattern)\ntheta = torch.linspace(0, 4*np.pi, 100)\nr = torch.linspace(0.5, 2, 100)\nx_input = torch.stack([r * torch.cos(theta), r * torch.sin(theta)], dim=1)\n\n# Create a linear layer (2D -> 2D transformation)\nlinear_layer = nn.Linear(2, 2)\nwith torch.no_grad():\n    # Set specific weights for a clear transformation\n    linear_layer.weight = nn.Parameter(torch.tensor([[1.5, 0.5], [-0.3, 1.2]]))\n    linear_layer.bias = nn.Parameter(torch.tensor([0.5, -0.3]))\n\n# Transform the points\nx_output = linear_layer(x_input)\n\n# Visualize\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n\n# Original points\nax1.scatter(x_input[:, 0].numpy(), x_input[:, 1].numpy(), \n           c=range(len(x_input)), cmap='viridis', s=30, alpha=0.7)\nax1.set_xlabel('Feature 1', fontsize=12)\nax1.set_ylabel('Feature 2', fontsize=12)\nax1.set_title('Input Data (Original)', fontsize=13, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.axis('equal')\nax1.set_xlim(-3, 3)\nax1.set_ylim(-3, 3)\n\n# Show the transformation\nax2.scatter(x_input[:, 0].numpy(), x_input[:, 1].numpy(), \n           c=range(len(x_input)), cmap='viridis', s=30, alpha=0.3, label='Original')\nax2.scatter(x_output[:, 0].detach().numpy(), x_output[:, 1].detach().numpy(), \n           c=range(len(x_output)), cmap='plasma', s=30, alpha=0.7, label='Transformed')\nax2.set_xlabel('Feature 1', fontsize=12)\nax2.set_ylabel('Feature 2', fontsize=12)\nax2.set_title('Linear Transformation', fontsize=13, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.legend()\nax2.axis('equal')\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\n\n# Transformed points\nax3.scatter(x_output[:, 0].detach().numpy(), x_output[:, 1].detach().numpy(), \n           c=range(len(x_output)), cmap='plasma', s=30, alpha=0.7)\nax3.set_xlabel('Feature 1', fontsize=12)\nax3.set_ylabel('Feature 2', fontsize=12)\nax3.set_title('Output Data (Transformed)', fontsize=13, fontweight='bold')\nax3.grid(True, alpha=0.3)\nax3.axis('equal')\nax3.set_xlim(-3, 3)\nax3.set_ylim(-3, 3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"What Linear Layers Do:\")\nprint(\"‚Ä¢ Rotation: Change the orientation of data\")\nprint(\"‚Ä¢ Scaling: Stretch or compress along axes\")\nprint(\"‚Ä¢ Translation: Shift the data (via bias)\")\nprint(f\"\\nWeight matrix:\\n{linear_layer.weight.data}\")\nprint(f\"Bias vector: {linear_layer.bias.data}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "44csvuxay1o",
   "source": "### Visualization: How Linear Layers Transform Data\n\nLet's visualize what a linear layer does geometrically - it rotates, scales, and shifts points in space.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Layer\n",
    "# Input: 3 features (e.g., Age, Height, Weight)\n",
    "# Output: 1 feature (e.g., Life Expectancy)\n",
    "layer = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "print(\"Weights (A):\", layer.weight)\n",
    "print(\"Bias (b):\", layer.bias)\n",
    "\n",
    "# Pass data through it\n",
    "input_data = torch.tensor([[1.0, 2.0, 3.0]]) # Batch of 1 sample\n",
    "output = layer(input_data)\n",
    "\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80163e2d",
   "metadata": {},
   "source": [
    "## Part 4: Activation Functions (The Spark)\n",
    "\n",
    "Linear layers can only learn straight lines. But the world is curved.\n",
    "\n",
    "To learn curves, we need **Non-Linearity**. We call these \"Activation Functions\".\n",
    "\n",
    "Think of a biological neuron. It gathers signals. If the signal is strong enough, it **FIRES** (Action Potential). If not, it stays silent.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: The most common. If x > 0, return x. If x < 0, return 0. (Like a switch).\n",
    "- **Sigmoid**: Squashes numbers between 0 and 1. (Like a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046079f",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive visualization of activation functions\nx = torch.linspace(-5, 5, 200)\nrelu = nn.ReLU()\nsigmoid = nn.Sigmoid()\ntanh = nn.Tanh()\nleaky_relu = nn.LeakyReLU(0.1)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# 1. ReLU\nax = axes[0, 0]\nax.plot(x, relu(x), linewidth=3, color='blue', label='ReLU')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax.fill_between(x.numpy(), 0, relu(x).numpy(), alpha=0.2, color='blue')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('ReLU: max(0, x)\\n\"The Switch\" - Dead below 0', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 2. Sigmoid\nax = axes[0, 1]\nax.plot(x, sigmoid(x), linewidth=3, color='green', label='Sigmoid')\nax.axhline(y=0.5, color='k', linestyle='--', alpha=0.3, label='Midpoint')\nax.axhline(y=0, color='r', linestyle=':', alpha=0.3)\nax.axhline(y=1, color='r', linestyle=':', alpha=0.3)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Sigmoid: 1/(1+e‚ÅªÀ£)\\n\"The Probability\" - Output [0,1]', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 3. Tanh\nax = axes[0, 2]\nax.plot(x, tanh(x), linewidth=3, color='orange', label='Tanh')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axhline(y=-1, color='r', linestyle=':', alpha=0.3)\nax.axhline(y=1, color='r', linestyle=':', alpha=0.3)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Tanh: (eÀ£-e‚ÅªÀ£)/(eÀ£+e‚ÅªÀ£)\\n\"Centered\" - Output [-1,1]', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 4. Leaky ReLU\nax = axes[1, 0]\nax.plot(x, leaky_relu(x), linewidth=3, color='purple', label='Leaky ReLU')\nax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax.fill_between(x.numpy(), 0, leaky_relu(x).numpy(), alpha=0.2, color='purple')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Leaky ReLU: max(0.1x, x)\\n\"Allows small negative values\"', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 5. Comparison of all\nax = axes[1, 1]\nax.plot(x, relu(x), linewidth=2, label='ReLU', alpha=0.8)\nax.plot(x, sigmoid(x), linewidth=2, label='Sigmoid', alpha=0.8)\nax.plot(x, tanh(x), linewidth=2, label='Tanh', alpha=0.8)\nax.plot(x, leaky_relu(x), linewidth=2, label='Leaky ReLU', alpha=0.8)\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Comparison of All Activations', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\n\n# 6. Why non-linearity matters\nax = axes[1, 2]\nx_demo = torch.linspace(-2, 2, 100)\n# Linear only (no activation)\nlinear_output = 2 * x_demo + 1\n# With ReLU activation\nnonlinear_output = relu(2 * x_demo + 1)\n\nax.plot(x_demo, linear_output, linewidth=2.5, label='Linear only (boring!)', color='gray', linestyle='--')\nax.plot(x_demo, nonlinear_output, linewidth=2.5, label='Linear + ReLU (learns curves!)', color='red')\nax.set_xlabel('Input (x)', fontsize=11)\nax.set_ylabel('Output', fontsize=11)\nax.set_title('Why We Need Activation Functions', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.legend()\nax.text(0, -1.5, 'Without activation: just a straight line!\\nWith activation: can learn complex patterns', \n        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Activation Function Properties:\")\nprint(\"‚Ä¢ ReLU: Fast, but can 'die' (always output 0)\")\nprint(\"‚Ä¢ Sigmoid: Smooth, but gradients vanish for large |x|\")\nprint(\"‚Ä¢ Tanh: Zero-centered, but also suffers from vanishing gradients\")\nprint(\"‚Ä¢ Leaky ReLU: Fixes dying ReLU problem with small negative slope\")"
  },
  {
   "cell_type": "code",
   "id": "wdnlkoazzpk",
   "source": "# Visualize the parameters of our network\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Get parameters from the model\nparams_list = list(model.named_parameters())\n\n# Plot Layer 1 weights\nax = axes[0, 0]\nweights_layer1 = params_list[0][1].detach().numpy()\nim1 = ax.imshow(weights_layer1, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\nax.set_xlabel('Input Features (10)', fontsize=11)\nax.set_ylabel('Hidden Neurons (5)', fontsize=11)\nax.set_title('Layer 1 Weights (5√ó10)\\nEach row = one neuron\\'s weights', fontsize=12, fontweight='bold')\nplt.colorbar(im1, ax=ax, label='Weight Value')\nfor i in range(weights_layer1.shape[0]):\n    for j in range(weights_layer1.shape[1]):\n        text = ax.text(j, i, f'{weights_layer1[i, j]:.2f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n\n# Plot Layer 1 bias\nax = axes[0, 1]\nbias_layer1 = params_list[1][1].detach().numpy()\nax.barh(range(len(bias_layer1)), bias_layer1, color='steelblue', edgecolor='black')\nax.set_ylabel('Hidden Neurons (5)', fontsize=11)\nax.set_xlabel('Bias Value', fontsize=11)\nax.set_title('Layer 1 Biases (5)\\nShift each neuron\\'s activation', fontsize=12, fontweight='bold')\nax.axvline(x=0, color='red', linestyle='--', linewidth=1)\nax.grid(True, alpha=0.3, axis='x')\nfor i, v in enumerate(bias_layer1):\n    ax.text(v + 0.02, i, f'{v:.3f}', va='center', fontsize=9)\n\n# Plot Layer 2 weights\nax = axes[1, 0]\nweights_layer2 = params_list[2][1].detach().numpy()\nim2 = ax.imshow(weights_layer2, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\nax.set_xlabel('Hidden Features (5)', fontsize=11)\nax.set_ylabel('Output Neurons (1)', fontsize=11)\nax.set_title('Layer 2 Weights (1√ó5)\\nCombines hidden features', fontsize=12, fontweight='bold')\nplt.colorbar(im2, ax=ax, label='Weight Value')\nfor i in range(weights_layer2.shape[0]):\n    for j in range(weights_layer2.shape[1]):\n        text = ax.text(j, i, f'{weights_layer2[i, j]:.2f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n\n# Plot parameter histogram\nax = axes[1, 1]\nall_weights = torch.cat([p.flatten() for name, p in model.named_parameters() if 'weight' in name])\nall_biases = torch.cat([p.flatten() for name, p in model.named_parameters() if 'bias' in name])\n\nax.hist(all_weights.detach().numpy(), bins=30, alpha=0.6, label='Weights', color='blue', edgecolor='black')\nax.hist(all_biases.detach().numpy(), bins=15, alpha=0.6, label='Biases', color='orange', edgecolor='black')\nax.set_xlabel('Parameter Value', fontsize=11)\nax.set_ylabel('Count', fontsize=11)\nax.set_title('Distribution of All Parameters', fontsize=12, fontweight='bold')\nax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\nprint(f\"‚Ä¢ Layer 1: {weights_layer1.size} weights + {bias_layer1.size} biases = {weights_layer1.size + bias_layer1.size}\")\nprint(f\"‚Ä¢ Layer 2: {weights_layer2.size} weights + {params_list[3][1].numel()} biases = {weights_layer2.size + params_list[3][1].numel()}\")\nprint(\"\\nThese are the 'knobs' gradient descent will tune during training!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n86the9lxz",
   "source": "### Visualization: Network Parameters\n\nLet's visualize the actual parameters (weights and biases) that the network learns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8c262b25",
   "metadata": {},
   "source": [
    "## Part 5: Building the Factory (nn.Module)\n",
    "\n",
    "To build a full network, we subclass `nn.Module`. We must define two things:\n",
    "\n",
    "1. `__init__`: **Define the stations**. (Create the layers).\n",
    "2. `forward`: **Define the conveyor belt**. (Connect the layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c962b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Station 1: 10 inputs -> 5 hidden features\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        # Station 2: 5 hidden -> 1 output\n",
    "        self.layer2 = nn.Linear(5, 1)\n",
    "        # The Spark\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The Conveyor Belt\n",
    "        x = self.layer1(x)      # Step 1\n",
    "        x = self.activation(x)  # Step 2 (Non-linearity)\n",
    "        x = self.layer2(x)      # Step 3\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64072644",
   "metadata": {},
   "source": [
    "## Part 6: The Deep Dive (Parameters)\n",
    "\n",
    "Where does the \"Knowledge\" live?\n",
    "\n",
    "It lives in the **Parameters** (Weights and Biases). PyTorch automatically tracks these for you because you used `nn.Linear`.\n",
    "\n",
    "Let's inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal Learnable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 7: Advanced nn.Module Patterns (FAANG Interview Essentials)\n\n### nn.Sequential, nn.ModuleList, nn.ModuleDict\n\nPyTorch provides containers for organizing layers. Knowing when to use each is critical."
  },
  {
   "cell_type": "code",
   "id": "r5gdkvnayz",
   "source": "# nn.Sequential - Simple Linear Stacking\nsequential_model = nn.Sequential(\n    nn.Linear(10, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 1)\n)\nprint(\"nn.Sequential - for simple layer stacking:\")\nprint(sequential_model)\n\n# nn.ModuleList - When you need to iterate/index layers dynamically\nclass DynamicNet(nn.Module):\n    def __init__(self, num_layers=3):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(10, 10) for _ in range(num_layers)\n        ])\n        self.activation = nn.ReLU()\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = self.activation(layer(x))\n        return x\n\ndynamic_model = DynamicNet(num_layers=4)\nprint(\"\\nnn.ModuleList - for dynamic/looped architectures:\")\nprint(f\"Number of layers: {len(dynamic_model.layers)}\")\n\n# nn.ModuleDict - Named layer access (great for multi-task learning)\nclass MultiTaskNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.shared = nn.Linear(10, 32)\n        self.heads = nn.ModuleDict({\n            'classification': nn.Linear(32, 5),\n            'regression': nn.Linear(32, 1),\n            'ranking': nn.Linear(32, 3)\n        })\n    \n    def forward(self, x, task='classification'):\n        x = torch.relu(self.shared(x))\n        return self.heads[task](x)\n\nmulti_task_model = MultiTaskNet()\nprint(\"\\nnn.ModuleDict - for named access (multi-task, multi-head):\")\nfor name, head in multi_task_model.heads.items():\n    print(f\"  Task '{name}': output size {head.out_features}\")\n\n# CRITICAL: Why not use regular Python list/dict?\nprint(\"\\n‚ö†Ô∏è CRITICAL: Always use nn.ModuleList/Dict, NOT regular Python list/dict!\")\nprint(\"  - nn.ModuleList: Parameters are registered and tracked\")\nprint(\"  - Python list: Parameters are INVISIBLE to optimizer!\")\n\n# Demo the difference\nclass BrokenNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = [nn.Linear(10, 10) for _ in range(3)]  # WRONG!\n        \nbroken = BrokenNet()\nprint(f\"\\nBroken (Python list): {sum(p.numel() for p in broken.parameters())} parameters tracked\")\nprint(f\"Correct (ModuleList): {sum(p.numel() for p in dynamic_model.parameters())} parameters tracked\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cs05qes03iw",
   "source": "### Weight Initialization - Critical for Training Stability\n\nPoor initialization = slow training or no convergence. Know these methods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5dznjj5b4jn",
   "source": "# Weight Initialization Methods\n\n# 1. Xavier/Glorot - For sigmoid/tanh activations\ndef xavier_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\n# 2. He/Kaiming - For ReLU activations (MOST COMMON)\ndef he_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\n# 3. Normal/Uniform with specific values\ndef custom_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, mean=0, std=0.01)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\n# Apply initialization to a model\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\nprint(\"Before initialization:\")\nprint(f\"  Layer 1 weight std: {model[0].weight.std():.4f}\")\n\nmodel.apply(he_init)\nprint(\"\\nAfter He initialization (for ReLU):\")\nprint(f\"  Layer 1 weight std: {model[0].weight.std():.4f}\")\n\n# Visualize initialization comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\ninit_methods = [\n    ('Xavier (Uniform)', lambda m: nn.init.xavier_uniform_(m.weight) if isinstance(m, nn.Linear) else None),\n    ('He/Kaiming (Normal)', lambda m: nn.init.kaiming_normal_(m.weight) if isinstance(m, nn.Linear) else None),\n    ('Normal(0, 1)', lambda m: nn.init.normal_(m.weight, 0, 1) if isinstance(m, nn.Linear) else None)\n]\n\nfor ax, (name, init_fn) in zip(axes, init_methods):\n    layer = nn.Linear(100, 100)\n    init_fn(layer)\n    weights = layer.weight.detach().flatten().numpy()\n    ax.hist(weights, bins=50, density=True, alpha=0.7, edgecolor='black')\n    ax.set_title(f'{name}\\nstd={weights.std():.3f}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Weight Value')\n    ax.set_ylabel('Density')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInitialization Guidelines:\")\nprint(\"‚Ä¢ Xavier: Keeps variance stable through sigmoid/tanh networks\")\nprint(\"‚Ä¢ He/Kaiming: Accounts for ReLU killing half the activations\")\nprint(\"‚Ä¢ Normal(0, 1): Often TOO large, causes saturation/explosion\")\nprint(\"\\nüîë FAANG Tip: Use He init for ReLU networks, Xavier for sigmoid/tanh\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ovzw1zhfnv",
   "source": "### Model Analysis - Parameter Counting and Layer Freezing\n\nEssential for transfer learning, debugging, and understanding model complexity.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "922chqbenhi",
   "source": "# Model Analysis Utilities\n\ndef count_parameters(model, trainable_only=True):\n    \"\"\"Count parameters in a model.\"\"\"\n    if trainable_only:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return sum(p.numel() for p in model.parameters())\n\ndef model_summary(model, input_size=None):\n    \"\"\"Print a summary of model layers and parameters.\"\"\"\n    print(f\"{'Layer Name':<30} {'Output Shape':<20} {'Param #':<15}\")\n    print(\"-\" * 65)\n    \n    total_params = 0\n    trainable_params = 0\n    \n    for name, module in model.named_modules():\n        if len(list(module.children())) == 0:  # Leaf module\n            params = sum(p.numel() for p in module.parameters())\n            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n            total_params += params\n            trainable_params += trainable\n            \n            if params > 0:\n                print(f\"{name:<30} {str(list(module.parameters())[0].shape):<20} {params:<15,}\")\n    \n    print(\"-\" * 65)\n    print(f\"Total params: {total_params:,}\")\n    print(f\"Trainable params: {trainable_params:,}\")\n    print(f\"Non-trainable params: {total_params - trainable_params:,}\")\n\n# Demo model\ndemo_model = nn.Sequential(\n    nn.Linear(784, 256),    # 784*256 + 256 = 200,960\n    nn.ReLU(),\n    nn.Linear(256, 128),    # 256*128 + 128 = 32,896\n    nn.ReLU(),\n    nn.Linear(128, 10)      # 128*10 + 10 = 1,290\n)\n\nprint(\"Model Summary:\")\nmodel_summary(demo_model)\n\n# Layer Freezing for Transfer Learning\nprint(\"\\n--- Layer Freezing for Transfer Learning ---\")\n\n# Freeze all layers\nfor param in demo_model.parameters():\n    param.requires_grad = False\n\nprint(f\"After freezing all: {count_parameters(demo_model)} trainable params\")\n\n# Unfreeze only the last layer (fine-tuning)\nfor param in demo_model[4].parameters():  # Last Linear layer\n    param.requires_grad = True\n\nprint(f\"After unfreezing last layer: {count_parameters(demo_model)} trainable params\")\n\n# Common pattern: Freeze backbone, train head\nclass TransferLearningModel(nn.Module):\n    def __init__(self, pretrained_backbone, num_classes):\n        super().__init__()\n        self.backbone = pretrained_backbone\n        \n        # Freeze backbone\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        \n        # New trainable head\n        self.head = nn.Linear(128, num_classes)\n    \n    def forward(self, x):\n        # Features from frozen backbone\n        with torch.no_grad():  # Extra safety, no gradient storage\n            features = self.backbone(x)\n        return self.head(features)\n\nprint(\"\\nüîë FAANG Tip: Freeze early layers (generic features), fine-tune later layers (task-specific)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5oxkntqc4nd",
   "source": "## Part 8: FAANG Interview Questions - Neural Network Architecture\n\n### Question 1: \"Explain the difference between nn.Module's __init__ and forward methods\"\n\n**Answer**:\n- `__init__`: Called once at instantiation. Define layers, buffers, and submodules.\n- `forward`: Called on every input. Defines the computation graph dynamically.\n\n```python\nclass MyModel(nn.Module):\n    def __init__(self):  # Define WHAT layers exist\n        super().__init__()\n        self.layer1 = nn.Linear(10, 5)\n        \n    def forward(self, x):  # Define HOW data flows\n        return self.layer1(x)\n```\n\nKey insight: PyTorch builds computation graph during `forward()`, enabling dynamic architectures.\n\n### Question 2: \"Why use ReLU instead of Sigmoid for hidden layers?\"\n\n**Answer**:\n| Property | ReLU | Sigmoid |\n|----------|------|---------|\n| Range | [0, ‚àû) | (0, 1) |\n| Gradient | 0 or 1 | 0 to 0.25 |\n| Computation | Fast (max) | Slow (exp) |\n| Vanishing Gradient | Only for x<0 | For |x|>2 |\n\nReLU advantages:\n1. **Sparse activation**: Only ~50% of neurons fire\n2. **No vanishing gradient** for positive values\n3. **Faster computation**: Just a max operation\n4. **Faster convergence**: ~6x faster than sigmoid\n\n### Question 3: \"What's the purpose of Batch Normalization?\"\n\n**Answer**:\nBatch Norm normalizes layer inputs to have mean=0, std=1, then applies learnable scale/shift.\n\n```python\n# BatchNorm does:\n# 1. Compute batch mean and variance\n# 2. Normalize: (x - mean) / sqrt(var + eps)\n# 3. Scale and shift: gamma * normalized + beta\n```\n\nBenefits:\n1. **Faster training**: Allows higher learning rates\n2. **Regularization**: Acts as noise (like dropout)\n3. **Reduces internal covariate shift**: Stabilizes layer inputs\n4. **Less sensitive to initialization**: More forgiving\n\n### Question 4: \"Explain the difference between model.train() and model.eval()\"\n\n**Answer**:\n```python\nmodel.train()  # Training mode\nmodel.eval()   # Evaluation mode\n```\n\nThese modes affect:\n1. **Dropout**: Active in train, disabled in eval\n2. **BatchNorm**: Uses batch stats in train, running stats in eval\n3. **Gradient behavior**: Unchanged (use `torch.no_grad()` separately)\n\nCommon mistake: Forgetting to call `model.eval()` before inference!\n\n### Question 5: \"How would you implement skip connections (residual connections)?\"\n\n**Answer**:\n```python\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.bn2 = nn.BatchNorm2d(channels)\n        \n    def forward(self, x):\n        residual = x  # Save input\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual  # Add skip connection!\n        return F.relu(out)\n```\n\nWhy skip connections work:\n1. **Gradient highway**: Gradients flow directly to early layers\n2. **Identity mapping**: Easy to learn \"do nothing\"\n3. **Enables very deep networks**: 100+ layers possible",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "dtcwcjftczf",
   "source": "## Summary: Neural Network Building Checklist\n\n### Foundation (Know These Cold)\n- [ ] `nn.Module` is the base class for all PyTorch models\n- [ ] `__init__` defines layers, `forward` defines data flow\n- [ ] Parameters are automatically tracked when using nn layers\n- [ ] Call `super().__init__()` in your module's `__init__`\n\n### Layers (Daily Usage)\n- [ ] `nn.Linear(in, out)` - Fully connected layer\n- [ ] `nn.Conv2d(in_ch, out_ch, kernel)` - Convolutional layer\n- [ ] `nn.BatchNorm2d(channels)` - Batch normalization\n- [ ] `nn.Dropout(p)` - Regularization via random zeroing\n\n### Activations (Interview Favorites)\n- [ ] ReLU - Default choice for hidden layers (fast, no vanishing gradient)\n- [ ] Sigmoid - Output for binary classification\n- [ ] Softmax - Output for multi-class classification\n- [ ] GELU/SiLU - Modern transformers\n\n### Containers (Architecture Patterns)\n- [ ] `nn.Sequential` - Simple layer stacking\n- [ ] `nn.ModuleList` - Dynamic/looped architectures\n- [ ] `nn.ModuleDict` - Named access (multi-task)\n\n### Advanced Topics (FAANG Level)\n- [ ] Weight initialization (Xavier for tanh, He for ReLU)\n- [ ] Layer freezing for transfer learning\n- [ ] Parameter counting and model analysis\n- [ ] Skip connections for deep networks\n\n---\n**Next**: Notebook 03 - Training Your First Model (The training loop)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}