{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f259846",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 19: RLHF and Alignment (The FAANG Standard)\n",
    "\n",
    "**Author:** [Your Name/Organization]  \n",
    "**Date:** 2025  \n",
    "\n",
    "Training an LLM to predict the next token is only half the battle. The raw model (Base Model) is often chaotic, repetitive, or even toxic. To make it a helpful assistant (like ChatGPT or Claude), we need **Alignment**.\n",
    "\n",
    "This tutorial covers the advanced techniques used to align models, focusing on the modern standard: **DPO (Direct Preference Optimization)**.\n",
    "\n",
    "## Learning Objectives\n",
    "1.  **Understand Alignment**: Why Supervised Fine-Tuning (SFT) isn't enough.\n",
    "2.  **RLHF vs. DPO**: The evolution from complex Reinforcement Learning to simple Probability Optimization.\n",
    "3.  **Implement DPO Loss**: Write the exact loss function used to align state-of-the-art models from scratch in PyTorch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e65bb",
   "metadata": {},
   "source": "## 1. Vocabulary First\n\nThis is a high-stakes interview topic. Know these terms cold.\n\n-   **SFT (Supervised Fine-Tuning)**: Training the model on high-quality \"Instruction -> Answer\" pairs. This teaches the *format* but not necessarily the *preference*.\n-   **Preference Data**: Data in the format `(Prompt, Chosen Response, Rejected Response)`. \"Chosen\" is better than \"Rejected\".\n-   **Reward Model**: A separate neural network trained to output a scalar score indicating how \"good\" a response is. Used in PPO.\n-   **RLHF (Reinforcement Learning from Human Feedback)**: The classic pipeline: SFT -> Reward Model -> PPO. Complex and unstable.\n-   **DPO (Direct Preference Optimization)**: A newer method (2023) that optimizes the policy *directly* on preference data without a separate reward model. Stable and efficient.\n-   **Reference Model**: A frozen copy of the SFT model. We want our new model to improve preferences *without drifting too far* from the reference (to prevent gibberish).\n\n### The Full Alignment Pipeline (3 Stages)\n\n```\nStage 1: Pre-training (Base Model)\n   └─ Next-token prediction on trillions of tokens\n   └─ Result: Knows language, facts, patterns, but is chaotic and uncontrollable\n   └─ Cost: $1M - $100M+ in compute\n\nStage 2: Supervised Fine-Tuning (SFT Model)\n   └─ Train on (instruction, response) pairs\n   └─ Result: Follows instructions, but often verbose, sycophantic, or unsafe\n   └─ Cost: $1K - $100K in compute\n\nStage 3: Alignment (Aligned Model)\n   └─ RLHF or DPO on preference data\n   └─ Result: Helpful, honest, and harmless responses\n   └─ Cost: $10K - $500K in compute\n```\n\n### Why SFT Alone Isn't Enough\n\nSFT teaches the model *what format* to respond in, but not *which response is better*. Consider:\n- **Prompt**: \"Explain quantum computing\"\n- **Response A**: Clear, concise, accurate 2-paragraph explanation\n- **Response B**: Verbose, vague, technically correct but confusing 5-paragraph essay\n\nBoth are valid (instruction, response) pairs for SFT. But a human clearly prefers A. Alignment training teaches the model to generate A-like responses over B-like responses.\n\n### RLHF vs DPO: The Key Difference\n\n**RLHF (Classic, Complex)**:\n```\n1. Collect preference data → Train Reward Model\n2. Use Reward Model to score LLM outputs\n3. Use PPO (Reinforcement Learning) to optimize the LLM\n4. Problems: Reward hacking, training instability, 3 models in memory\n```\n\n**DPO (Modern, Simple)**:\n```\n1. Collect preference data\n2. Directly optimize the LLM using preference pairs\n3. No reward model needed, no RL loop\n4. Just a clever loss function (derived from the RLHF objective)\n```\n\n**Why DPO won**: RLHF requires training and maintaining a separate reward model, running PPO (which is notoriously unstable), and keeping 3 models in GPU memory simultaneously (policy, reference, reward). DPO collapses all of this into a single loss function.\n\n### Reward Hacking\n\nIn RLHF, the model can learn to \"game\" the reward model instead of actually improving:\n- The reward model gives high scores to long responses → The LLM becomes extremely verbose\n- The reward model likes confident language → The LLM becomes overconfident even when wrong\n- The reward model prefers certain phrases → The LLM overuses those phrases\n\n**The Reference Model prevents this**: By penalizing the model for drifting too far from the SFT model (via KL divergence), we prevent it from collapsing into a reward-hacking mode.\n\n### Preference Data Quality\n\nThe quality of alignment depends entirely on the preference data:\n- **Who annotates?** Expert annotators > crowd workers > synthetic data (from stronger LLMs)\n- **How many pairs?** Typically 10K-100K preference pairs for good alignment\n- **Annotation disagreement?** If humans disagree on which response is better, the model will be confused. High inter-annotator agreement is critical.\n- **Diversity matters**: Preferences must cover diverse topics, tones, and difficulty levels"
  },
  {
   "cell_type": "markdown",
   "id": "f4178ee3",
   "metadata": {},
   "source": [
    "## 2. The Mathematics of DPO\n",
    "\n",
    "DPO is elegant because it derives a loss function directly from the optimal policy of the RLHF objective. \n",
    "\n",
    "The core idea is to increase the likelihood of the **Chosen** response ($y_w$) and decrease the likelihood of the **Rejected** response ($y_l$), weighted by how much the model already knows (the Reference Model).\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$ L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right] $$\n",
    "\n",
    "Where:\n",
    "-   $\\pi_\\theta$: The model we are training.\n",
    "-   $\\pi_{ref}$: The frozen reference model.\n",
    "-   $y_w$: Winning (Chosen) response.\n",
    "-   $y_l$: Losing (Rejected) response.\n",
    "-   $\\beta$: A hyperparameter (temperature) controlling deviation from the reference (usually 0.1).\n",
    "-   $\\sigma$: The Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1114b94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:12.476193Z",
     "iopub.status.busy": "2025-11-20T05:10:12.475968Z",
     "iopub.status.idle": "2025-11-20T05:10:13.348958Z",
     "shell.execute_reply": "2025-11-20T05:10:13.348501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116df6f",
   "metadata": {},
   "source": [
    "## 3. Implementing DPO Loss from Scratch\n",
    "\n",
    "This is the \"Whiteboard Coding\" part. We will implement the loss function assuming we have the log-probabilities of the chosen and rejected tokens.\n",
    "\n",
    "In a real training loop, you would:\n",
    "1.  Forward pass `Prompt + Chosen` through Policy Model -> Get LogProbs.\n",
    "2.  Forward pass `Prompt + Rejected` through Policy Model -> Get LogProbs.\n",
    "3.  Forward pass `Prompt + Chosen` through Reference Model -> Get LogProbs (No Grad).\n",
    "4.  Forward pass `Prompt + Rejected` through Reference Model -> Get LogProbs (No Grad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a946e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:13.350184Z",
     "iopub.status.busy": "2025-11-20T05:10:13.350072Z",
     "iopub.status.idle": "2025-11-20T05:10:13.352386Z",
     "shell.execute_reply": "2025-11-20T05:10:13.352032Z"
    }
   },
   "outputs": [],
   "source": [
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             ref_chosen_logps, ref_rejected_logps, \n",
    "             beta=0.1):\n",
    "    \"\"\"\n",
    "    Computes the DPO loss for a batch of preferences.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log-probs of chosen responses from the model being trained.\n",
    "        policy_rejected_logps: Log-probs of rejected responses from the model being trained.\n",
    "        ref_chosen_logps: Log-probs of chosen responses from the frozen reference model.\n",
    "        ref_rejected_logps: Log-probs of rejected responses from the frozen reference model.\n",
    "        beta: Temperature parameter (strength of the KL penalty).\n",
    "        \n",
    "    Returns:\n",
    "        losses: The loss for each example in the batch.\n",
    "        rewards_chosen: Implicit rewards for chosen examples (for logging).\n",
    "        rewards_rejected: Implicit rewards for rejected examples (for logging).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the log-ratio for the Policy Model\n",
    "    # log( pi(y|x) )\n",
    "    # We want to maximize (policy_chosen - policy_rejected)\n",
    "    \n",
    "    # 2. Calculate the log-ratio for the Reference Model\n",
    "    # log( ref(y|x) )\n",
    "    \n",
    "    # 3. The core DPO trick: Implicit Reward\n",
    "    # The \"reward\" is the difference between the policy and reference log-probs\n",
    "    # scaled by beta.\n",
    "    logr_chosen = policy_chosen_logps - ref_chosen_logps\n",
    "    logr_rejected = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # 4. The DPO objective maximizes the margin between chosen and rejected\n",
    "    logits = beta * (logr_chosen - logr_rejected)\n",
    "    \n",
    "    # 5. The Loss is -log(sigmoid(logits))\n",
    "    # F.logsigmoid is numerically more stable than log(sigmoid(x))\n",
    "    losses = -F.logsigmoid(logits)\n",
    "    \n",
    "    # Optional: Calculate \"Implicit Rewards\" for visualization\n",
    "    # This helps us see if the model is actually learning the preference\n",
    "    with torch.no_grad():\n",
    "        rewards_chosen = beta * logr_chosen\n",
    "        rewards_rejected = beta * logr_rejected\n",
    "        \n",
    "    return losses, rewards_chosen, rewards_rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65302a",
   "metadata": {},
   "source": [
    "## 4. Testing the Loss Function\n",
    "\n",
    "Let's verify this works with some dummy data. \n",
    "\n",
    "Imagine we have a batch of 2 examples.\n",
    "-   **Example 1**: The model assigns higher probability to the chosen response than the reference model. This is **GOOD**. Loss should be low.\n",
    "-   **Example 2**: The model assigns lower probability to the chosen response. This is **BAD**. Loss should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005e418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:13.353555Z",
     "iopub.status.busy": "2025-11-20T05:10:13.353488Z",
     "iopub.status.idle": "2025-11-20T05:10:13.379606Z",
     "shell.execute_reply": "2025-11-20T05:10:13.379292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inputs ---\n",
      "Policy Chosen LogProbs:   tensor([-10., -10.])\n",
      "Policy Rejected LogProbs: tensor([-15.,  -5.])\n",
      "\n",
      "--- Results ---\n",
      "Losses: tensor([0.0789, 2.5789])\n",
      "Rewards Chosen: tensor([0., 0.])\n",
      "Rewards Rejected: tensor([-2.5000,  2.5000])\n",
      "\n",
      "--- Interpretation ---\n",
      "Example 1 Loss: 0.0789 (Low, because Policy correctly disliked the rejected response)\n",
      "Example 2 Loss: 2.5789 (High, because Policy wrongly liked the rejected response)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data (Batch Size = 2)\n",
    "\n",
    "# Example 1: Good case (Policy prefers chosen more than Ref)\n",
    "# Example 2: Bad case (Policy prefers rejected more than Ref)\n",
    "\n",
    "policy_chosen = torch.tensor([-10.0, -10.0]) \n",
    "policy_rejected = torch.tensor([-15.0, -5.0]) # Ex 1: Rej is unlikely (-15). Ex 2: Rej is likely (-5).\n",
    "\n",
    "ref_chosen = torch.tensor([-10.0, -10.0])\n",
    "ref_rejected = torch.tensor([-10.0, -10.0])\n",
    "\n",
    "print(\"--- Inputs ---\")\n",
    "print(f\"Policy Chosen LogProbs:   {policy_chosen}\")\n",
    "print(f\"Policy Rejected LogProbs: {policy_rejected}\")\n",
    "\n",
    "losses, r_chosen, r_rejected = dpo_loss(\n",
    "    policy_chosen, policy_rejected,\n",
    "    ref_chosen, ref_rejected,\n",
    "    beta=0.5 # Higher beta = stronger constraint\n",
    ")\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Losses: {losses}\")\n",
    "print(f\"Rewards Chosen: {r_chosen}\")\n",
    "print(f\"Rewards Rejected: {r_rejected}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(f\"Example 1 Loss: {losses[0]:.4f} (Low, because Policy correctly disliked the rejected response)\")\n",
    "print(f\"Example 2 Loss: {losses[1]:.4f} (High, because Policy wrongly liked the rejected response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39e9ab",
   "metadata": {},
   "source": [
    "## 5. The Training Loop (Conceptual)\n",
    "\n",
    "In a real scenario, you would integrate this into a PyTorch loop.\n",
    "\n",
    "```python\n",
    "# Pseudocode for DPO Training Loop\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Forward Pass Policy\n",
    "    policy_logps_chosen = model(batch['chosen_ids'])\n",
    "    policy_logps_rejected = model(batch['rejected_ids'])\n",
    "    \n",
    "    # 2. Forward Pass Reference (No Grad)\n",
    "    with torch.no_grad():\n",
    "        ref_logps_chosen = ref_model(batch['chosen_ids'])\n",
    "        ref_logps_rejected = ref_model(batch['rejected_ids'])\n",
    "        \n",
    "    # 3. Compute Loss\n",
    "    loss, _, _ = dpo_loss(\n",
    "        policy_logps_chosen, policy_logps_rejected,\n",
    "        ref_logps_chosen, ref_logps_rejected\n",
    "    )\n",
    "    \n",
    "    # 4. Backprop\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61221a54",
   "metadata": {},
   "source": "## 6. Key Takeaways\n\n1.  **Why DPO?** It removes the need for a separate Reward Model and the unstable PPO loop. It optimizes the policy directly against the preference data. This is why most teams in 2025 use DPO (or its variants like IPO, KTO) instead of RLHF.\n\n2.  **The Reference Model**: Crucial for preventing the model from \"gaming\" the system or outputting gibberish. It acts as a regularizer (KL Divergence). Without it, the model can collapse into degenerate outputs that maximize the implicit reward.\n\n3.  **Beta**: The hyperparameter that controls how much we trust the reference model vs. the preference data. Higher beta = stay closer to reference (more conservative). Lower beta = optimize preferences more aggressively (risk of reward hacking).\n\n4.  **The 3-stage pipeline** (pre-training → SFT → alignment) is the standard for building production LLMs. Each stage serves a distinct purpose: knowledge, format, then preference.\n\n5.  **Preference data quality is everything**: The best alignment algorithm in the world can't fix bad preference data. Invest in high-quality, diverse, consistently-annotated preference pairs."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}