{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f259846",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 19: RLHF and Alignment (The FAANG Standard)\n",
    "\n",
    "**Author:** [Your Name/Organization]  \n",
    "**Date:** 2025  \n",
    "\n",
    "Training an LLM to predict the next token is only half the battle. The raw model (Base Model) is often chaotic, repetitive, or even toxic. To make it a helpful assistant (like ChatGPT or Claude), we need **Alignment**.\n",
    "\n",
    "This tutorial covers the advanced techniques used to align models, focusing on the modern standard: **DPO (Direct Preference Optimization)**.\n",
    "\n",
    "## Learning Objectives\n",
    "1.  **Understand Alignment**: Why Supervised Fine-Tuning (SFT) isn't enough.\n",
    "2.  **RLHF vs. DPO**: The evolution from complex Reinforcement Learning to simple Probability Optimization.\n",
    "3.  **Implement DPO Loss**: Write the exact loss function used to align state-of-the-art models from scratch in PyTorch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e65bb",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "This is a high-stakes interview topic. Know these terms cold.\n",
    "\n",
    "-   **SFT (Supervised Fine-Tuning)**: Training the model on high-quality \"Instruction -> Answer\" pairs. This teaches the *format* but not necessarily the *preference*.\n",
    "-   **Preference Data**: Data in the format `(Prompt, Chosen Response, Rejected Response)`. \"Chosen\" is better than \"Rejected\".\n",
    "-   **Reward Model**: A separate neural network trained to output a scalar score indicating how \"good\" a response is. Used in PPO.\n",
    "-   **RLHF (Reinforcement Learning from Human Feedback)**: The classic pipeline: SFT -> Reward Model -> PPO. Complex and unstable.\n",
    "-   **DPO (Direct Preference Optimization)**: A newer method (2023) that optimizes the policy *directly* on preference data without a separate reward model. Stable and efficient.\n",
    "-   **Reference Model**: A frozen copy of the SFT model. We want our new model to improve preferences *without drifting too far* from the reference (to prevent gibberish)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4178ee3",
   "metadata": {},
   "source": [
    "## 2. The Mathematics of DPO\n",
    "\n",
    "DPO is elegant because it derives a loss function directly from the optimal policy of the RLHF objective. \n",
    "\n",
    "The core idea is to increase the likelihood of the **Chosen** response ($y_w$) and decrease the likelihood of the **Rejected** response ($y_l$), weighted by how much the model already knows (the Reference Model).\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$ L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right] $$\n",
    "\n",
    "Where:\n",
    "-   $\\pi_\\theta$: The model we are training.\n",
    "-   $\\pi_{ref}$: The frozen reference model.\n",
    "-   $y_w$: Winning (Chosen) response.\n",
    "-   $y_l$: Losing (Rejected) response.\n",
    "-   $\\beta$: A hyperparameter (temperature) controlling deviation from the reference (usually 0.1).\n",
    "-   $\\sigma$: The Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1114b94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:12.476193Z",
     "iopub.status.busy": "2025-11-20T05:10:12.475968Z",
     "iopub.status.idle": "2025-11-20T05:10:13.348958Z",
     "shell.execute_reply": "2025-11-20T05:10:13.348501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116df6f",
   "metadata": {},
   "source": [
    "## 3. Implementing DPO Loss from Scratch\n",
    "\n",
    "This is the \"Whiteboard Coding\" part. We will implement the loss function assuming we have the log-probabilities of the chosen and rejected tokens.\n",
    "\n",
    "In a real training loop, you would:\n",
    "1.  Forward pass `Prompt + Chosen` through Policy Model -> Get LogProbs.\n",
    "2.  Forward pass `Prompt + Rejected` through Policy Model -> Get LogProbs.\n",
    "3.  Forward pass `Prompt + Chosen` through Reference Model -> Get LogProbs (No Grad).\n",
    "4.  Forward pass `Prompt + Rejected` through Reference Model -> Get LogProbs (No Grad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a946e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:13.350184Z",
     "iopub.status.busy": "2025-11-20T05:10:13.350072Z",
     "iopub.status.idle": "2025-11-20T05:10:13.352386Z",
     "shell.execute_reply": "2025-11-20T05:10:13.352032Z"
    }
   },
   "outputs": [],
   "source": [
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             ref_chosen_logps, ref_rejected_logps, \n",
    "             beta=0.1):\n",
    "    \"\"\"\n",
    "    Computes the DPO loss for a batch of preferences.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log-probs of chosen responses from the model being trained.\n",
    "        policy_rejected_logps: Log-probs of rejected responses from the model being trained.\n",
    "        ref_chosen_logps: Log-probs of chosen responses from the frozen reference model.\n",
    "        ref_rejected_logps: Log-probs of rejected responses from the frozen reference model.\n",
    "        beta: Temperature parameter (strength of the KL penalty).\n",
    "        \n",
    "    Returns:\n",
    "        losses: The loss for each example in the batch.\n",
    "        rewards_chosen: Implicit rewards for chosen examples (for logging).\n",
    "        rewards_rejected: Implicit rewards for rejected examples (for logging).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the log-ratio for the Policy Model\n",
    "    # log( pi(y|x) )\n",
    "    # We want to maximize (policy_chosen - policy_rejected)\n",
    "    \n",
    "    # 2. Calculate the log-ratio for the Reference Model\n",
    "    # log( ref(y|x) )\n",
    "    \n",
    "    # 3. The core DPO trick: Implicit Reward\n",
    "    # The \"reward\" is the difference between the policy and reference log-probs\n",
    "    # scaled by beta.\n",
    "    logr_chosen = policy_chosen_logps - ref_chosen_logps\n",
    "    logr_rejected = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # 4. The DPO objective maximizes the margin between chosen and rejected\n",
    "    logits = beta * (logr_chosen - logr_rejected)\n",
    "    \n",
    "    # 5. The Loss is -log(sigmoid(logits))\n",
    "    # F.logsigmoid is numerically more stable than log(sigmoid(x))\n",
    "    losses = -F.logsigmoid(logits)\n",
    "    \n",
    "    # Optional: Calculate \"Implicit Rewards\" for visualization\n",
    "    # This helps us see if the model is actually learning the preference\n",
    "    with torch.no_grad():\n",
    "        rewards_chosen = beta * logr_chosen\n",
    "        rewards_rejected = beta * logr_rejected\n",
    "        \n",
    "    return losses, rewards_chosen, rewards_rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65302a",
   "metadata": {},
   "source": [
    "## 4. Testing the Loss Function\n",
    "\n",
    "Let's verify this works with some dummy data. \n",
    "\n",
    "Imagine we have a batch of 2 examples.\n",
    "-   **Example 1**: The model assigns higher probability to the chosen response than the reference model. This is **GOOD**. Loss should be low.\n",
    "-   **Example 2**: The model assigns lower probability to the chosen response. This is **BAD**. Loss should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005e418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:10:13.353555Z",
     "iopub.status.busy": "2025-11-20T05:10:13.353488Z",
     "iopub.status.idle": "2025-11-20T05:10:13.379606Z",
     "shell.execute_reply": "2025-11-20T05:10:13.379292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inputs ---\n",
      "Policy Chosen LogProbs:   tensor([-10., -10.])\n",
      "Policy Rejected LogProbs: tensor([-15.,  -5.])\n",
      "\n",
      "--- Results ---\n",
      "Losses: tensor([0.0789, 2.5789])\n",
      "Rewards Chosen: tensor([0., 0.])\n",
      "Rewards Rejected: tensor([-2.5000,  2.5000])\n",
      "\n",
      "--- Interpretation ---\n",
      "Example 1 Loss: 0.0789 (Low, because Policy correctly disliked the rejected response)\n",
      "Example 2 Loss: 2.5789 (High, because Policy wrongly liked the rejected response)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data (Batch Size = 2)\n",
    "\n",
    "# Example 1: Good case (Policy prefers chosen more than Ref)\n",
    "# Example 2: Bad case (Policy prefers rejected more than Ref)\n",
    "\n",
    "policy_chosen = torch.tensor([-10.0, -10.0]) \n",
    "policy_rejected = torch.tensor([-15.0, -5.0]) # Ex 1: Rej is unlikely (-15). Ex 2: Rej is likely (-5).\n",
    "\n",
    "ref_chosen = torch.tensor([-10.0, -10.0])\n",
    "ref_rejected = torch.tensor([-10.0, -10.0])\n",
    "\n",
    "print(\"--- Inputs ---\")\n",
    "print(f\"Policy Chosen LogProbs:   {policy_chosen}\")\n",
    "print(f\"Policy Rejected LogProbs: {policy_rejected}\")\n",
    "\n",
    "losses, r_chosen, r_rejected = dpo_loss(\n",
    "    policy_chosen, policy_rejected,\n",
    "    ref_chosen, ref_rejected,\n",
    "    beta=0.5 # Higher beta = stronger constraint\n",
    ")\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Losses: {losses}\")\n",
    "print(f\"Rewards Chosen: {r_chosen}\")\n",
    "print(f\"Rewards Rejected: {r_rejected}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(f\"Example 1 Loss: {losses[0]:.4f} (Low, because Policy correctly disliked the rejected response)\")\n",
    "print(f\"Example 2 Loss: {losses[1]:.4f} (High, because Policy wrongly liked the rejected response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39e9ab",
   "metadata": {},
   "source": [
    "## 5. The Training Loop (Conceptual)\n",
    "\n",
    "In a real scenario, you would integrate this into a PyTorch loop.\n",
    "\n",
    "```python\n",
    "# Pseudocode for DPO Training Loop\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Forward Pass Policy\n",
    "    policy_logps_chosen = model(batch['chosen_ids'])\n",
    "    policy_logps_rejected = model(batch['rejected_ids'])\n",
    "    \n",
    "    # 2. Forward Pass Reference (No Grad)\n",
    "    with torch.no_grad():\n",
    "        ref_logps_chosen = ref_model(batch['chosen_ids'])\n",
    "        ref_logps_rejected = ref_model(batch['rejected_ids'])\n",
    "        \n",
    "    # 3. Compute Loss\n",
    "    loss, _, _ = dpo_loss(\n",
    "        policy_logps_chosen, policy_logps_rejected,\n",
    "        ref_logps_chosen, ref_logps_rejected\n",
    "    )\n",
    "    \n",
    "    # 4. Backprop\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61221a54",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways for Interviews\n",
    "\n",
    "1.  **Why DPO?** It removes the need for a separate Reward Model and the unstable PPO loop. It optimizes the policy directly against the preference data.\n",
    "2.  **The Reference Model**: Crucial for preventing the model from \"gaming\" the system or outputting gibberish. It acts as a regularizer (KL Divergence).\n",
    "3.  **Beta**: The hyperparameter that controls how much we trust the reference model vs. the preference data.\n",
    "\n",
    "You now have the code to implement the core of modern LLM alignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
