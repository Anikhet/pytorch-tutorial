{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Agentic AI Systems\n",
    "\n",
    "This notebook covers production-grade agentic AI systems - a critical skill for FAANG ML engineers building autonomous AI applications.\n",
    "\n",
    "## Topics Covered\n",
    "1. **ReAct Pattern** - Reasoning and Acting framework\n",
    "2. **Multi-Agent Systems** - Orchestrating multiple specialized agents\n",
    "3. **Agent Memory Systems** - Working, episodic, and long-term memory\n",
    "4. **Tool Use & Function Calling** - Safe tool execution patterns\n",
    "5. **Agent Evaluation & Monitoring** - Measuring agent performance\n",
    "6. **Production Patterns** - Reliability, safety, and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Callable, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import operator\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ReAct Agent Pattern\n",
    "\n",
    "ReAct (Reasoning + Acting) is a paradigm where agents interleave reasoning traces with actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(Enum):\n",
    "    \"\"\"Agent execution states\"\"\"\n",
    "    IDLE = \"idle\"\n",
    "    THINKING = \"thinking\"\n",
    "    ACTING = \"acting\"\n",
    "    OBSERVING = \"observing\"\n",
    "    FINISHED = \"finished\"\n",
    "    ERROR = \"error\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentAction:\n",
    "    \"\"\"Represents an action taken by the agent\"\"\"\n",
    "    tool_name: str\n",
    "    tool_input: Dict[str, Any]\n",
    "    reasoning: str\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentObservation:\n",
    "    \"\"\"Represents an observation from tool execution\"\"\"\n",
    "    content: str\n",
    "    success: bool\n",
    "    tool_name: str\n",
    "    execution_time: float\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentStep:\n",
    "    \"\"\"A single step in agent execution\"\"\"\n",
    "    thought: str\n",
    "    action: Optional[AgentAction]\n",
    "    observation: Optional[AgentObservation]\n",
    "    step_number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeMathParser:\n",
    "    \"\"\"\n",
    "    Safe mathematical expression parser.\n",
    "    Avoids eval() by using AST parsing with whitelisted operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Whitelisted operators\n",
    "    OPERATORS = {\n",
    "        ast.Add: operator.add,\n",
    "        ast.Sub: operator.sub,\n",
    "        ast.Mult: operator.mul,\n",
    "        ast.Div: operator.truediv,\n",
    "        ast.FloorDiv: operator.floordiv,\n",
    "        ast.Mod: operator.mod,\n",
    "        ast.Pow: operator.pow,\n",
    "        ast.USub: operator.neg,\n",
    "        ast.UAdd: operator.pos,\n",
    "    }\n",
    "    \n",
    "    # Whitelisted functions\n",
    "    FUNCTIONS = {\n",
    "        'abs': abs,\n",
    "        'round': round,\n",
    "        'min': min,\n",
    "        'max': max,\n",
    "        'sum': sum,\n",
    "        'sqrt': lambda x: x ** 0.5,\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def evaluate(cls, expression: str) -> float:\n",
    "        \"\"\"Safely evaluate a mathematical expression\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(expression, mode='eval')\n",
    "            return cls._eval_node(tree.body)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid expression: {expression}. Error: {e}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def _eval_node(cls, node) -> float:\n",
    "        \"\"\"Recursively evaluate AST nodes\"\"\"\n",
    "        if isinstance(node, ast.Constant):  # Numbers\n",
    "            if isinstance(node.value, (int, float)):\n",
    "                return node.value\n",
    "            raise ValueError(f\"Unsupported constant: {node.value}\")\n",
    "        \n",
    "        elif isinstance(node, ast.BinOp):  # Binary operations\n",
    "            op_type = type(node.op)\n",
    "            if op_type not in cls.OPERATORS:\n",
    "                raise ValueError(f\"Unsupported operator: {op_type}\")\n",
    "            left = cls._eval_node(node.left)\n",
    "            right = cls._eval_node(node.right)\n",
    "            return cls.OPERATORS[op_type](left, right)\n",
    "        \n",
    "        elif isinstance(node, ast.UnaryOp):  # Unary operations\n",
    "            op_type = type(node.op)\n",
    "            if op_type not in cls.OPERATORS:\n",
    "                raise ValueError(f\"Unsupported operator: {op_type}\")\n",
    "            operand = cls._eval_node(node.operand)\n",
    "            return cls.OPERATORS[op_type](operand)\n",
    "        \n",
    "        elif isinstance(node, ast.Call):  # Function calls\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                func_name = node.func.id\n",
    "                if func_name not in cls.FUNCTIONS:\n",
    "                    raise ValueError(f\"Unsupported function: {func_name}\")\n",
    "                args = [cls._eval_node(arg) for arg in node.args]\n",
    "                return cls.FUNCTIONS[func_name](*args)\n",
    "            raise ValueError(\"Unsupported function call\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported node type: {type(node)}\")\n",
    "\n",
    "\n",
    "# Test safe math parser\n",
    "parser = SafeMathParser()\n",
    "print(f\"2 + 3 * 4 = {parser.evaluate('2 + 3 * 4')}\")\n",
    "print(f\"sqrt(16) = {parser.evaluate('sqrt(16)')}\")\n",
    "print(f\"(10 + 5) / 3 = {parser.evaluate('(10 + 5) / 3')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(ABC):\n",
    "    \"\"\"Base class for agent tools\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def parameters(self) -> Dict[str, Any]:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, **kwargs) -> str:\n",
    "        pass\n",
    "    \n",
    "    def to_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert tool to OpenAI function schema format\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "                \"parameters\": self.parameters\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class CalculatorTool(Tool):\n",
    "    \"\"\"Safe calculator tool using AST-based parsing\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"calculator\"\n",
    "    \n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Performs mathematical calculations safely. Supports +, -, *, /, **, sqrt, abs, min, max.\"\n",
    "    \n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    \n",
    "    def execute(self, expression: str) -> str:\n",
    "        try:\n",
    "            result = SafeMathParser.evaluate(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "class SearchTool(Tool):\n",
    "    \"\"\"Simulated search tool\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knowledge_base = {\n",
    "            \"pytorch\": \"PyTorch is an open-source machine learning framework developed by Meta AI.\",\n",
    "            \"transformer\": \"Transformers are neural network architectures using self-attention mechanisms.\",\n",
    "            \"llm\": \"Large Language Models are AI models trained on vast text data for language tasks.\",\n",
    "            \"rag\": \"Retrieval-Augmented Generation combines retrieval with generation for accurate responses.\",\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"search\"\n",
    "    \n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Search for information on a given topic\"\n",
    "    \n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    \n",
    "    def execute(self, query: str) -> str:\n",
    "        query_lower = query.lower()\n",
    "        for key, value in self.knowledge_base.items():\n",
    "            if key in query_lower:\n",
    "                return f\"Found: {value}\"\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "\n",
    "# Test tools\n",
    "calc = CalculatorTool()\n",
    "search = SearchTool()\n",
    "\n",
    "print(calc.execute(expression=\"2 ** 10\"))\n",
    "print(search.execute(query=\"What is PyTorch?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    ReAct Agent implementing Reasoning + Acting pattern.\n",
    "    \n",
    "    The agent follows a loop:\n",
    "    1. Thought: Reason about what to do next\n",
    "    2. Action: Select and execute a tool\n",
    "    3. Observation: Process tool output\n",
    "    4. Repeat until task is complete\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: List[Tool],\n",
    "        max_steps: int = 10,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "        self.max_steps = max_steps\n",
    "        self.verbose = verbose\n",
    "        self.state = AgentState.IDLE\n",
    "        self.steps: List[AgentStep] = []\n",
    "        self.scratchpad = \"\"\n",
    "    \n",
    "    def _format_tools(self) -> str:\n",
    "        \"\"\"Format available tools for prompt\"\"\"\n",
    "        tool_descriptions = []\n",
    "        for name, tool in self.tools.items():\n",
    "            tool_descriptions.append(f\"- {name}: {tool.description}\")\n",
    "        return \"\\n\".join(tool_descriptions)\n",
    "    \n",
    "    def _parse_action(self, response: str) -> Optional[Tuple[str, Dict[str, Any]]]:\n",
    "        \"\"\"Parse action from LLM response\"\"\"\n",
    "        # Look for Action: tool_name(args) pattern\n",
    "        action_pattern = r\"Action:\\s*(\\w+)\\((.*)\\)\"\n",
    "        match = re.search(action_pattern, response)\n",
    "        \n",
    "        if match:\n",
    "            tool_name = match.group(1)\n",
    "            args_str = match.group(2)\n",
    "            \n",
    "            # Parse arguments (simplified)\n",
    "            args = {}\n",
    "            if args_str:\n",
    "                # Handle key=value format\n",
    "                arg_pattern = r'(\\w+)=[\"\\']?([^\"\\'\\),]+)[\"\\']?'\n",
    "                for arg_match in re.finditer(arg_pattern, args_str):\n",
    "                    args[arg_match.group(1)] = arg_match.group(2)\n",
    "                \n",
    "                # If no key=value, treat as single positional arg\n",
    "                if not args and args_str.strip():\n",
    "                    # Get first parameter name from tool\n",
    "                    if tool_name in self.tools:\n",
    "                        tool = self.tools[tool_name]\n",
    "                        params = tool.parameters.get(\"properties\", {})\n",
    "                        if params:\n",
    "                            first_param = list(params.keys())[0]\n",
    "                            args[first_param] = args_str.strip().strip('\"\\'')\n",
    "            \n",
    "            return tool_name, args\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _execute_tool(self, tool_name: str, args: Dict[str, Any]) -> AgentObservation:\n",
    "        \"\"\"Execute a tool and return observation\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if tool_name not in self.tools:\n",
    "            return AgentObservation(\n",
    "                content=f\"Tool '{tool_name}' not found. Available: {list(self.tools.keys())}\",\n",
    "                success=False,\n",
    "                tool_name=tool_name,\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            result = self.tools[tool_name].execute(**args)\n",
    "            return AgentObservation(\n",
    "                content=result,\n",
    "                success=True,\n",
    "                tool_name=tool_name,\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return AgentObservation(\n",
    "                content=f\"Error executing {tool_name}: {str(e)}\",\n",
    "                success=False,\n",
    "                tool_name=tool_name,\n",
    "                execution_time=time.time() - start_time\n",
    "            )\n",
    "    \n",
    "    def _simulate_llm_response(self, task: str, step: int) -> str:\n",
    "        \"\"\"\n",
    "        Simulate LLM response for demonstration.\n",
    "        In production, this would call an actual LLM API.\n",
    "        \"\"\"\n",
    "        # Simple rule-based simulation for demo\n",
    "        if step == 0:\n",
    "            if \"calculate\" in task.lower() or any(op in task for op in ['+', '-', '*', '/']):\n",
    "                # Extract math expression\n",
    "                numbers = re.findall(r'\\d+(?:\\.\\d+)?', task)\n",
    "                if len(numbers) >= 2:\n",
    "                    expr = f\"{numbers[0]} + {numbers[1]}\"\n",
    "                    if \"multiply\" in task.lower() or \"*\" in task:\n",
    "                        expr = f\"{numbers[0]} * {numbers[1]}\"\n",
    "                    elif \"divide\" in task.lower() or \"/\" in task:\n",
    "                        expr = f\"{numbers[0]} / {numbers[1]}\"\n",
    "                    return f\"Thought: I need to perform a calculation.\\nAction: calculator(expression=\\\"{expr}\\\")\"\n",
    "            elif \"search\" in task.lower() or \"what is\" in task.lower():\n",
    "                topic = task.split()[-1] if task.split() else \"topic\"\n",
    "                return f\"Thought: I need to search for information.\\nAction: search(query=\\\"{topic}\\\")\"\n",
    "        \n",
    "        return \"Thought: I have gathered enough information.\\nFinal Answer: Task completed based on observations.\"\n",
    "    \n",
    "    def run(self, task: str) -> str:\n",
    "        \"\"\"Execute the agent on a task\"\"\"\n",
    "        self.state = AgentState.THINKING\n",
    "        self.steps = []\n",
    "        self.scratchpad = f\"Task: {task}\\n\\nAvailable Tools:\\n{self._format_tools()}\\n\\n\"\n",
    "        \n",
    "        for step_num in range(self.max_steps):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n--- Step {step_num + 1} ---\")\n",
    "            \n",
    "            # Get LLM response (simulated)\n",
    "            response = self._simulate_llm_response(task, step_num)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Response: {response}\")\n",
    "            \n",
    "            # Check for final answer\n",
    "            if \"Final Answer:\" in response:\n",
    "                self.state = AgentState.FINISHED\n",
    "                final_answer = response.split(\"Final Answer:\")[-1].strip()\n",
    "                return final_answer\n",
    "            \n",
    "            # Parse and execute action\n",
    "            action_result = self._parse_action(response)\n",
    "            \n",
    "            if action_result:\n",
    "                tool_name, args = action_result\n",
    "                self.state = AgentState.ACTING\n",
    "                \n",
    "                # Extract thought\n",
    "                thought = \"\"\n",
    "                if \"Thought:\" in response:\n",
    "                    thought = response.split(\"Thought:\")[1].split(\"Action:\")[0].strip()\n",
    "                \n",
    "                action = AgentAction(\n",
    "                    tool_name=tool_name,\n",
    "                    tool_input=args,\n",
    "                    reasoning=thought\n",
    "                )\n",
    "                \n",
    "                # Execute tool\n",
    "                self.state = AgentState.OBSERVING\n",
    "                observation = self._execute_tool(tool_name, args)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Observation: {observation.content}\")\n",
    "                \n",
    "                # Record step\n",
    "                self.steps.append(AgentStep(\n",
    "                    thought=thought,\n",
    "                    action=action,\n",
    "                    observation=observation,\n",
    "                    step_number=step_num + 1\n",
    "                ))\n",
    "                \n",
    "                # Update scratchpad\n",
    "                self.scratchpad += f\"\\nStep {step_num + 1}:\\n\"\n",
    "                self.scratchpad += f\"Thought: {thought}\\n\"\n",
    "                self.scratchpad += f\"Action: {tool_name}({args})\\n\"\n",
    "                self.scratchpad += f\"Observation: {observation.content}\\n\"\n",
    "            else:\n",
    "                # No valid action found\n",
    "                self.state = AgentState.THINKING\n",
    "        \n",
    "        self.state = AgentState.ERROR\n",
    "        return \"Max steps reached without completing task.\"\n",
    "\n",
    "\n",
    "# Test ReAct Agent\n",
    "agent = ReActAgent(\n",
    "    tools=[CalculatorTool(), SearchTool()],\n",
    "    max_steps=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = agent.run(\"Calculate 15 multiply 20\")\n",
    "print(f\"\\nFinal Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Agent Systems\n",
    "\n",
    "Complex tasks often require multiple specialized agents working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentMessage:\n",
    "    \"\"\"Message passed between agents\"\"\"\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    content: str\n",
    "    message_type: str  # 'task', 'result', 'query', 'feedback'\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class SpecializedAgent:\n",
    "    \"\"\"Base class for specialized agents in a multi-agent system\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, specialty: str):\n",
    "        self.name = name\n",
    "        self.specialty = specialty\n",
    "        self.message_history: List[AgentMessage] = []\n",
    "    \n",
    "    def receive_message(self, message: AgentMessage) -> AgentMessage:\n",
    "        \"\"\"Process incoming message and generate response\"\"\"\n",
    "        self.message_history.append(message)\n",
    "        response_content = self._process_task(message.content)\n",
    "        \n",
    "        return AgentMessage(\n",
    "            sender=self.name,\n",
    "            receiver=message.sender,\n",
    "            content=response_content,\n",
    "            message_type=\"result\"\n",
    "        )\n",
    "    \n",
    "    def _process_task(self, task: str) -> str:\n",
    "        \"\"\"Override in subclasses for specialized processing\"\"\"\n",
    "        return f\"[{self.name}] Processed: {task}\"\n",
    "\n",
    "\n",
    "class ResearchAgent(SpecializedAgent):\n",
    "    \"\"\"Agent specialized in research and information gathering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ResearchAgent\", \"information_retrieval\")\n",
    "        self.knowledge = {\n",
    "            \"ml\": \"Machine Learning is a subset of AI focused on learning from data.\",\n",
    "            \"deep_learning\": \"Deep Learning uses neural networks with many layers.\",\n",
    "            \"nlp\": \"Natural Language Processing enables machines to understand human language.\"\n",
    "        }\n",
    "    \n",
    "    def _process_task(self, task: str) -> str:\n",
    "        task_lower = task.lower()\n",
    "        findings = []\n",
    "        \n",
    "        for key, value in self.knowledge.items():\n",
    "            if key.replace(\"_\", \" \") in task_lower or key in task_lower:\n",
    "                findings.append(value)\n",
    "        \n",
    "        if findings:\n",
    "            return f\"Research findings: {' '.join(findings)}\"\n",
    "        return \"No specific research findings. General analysis needed.\"\n",
    "\n",
    "\n",
    "class AnalysisAgent(SpecializedAgent):\n",
    "    \"\"\"Agent specialized in data analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"AnalysisAgent\", \"data_analysis\")\n",
    "    \n",
    "    def _process_task(self, task: str) -> str:\n",
    "        # Simulate analysis\n",
    "        if \"compare\" in task.lower():\n",
    "            return \"Comparative analysis: Both approaches have merits. Recommend hybrid solution.\"\n",
    "        elif \"evaluate\" in task.lower():\n",
    "            return \"Evaluation complete: 85% confidence in proposed solution.\"\n",
    "        else:\n",
    "            return \"Analysis: Data patterns suggest strong correlation with target metrics.\"\n",
    "\n",
    "\n",
    "class CodeAgent(SpecializedAgent):\n",
    "    \"\"\"Agent specialized in code generation and review\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"CodeAgent\", \"code_generation\")\n",
    "    \n",
    "    def _process_task(self, task: str) -> str:\n",
    "        if \"implement\" in task.lower() or \"code\" in task.lower():\n",
    "            return \"\"\"Generated code structure:\n",
    "```python\n",
    "class Solution:\n",
    "    def __init__(self):\n",
    "        self.initialized = True\n",
    "    \n",
    "    def process(self, data):\n",
    "        return self._transform(data)\n",
    "```\"\"\"\n",
    "        elif \"review\" in task.lower():\n",
    "            return \"Code review: No critical issues. Suggest adding error handling.\"\n",
    "        else:\n",
    "            return \"Code task received. Ready to generate implementation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorAgent:\n",
    "    \"\"\"\n",
    "    Supervisor agent that orchestrates multiple specialized agents.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Task decomposition\n",
    "    - Agent selection and routing\n",
    "    - Result aggregation\n",
    "    - Conflict resolution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agents: List[SpecializedAgent]):\n",
    "        self.agents = {agent.name: agent for agent in agents}\n",
    "        self.conversation_history: List[AgentMessage] = []\n",
    "        self.task_queue: deque = deque()\n",
    "    \n",
    "    def _decompose_task(self, task: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Decompose a complex task into subtasks for different agents.\n",
    "        Returns list of (agent_name, subtask) tuples.\n",
    "        \"\"\"\n",
    "        subtasks = []\n",
    "        task_lower = task.lower()\n",
    "        \n",
    "        # Simple rule-based decomposition (in production: use LLM)\n",
    "        if \"research\" in task_lower or \"find\" in task_lower or \"what is\" in task_lower:\n",
    "            subtasks.append((\"ResearchAgent\", f\"Research: {task}\"))\n",
    "        \n",
    "        if \"analyze\" in task_lower or \"compare\" in task_lower or \"evaluate\" in task_lower:\n",
    "            subtasks.append((\"AnalysisAgent\", f\"Analyze: {task}\"))\n",
    "        \n",
    "        if \"implement\" in task_lower or \"code\" in task_lower or \"build\" in task_lower:\n",
    "            subtasks.append((\"CodeAgent\", f\"Implement: {task}\"))\n",
    "        \n",
    "        # Default: send to all agents if no specific match\n",
    "        if not subtasks:\n",
    "            for agent_name in self.agents:\n",
    "                subtasks.append((agent_name, task))\n",
    "        \n",
    "        return subtasks\n",
    "    \n",
    "    def _aggregate_results(self, results: List[AgentMessage]) -> str:\n",
    "        \"\"\"Aggregate results from multiple agents\"\"\"\n",
    "        aggregated = \"\\n\\n=== Aggregated Results ===\\n\\n\"\n",
    "        \n",
    "        for result in results:\n",
    "            aggregated += f\"From {result.sender}:\\n{result.content}\\n\\n\"\n",
    "        \n",
    "        # Add synthesis (in production: use LLM for intelligent synthesis)\n",
    "        aggregated += \"=== Synthesis ===\\n\"\n",
    "        aggregated += \"Based on inputs from all agents, the task has been addressed comprehensively.\"\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def execute(self, task: str) -> str:\n",
    "        \"\"\"Execute a task using the multi-agent system\"\"\"\n",
    "        print(f\"\\nSupervisor received task: {task}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Decompose task\n",
    "        subtasks = self._decompose_task(task)\n",
    "        print(f\"\\nDecomposed into {len(subtasks)} subtasks\")\n",
    "        \n",
    "        # Execute subtasks\n",
    "        results = []\n",
    "        for agent_name, subtask in subtasks:\n",
    "            if agent_name in self.agents:\n",
    "                print(f\"\\nRouting to {agent_name}: {subtask}\")\n",
    "                \n",
    "                message = AgentMessage(\n",
    "                    sender=\"Supervisor\",\n",
    "                    receiver=agent_name,\n",
    "                    content=subtask,\n",
    "                    message_type=\"task\"\n",
    "                )\n",
    "                \n",
    "                response = self.agents[agent_name].receive_message(message)\n",
    "                results.append(response)\n",
    "                self.conversation_history.extend([message, response])\n",
    "                \n",
    "                print(f\"Response: {response.content[:100]}...\")\n",
    "        \n",
    "        # Aggregate results\n",
    "        final_result = self._aggregate_results(results)\n",
    "        return final_result\n",
    "\n",
    "\n",
    "# Test Multi-Agent System\n",
    "supervisor = SupervisorAgent([\n",
    "    ResearchAgent(),\n",
    "    AnalysisAgent(),\n",
    "    CodeAgent()\n",
    "])\n",
    "\n",
    "result = supervisor.execute(\n",
    "    \"Research machine learning, analyze the findings, and implement a simple solution\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Memory Systems\n",
    "\n",
    "Effective agents need various types of memory for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MemoryEntry:\n",
    "    \"\"\"A single memory entry\"\"\"\n",
    "    content: str\n",
    "    memory_type: str  # 'working', 'episodic', 'semantic', 'procedural'\n",
    "    timestamp: float\n",
    "    importance: float  # 0-1 scale\n",
    "    access_count: int = 0\n",
    "    last_accessed: float = None\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.last_accessed is None:\n",
    "            self.last_accessed = self.timestamp\n",
    "\n",
    "\n",
    "class AgentMemory:\n",
    "    \"\"\"\n",
    "    Comprehensive memory system for agents.\n",
    "    \n",
    "    Memory Types:\n",
    "    - Working Memory: Current context, limited capacity\n",
    "    - Short-term Memory: Recent interactions, moderate capacity\n",
    "    - Long-term Memory: Persistent knowledge, unlimited capacity\n",
    "    - Episodic Memory: Specific events/interactions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        working_memory_capacity: int = 5,\n",
    "        short_term_capacity: int = 50,\n",
    "        embedding_dim: int = 768\n",
    "    ):\n",
    "        self.working_memory_capacity = working_memory_capacity\n",
    "        self.short_term_capacity = short_term_capacity\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Memory stores\n",
    "        self.working_memory: deque = deque(maxlen=working_memory_capacity)\n",
    "        self.short_term_memory: List[MemoryEntry] = []\n",
    "        self.long_term_memory: List[MemoryEntry] = []\n",
    "        self.episodic_memory: List[MemoryEntry] = []\n",
    "        \n",
    "        # Simple embedding model (in production: use proper embeddings)\n",
    "        self.embedding_layer = nn.Linear(100, embedding_dim)\n",
    "    \n",
    "    def _compute_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Compute embedding for text (simplified)\"\"\"\n",
    "        # In production: use sentence-transformers or similar\n",
    "        # Simple hash-based embedding for demo\n",
    "        hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16)\n",
    "        np.random.seed(hash_val % (2**32))\n",
    "        return np.random.randn(self.embedding_dim).astype(np.float32)\n",
    "    \n",
    "    def _compute_importance(self, content: str) -> float:\n",
    "        \"\"\"Compute importance score for memory\"\"\"\n",
    "        # Simple heuristics (in production: use LLM or learned model)\n",
    "        importance = 0.5\n",
    "        \n",
    "        # Longer content might be more important\n",
    "        importance += min(len(content) / 1000, 0.2)\n",
    "        \n",
    "        # Keywords that indicate importance\n",
    "        important_keywords = ['error', 'important', 'critical', 'remember', 'key', 'must']\n",
    "        for keyword in important_keywords:\n",
    "            if keyword in content.lower():\n",
    "                importance += 0.1\n",
    "        \n",
    "        return min(importance, 1.0)\n",
    "    \n",
    "    def add_to_working_memory(self, content: str) -> None:\n",
    "        \"\"\"Add to working memory (most recent context)\"\"\"\n",
    "        entry = MemoryEntry(\n",
    "            content=content,\n",
    "            memory_type='working',\n",
    "            timestamp=time.time(),\n",
    "            importance=1.0,  # Working memory is always high priority\n",
    "            embedding=self._compute_embedding(content)\n",
    "        )\n",
    "        self.working_memory.append(entry)\n",
    "    \n",
    "    def add_to_short_term(self, content: str, memory_type: str = 'episodic') -> None:\n",
    "        \"\"\"Add to short-term memory with importance scoring\"\"\"\n",
    "        importance = self._compute_importance(content)\n",
    "        \n",
    "        entry = MemoryEntry(\n",
    "            content=content,\n",
    "            memory_type=memory_type,\n",
    "            timestamp=time.time(),\n",
    "            importance=importance,\n",
    "            embedding=self._compute_embedding(content)\n",
    "        )\n",
    "        \n",
    "        self.short_term_memory.append(entry)\n",
    "        \n",
    "        # Consolidate to long-term if exceeding capacity\n",
    "        if len(self.short_term_memory) > self.short_term_capacity:\n",
    "            self._consolidate_to_long_term()\n",
    "    \n",
    "    def add_episode(self, episode: Dict[str, Any]) -> None:\n",
    "        \"\"\"Add an episodic memory (specific interaction/event)\"\"\"\n",
    "        content = json.dumps(episode)\n",
    "        \n",
    "        entry = MemoryEntry(\n",
    "            content=content,\n",
    "            memory_type='episodic',\n",
    "            timestamp=time.time(),\n",
    "            importance=self._compute_importance(content),\n",
    "            embedding=self._compute_embedding(content),\n",
    "            metadata={'episode_type': episode.get('type', 'unknown')}\n",
    "        )\n",
    "        \n",
    "        self.episodic_memory.append(entry)\n",
    "    \n",
    "    def _consolidate_to_long_term(self) -> None:\n",
    "        \"\"\"Move important memories from short-term to long-term\"\"\"\n",
    "        # Sort by importance\n",
    "        self.short_term_memory.sort(key=lambda x: x.importance, reverse=True)\n",
    "        \n",
    "        # Keep top half in short-term, move rest to long-term\n",
    "        midpoint = len(self.short_term_memory) // 2\n",
    "        \n",
    "        for entry in self.short_term_memory[midpoint:]:\n",
    "            entry.memory_type = 'long_term'\n",
    "            self.long_term_memory.append(entry)\n",
    "        \n",
    "        self.short_term_memory = self.short_term_memory[:midpoint]\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, memory_types: List[str] = None) -> List[MemoryEntry]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant memories using similarity search.\n",
    "        \n",
    "        Uses combination of:\n",
    "        - Semantic similarity (embedding cosine similarity)\n",
    "        - Recency (time decay)\n",
    "        - Importance score\n",
    "        \"\"\"\n",
    "        query_embedding = self._compute_embedding(query)\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Collect all relevant memories\n",
    "        all_memories = []\n",
    "        \n",
    "        if memory_types is None:\n",
    "            memory_types = ['working', 'episodic', 'short_term', 'long_term']\n",
    "        \n",
    "        if 'working' in memory_types:\n",
    "            all_memories.extend(list(self.working_memory))\n",
    "        if 'short_term' in memory_types or 'episodic' in memory_types:\n",
    "            all_memories.extend(self.short_term_memory)\n",
    "        if 'long_term' in memory_types:\n",
    "            all_memories.extend(self.long_term_memory)\n",
    "        if 'episodic' in memory_types:\n",
    "            all_memories.extend(self.episodic_memory)\n",
    "        \n",
    "        # Score each memory\n",
    "        scored_memories = []\n",
    "        for memory in all_memories:\n",
    "            if memory.embedding is not None:\n",
    "                # Cosine similarity\n",
    "                similarity = np.dot(query_embedding, memory.embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(memory.embedding) + 1e-8\n",
    "                )\n",
    "                \n",
    "                # Time decay (exponential decay over hours)\n",
    "                time_diff_hours = (current_time - memory.timestamp) / 3600\n",
    "                recency_score = np.exp(-0.1 * time_diff_hours)\n",
    "                \n",
    "                # Combined score\n",
    "                score = 0.5 * similarity + 0.3 * recency_score + 0.2 * memory.importance\n",
    "                \n",
    "                scored_memories.append((score, memory))\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        scored_memories.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for score, memory in scored_memories[:top_k]:\n",
    "            memory.access_count += 1\n",
    "            memory.last_accessed = current_time\n",
    "            results.append(memory)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_context_window(self) -> str:\n",
    "        \"\"\"Get current context from working memory\"\"\"\n",
    "        context_parts = []\n",
    "        for entry in self.working_memory:\n",
    "            context_parts.append(entry.content)\n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def summarize_memory_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get memory system statistics\"\"\"\n",
    "        return {\n",
    "            \"working_memory_size\": len(self.working_memory),\n",
    "            \"working_memory_capacity\": self.working_memory_capacity,\n",
    "            \"short_term_memory_size\": len(self.short_term_memory),\n",
    "            \"long_term_memory_size\": len(self.long_term_memory),\n",
    "            \"episodic_memory_size\": len(self.episodic_memory),\n",
    "            \"total_memories\": (\n",
    "                len(self.working_memory) + len(self.short_term_memory) +\n",
    "                len(self.long_term_memory) + len(self.episodic_memory)\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "# Test Memory System\n",
    "memory = AgentMemory(working_memory_capacity=5, short_term_capacity=10)\n",
    "\n",
    "# Add some memories\n",
    "memory.add_to_working_memory(\"User asked about machine learning\")\n",
    "memory.add_to_working_memory(\"Explained neural networks\")\n",
    "memory.add_to_short_term(\"Important: User prefers PyTorch over TensorFlow\")\n",
    "memory.add_to_short_term(\"Discussed transformer architectures\")\n",
    "memory.add_episode({\"type\": \"error\", \"content\": \"API call failed\", \"resolution\": \"Retried successfully\"})\n",
    "\n",
    "# Retrieve relevant memories\n",
    "results = memory.retrieve(\"Tell me about neural networks\", top_k=3)\n",
    "print(\"Retrieved memories:\")\n",
    "for r in results:\n",
    "    print(f\"  - {r.content[:50]}... (importance: {r.importance:.2f})\")\n",
    "\n",
    "print(f\"\\nMemory stats: {memory.summarize_memory_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Production Agent Patterns\n",
    "\n",
    "Patterns for building reliable, safe, and scalable agent systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern for agent reliability.\n",
    "    Prevents cascading failures in multi-agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        failure_threshold: int = 5,\n",
    "        recovery_timeout: float = 60.0,\n",
    "        half_open_requests: int = 3\n",
    "    ):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.half_open_requests = half_open_requests\n",
    "        \n",
    "        self.failure_count = 0\n",
    "        self.success_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = \"closed\"  # closed, open, half-open\n",
    "    \n",
    "    def can_execute(self) -> bool:\n",
    "        \"\"\"Check if execution is allowed\"\"\"\n",
    "        if self.state == \"closed\":\n",
    "            return True\n",
    "        \n",
    "        if self.state == \"open\":\n",
    "            # Check if recovery timeout has passed\n",
    "            if time.time() - self.last_failure_time >= self.recovery_timeout:\n",
    "                self.state = \"half-open\"\n",
    "                self.success_count = 0\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        if self.state == \"half-open\":\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def record_success(self) -> None:\n",
    "        \"\"\"Record successful execution\"\"\"\n",
    "        self.success_count += 1\n",
    "        \n",
    "        if self.state == \"half-open\":\n",
    "            if self.success_count >= self.half_open_requests:\n",
    "                self.state = \"closed\"\n",
    "                self.failure_count = 0\n",
    "        \n",
    "        if self.state == \"closed\":\n",
    "            self.failure_count = max(0, self.failure_count - 1)\n",
    "    \n",
    "    def record_failure(self) -> None:\n",
    "        \"\"\"Record failed execution\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        \n",
    "        if self.state == \"half-open\":\n",
    "            self.state = \"open\"\n",
    "        elif self.state == \"closed\" and self.failure_count >= self.failure_threshold:\n",
    "            self.state = \"open\"\n",
    "\n",
    "\n",
    "class AgentRateLimiter:\n",
    "    \"\"\"\n",
    "    Rate limiter for agent API calls.\n",
    "    Uses token bucket algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokens_per_second: float = 10.0, max_tokens: int = 100):\n",
    "        self.tokens_per_second = tokens_per_second\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tokens = max_tokens\n",
    "        self.last_update = time.time()\n",
    "    \n",
    "    def acquire(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"Try to acquire tokens, return True if successful\"\"\"\n",
    "        self._refill()\n",
    "        \n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _refill(self) -> None:\n",
    "        \"\"\"Refill tokens based on elapsed time\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_update\n",
    "        self.tokens = min(\n",
    "            self.max_tokens,\n",
    "            self.tokens + elapsed * self.tokens_per_second\n",
    "        )\n",
    "        self.last_update = now\n",
    "\n",
    "\n",
    "class SafetyGuard:\n",
    "    \"\"\"\n",
    "    Safety guard for agent actions.\n",
    "    Prevents dangerous or unauthorized actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.blocked_patterns = [\n",
    "            r'rm\\s+-rf',\n",
    "            r'sudo\\s+',\n",
    "            r'DROP\\s+TABLE',\n",
    "            r'DELETE\\s+FROM.*WHERE\\s+1\\s*=\\s*1',\n",
    "            r'format\\s+c:',\n",
    "        ]\n",
    "        self.max_action_length = 10000\n",
    "        self.allowed_tools = set()\n",
    "    \n",
    "    def register_tool(self, tool_name: str) -> None:\n",
    "        \"\"\"Register an allowed tool\"\"\"\n",
    "        self.allowed_tools.add(tool_name)\n",
    "    \n",
    "    def check_action(self, action: AgentAction) -> Tuple[bool, str]:\n",
    "        \"\"\"Check if action is safe to execute\"\"\"\n",
    "        # Check tool is allowed\n",
    "        if self.allowed_tools and action.tool_name not in self.allowed_tools:\n",
    "            return False, f\"Tool '{action.tool_name}' is not in allowed list\"\n",
    "        \n",
    "        # Check action length\n",
    "        action_str = json.dumps(action.tool_input)\n",
    "        if len(action_str) > self.max_action_length:\n",
    "            return False, f\"Action too long: {len(action_str)} > {self.max_action_length}\"\n",
    "        \n",
    "        # Check for dangerous patterns\n",
    "        for pattern in self.blocked_patterns:\n",
    "            if re.search(pattern, action_str, re.IGNORECASE):\n",
    "                return False, f\"Blocked pattern detected: {pattern}\"\n",
    "        \n",
    "        return True, \"Action approved\"\n",
    "\n",
    "\n",
    "# Test production patterns\n",
    "circuit_breaker = AgentCircuitBreaker(failure_threshold=3)\n",
    "rate_limiter = AgentRateLimiter(tokens_per_second=5)\n",
    "safety_guard = SafetyGuard()\n",
    "\n",
    "safety_guard.register_tool(\"calculator\")\n",
    "safety_guard.register_tool(\"search\")\n",
    "\n",
    "# Test safety guard\n",
    "safe_action = AgentAction(\n",
    "    tool_name=\"calculator\",\n",
    "    tool_input={\"expression\": \"2 + 2\"},\n",
    "    reasoning=\"Simple calculation\"\n",
    ")\n",
    "\n",
    "unsafe_action = AgentAction(\n",
    "    tool_name=\"shell\",\n",
    "    tool_input={\"command\": \"rm -rf /\"},\n",
    "    reasoning=\"Dangerous command\"\n",
    ")\n",
    "\n",
    "print(f\"Safe action check: {safety_guard.check_action(safe_action)}\")\n",
    "print(f\"Unsafe action check: {safety_guard.check_action(unsafe_action)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation & Monitoring\n",
    "\n",
    "Measuring and monitoring agent performance in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentMetrics:\n",
    "    \"\"\"Metrics collected for agent execution\"\"\"\n",
    "    task_id: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    total_steps: int\n",
    "    successful_tool_calls: int\n",
    "    failed_tool_calls: int\n",
    "    total_tokens: int\n",
    "    task_completed: bool\n",
    "    error_message: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.successful_tool_calls + self.failed_tool_calls\n",
    "        return self.successful_tool_calls / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "class AgentEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates agent performance across multiple dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history: List[AgentMetrics] = []\n",
    "        self.evaluation_results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def evaluate_task_completion(\n",
    "        self,\n",
    "        task: str,\n",
    "        agent_output: str,\n",
    "        expected_output: str = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate task completion quality.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Completeness: Does the output address the task?\n",
    "        task_keywords = set(task.lower().split())\n",
    "        output_keywords = set(agent_output.lower().split())\n",
    "        keyword_overlap = len(task_keywords & output_keywords) / len(task_keywords) if task_keywords else 0\n",
    "        scores['completeness'] = min(keyword_overlap * 2, 1.0)\n",
    "        \n",
    "        # Length appropriateness\n",
    "        output_length = len(agent_output)\n",
    "        if output_length < 10:\n",
    "            scores['length_score'] = 0.2\n",
    "        elif output_length < 50:\n",
    "            scores['length_score'] = 0.5\n",
    "        elif output_length < 500:\n",
    "            scores['length_score'] = 1.0\n",
    "        else:\n",
    "            scores['length_score'] = 0.8  # Might be too verbose\n",
    "        \n",
    "        # Accuracy (if expected output provided)\n",
    "        if expected_output:\n",
    "            expected_keywords = set(expected_output.lower().split())\n",
    "            accuracy_overlap = len(output_keywords & expected_keywords) / len(expected_keywords) if expected_keywords else 0\n",
    "            scores['accuracy'] = accuracy_overlap\n",
    "        \n",
    "        # Overall score\n",
    "        scores['overall'] = np.mean(list(scores.values()))\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def evaluate_efficiency(\n",
    "        self,\n",
    "        metrics: AgentMetrics,\n",
    "        baseline_steps: int = 5,\n",
    "        baseline_duration: float = 10.0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate agent efficiency.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Step efficiency\n",
    "        scores['step_efficiency'] = min(baseline_steps / max(metrics.total_steps, 1), 1.0)\n",
    "        \n",
    "        # Time efficiency\n",
    "        scores['time_efficiency'] = min(baseline_duration / max(metrics.duration, 0.1), 1.0)\n",
    "        \n",
    "        # Tool success rate\n",
    "        scores['tool_success_rate'] = metrics.success_rate\n",
    "        \n",
    "        # Token efficiency (lower is better)\n",
    "        tokens_per_step = metrics.total_tokens / max(metrics.total_steps, 1)\n",
    "        scores['token_efficiency'] = min(500 / max(tokens_per_step, 1), 1.0)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def evaluate_safety(\n",
    "        self,\n",
    "        agent_steps: List[AgentStep],\n",
    "        safety_guard: SafetyGuard\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate agent safety behavior.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'total_actions': len(agent_steps),\n",
    "            'blocked_actions': 0,\n",
    "            'safety_score': 1.0,\n",
    "            'violations': []\n",
    "        }\n",
    "        \n",
    "        for step in agent_steps:\n",
    "            if step.action:\n",
    "                is_safe, message = safety_guard.check_action(step.action)\n",
    "                if not is_safe:\n",
    "                    results['blocked_actions'] += 1\n",
    "                    results['violations'].append({\n",
    "                        'step': step.step_number,\n",
    "                        'action': step.action.tool_name,\n",
    "                        'message': message\n",
    "                    })\n",
    "        \n",
    "        if results['total_actions'] > 0:\n",
    "            results['safety_score'] = 1.0 - (results['blocked_actions'] / results['total_actions'])\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class AgentMonitor:\n",
    "    \"\"\"\n",
    "    Real-time monitoring for agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_tasks: Dict[str, Dict[str, Any]] = {}\n",
    "        self.completed_tasks: List[Dict[str, Any]] = []\n",
    "        self.alerts: List[Dict[str, Any]] = []\n",
    "        \n",
    "        # Thresholds\n",
    "        self.max_duration_threshold = 60.0  # seconds\n",
    "        self.max_steps_threshold = 20\n",
    "        self.min_success_rate_threshold = 0.7\n",
    "    \n",
    "    def start_task(self, task_id: str, task: str) -> None:\n",
    "        \"\"\"Record task start\"\"\"\n",
    "        self.active_tasks[task_id] = {\n",
    "            'task': task,\n",
    "            'start_time': time.time(),\n",
    "            'steps': 0,\n",
    "            'successful_calls': 0,\n",
    "            'failed_calls': 0\n",
    "        }\n",
    "    \n",
    "    def record_step(self, task_id: str, success: bool) -> None:\n",
    "        \"\"\"Record a step in task execution\"\"\"\n",
    "        if task_id in self.active_tasks:\n",
    "            self.active_tasks[task_id]['steps'] += 1\n",
    "            if success:\n",
    "                self.active_tasks[task_id]['successful_calls'] += 1\n",
    "            else:\n",
    "                self.active_tasks[task_id]['failed_calls'] += 1\n",
    "            \n",
    "            self._check_alerts(task_id)\n",
    "    \n",
    "    def _check_alerts(self, task_id: str) -> None:\n",
    "        \"\"\"Check if any alert thresholds are exceeded\"\"\"\n",
    "        task_info = self.active_tasks[task_id]\n",
    "        \n",
    "        # Duration alert\n",
    "        duration = time.time() - task_info['start_time']\n",
    "        if duration > self.max_duration_threshold:\n",
    "            self._create_alert(task_id, 'duration_exceeded', f\"Duration {duration:.1f}s exceeds threshold\")\n",
    "        \n",
    "        # Steps alert\n",
    "        if task_info['steps'] > self.max_steps_threshold:\n",
    "            self._create_alert(task_id, 'steps_exceeded', f\"Steps {task_info['steps']} exceeds threshold\")\n",
    "        \n",
    "        # Success rate alert\n",
    "        total_calls = task_info['successful_calls'] + task_info['failed_calls']\n",
    "        if total_calls >= 5:\n",
    "            success_rate = task_info['successful_calls'] / total_calls\n",
    "            if success_rate < self.min_success_rate_threshold:\n",
    "                self._create_alert(task_id, 'low_success_rate', f\"Success rate {success_rate:.1%} below threshold\")\n",
    "    \n",
    "    def _create_alert(self, task_id: str, alert_type: str, message: str) -> None:\n",
    "        \"\"\"Create an alert\"\"\"\n",
    "        alert = {\n",
    "            'task_id': task_id,\n",
    "            'type': alert_type,\n",
    "            'message': message,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.alerts.append(alert)\n",
    "        logger.warning(f\"Agent Alert: {message}\")\n",
    "    \n",
    "    def end_task(self, task_id: str, success: bool) -> None:\n",
    "        \"\"\"Record task completion\"\"\"\n",
    "        if task_id in self.active_tasks:\n",
    "            task_info = self.active_tasks.pop(task_id)\n",
    "            task_info['end_time'] = time.time()\n",
    "            task_info['success'] = success\n",
    "            task_info['duration'] = task_info['end_time'] - task_info['start_time']\n",
    "            self.completed_tasks.append(task_info)\n",
    "    \n",
    "    def get_dashboard_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics for monitoring dashboard\"\"\"\n",
    "        completed = self.completed_tasks\n",
    "        \n",
    "        if not completed:\n",
    "            return {'message': 'No completed tasks yet'}\n",
    "        \n",
    "        return {\n",
    "            'total_tasks': len(completed),\n",
    "            'success_rate': sum(1 for t in completed if t['success']) / len(completed),\n",
    "            'avg_duration': np.mean([t['duration'] for t in completed]),\n",
    "            'avg_steps': np.mean([t['steps'] for t in completed]),\n",
    "            'active_tasks': len(self.active_tasks),\n",
    "            'total_alerts': len(self.alerts),\n",
    "            'recent_alerts': self.alerts[-5:] if self.alerts else []\n",
    "        }\n",
    "\n",
    "\n",
    "# Test evaluation and monitoring\n",
    "evaluator = AgentEvaluator()\n",
    "monitor = AgentMonitor()\n",
    "\n",
    "# Simulate task execution\n",
    "task_id = \"task_001\"\n",
    "monitor.start_task(task_id, \"Calculate 2 + 2\")\n",
    "\n",
    "for i in range(3):\n",
    "    monitor.record_step(task_id, success=True)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "monitor.end_task(task_id, success=True)\n",
    "\n",
    "# Evaluate\n",
    "task_scores = evaluator.evaluate_task_completion(\n",
    "    task=\"Calculate the sum of 2 and 2\",\n",
    "    agent_output=\"The sum of 2 and 2 is 4. I used the calculator tool to compute this.\",\n",
    "    expected_output=\"4\"\n",
    ")\n",
    "print(f\"Task completion scores: {task_scores}\")\n",
    "\n",
    "print(f\"\\nDashboard metrics: {monitor.get_dashboard_metrics()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Agentic RAG\n",
    "\n",
    "Combining agents with Retrieval-Augmented Generation for enhanced capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAG:\n",
    "    \"\"\"\n",
    "    Agent-enhanced RAG system.\n",
    "    \n",
    "    Combines:\n",
    "    - Intelligent query routing\n",
    "    - Multi-step retrieval\n",
    "    - Self-reflection and refinement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        knowledge_base: Dict[str, str],\n",
    "        embedding_dim: int = 128\n",
    "    ):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Pre-compute embeddings for knowledge base\n",
    "        self.kb_embeddings = {}\n",
    "        for key, value in knowledge_base.items():\n",
    "            self.kb_embeddings[key] = self._compute_embedding(value)\n",
    "    \n",
    "    def _compute_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Compute simple embedding (demo purposes)\"\"\"\n",
    "        hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16)\n",
    "        np.random.seed(hash_val % (2**32))\n",
    "        return np.random.randn(self.embedding_dim).astype(np.float32)\n",
    "    \n",
    "    def _route_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Route query to appropriate retrieval strategy.\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in ['compare', 'difference', 'versus', 'vs']):\n",
    "            return 'comparison'\n",
    "        elif any(word in query_lower for word in ['how to', 'steps', 'guide', 'tutorial']):\n",
    "            return 'procedural'\n",
    "        elif any(word in query_lower for word in ['what is', 'define', 'explain']):\n",
    "            return 'factual'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def _retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents.\n",
    "        \"\"\"\n",
    "        query_embedding = self._compute_embedding(query)\n",
    "        \n",
    "        scored_docs = []\n",
    "        for key, value in self.knowledge_base.items():\n",
    "            doc_embedding = self.kb_embeddings[key]\n",
    "            similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding) + 1e-8\n",
    "            )\n",
    "            scored_docs.append((key, value, float(similarity)))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "        return scored_docs[:top_k]\n",
    "    \n",
    "    def _decompose_query(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decompose complex query into sub-queries.\n",
    "        \"\"\"\n",
    "        sub_queries = [query]  # Always include original\n",
    "        \n",
    "        # Simple decomposition based on 'and'\n",
    "        if ' and ' in query.lower():\n",
    "            parts = query.lower().split(' and ')\n",
    "            sub_queries.extend(parts)\n",
    "        \n",
    "        # Handle comparison queries\n",
    "        if 'compare' in query.lower() or 'vs' in query.lower():\n",
    "            # Extract entities being compared\n",
    "            words = query.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if word.lower() in ['vs', 'versus', 'and', 'compare']:\n",
    "                    if i > 0:\n",
    "                        sub_queries.append(f\"what is {words[i-1]}\")\n",
    "                    if i < len(words) - 1:\n",
    "                        sub_queries.append(f\"what is {words[i+1]}\")\n",
    "        \n",
    "        return list(set(sub_queries))\n",
    "    \n",
    "    def _self_reflect(self, query: str, response: str, context: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Self-reflection to evaluate response quality.\n",
    "        \"\"\"\n",
    "        reflection = {\n",
    "            'is_grounded': False,\n",
    "            'is_complete': False,\n",
    "            'needs_refinement': False,\n",
    "            'missing_aspects': []\n",
    "        }\n",
    "        \n",
    "        # Check if response is grounded in context\n",
    "        response_words = set(response.lower().split())\n",
    "        context_text = ' '.join(context).lower()\n",
    "        context_words = set(context_text.split())\n",
    "        \n",
    "        overlap = len(response_words & context_words) / len(response_words) if response_words else 0\n",
    "        reflection['is_grounded'] = overlap > 0.3\n",
    "        \n",
    "        # Check if response addresses query\n",
    "        query_keywords = set(query.lower().split()) - {'what', 'is', 'the', 'a', 'an', 'how', 'to'}\n",
    "        addressed = sum(1 for kw in query_keywords if kw in response.lower())\n",
    "        reflection['is_complete'] = addressed / len(query_keywords) > 0.5 if query_keywords else True\n",
    "        \n",
    "        # Identify missing aspects\n",
    "        for kw in query_keywords:\n",
    "            if kw not in response.lower():\n",
    "                reflection['missing_aspects'].append(kw)\n",
    "        \n",
    "        reflection['needs_refinement'] = not reflection['is_grounded'] or not reflection['is_complete']\n",
    "        \n",
    "        return reflection\n",
    "    \n",
    "    def query(self, query: str, max_iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute agentic RAG query.\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'query': query,\n",
    "            'route': self._route_query(query),\n",
    "            'iterations': [],\n",
    "            'final_response': '',\n",
    "            'sources': []\n",
    "        }\n",
    "        \n",
    "        # Decompose query\n",
    "        sub_queries = self._decompose_query(query)\n",
    "        \n",
    "        all_context = []\n",
    "        for sub_query in sub_queries:\n",
    "            retrieved = self._retrieve(sub_query, top_k=2)\n",
    "            for key, value, score in retrieved:\n",
    "                if value not in all_context:\n",
    "                    all_context.append(value)\n",
    "                    result['sources'].append({'key': key, 'score': score})\n",
    "        \n",
    "        # Generate initial response\n",
    "        response = f\"Based on the retrieved information: {' '.join(all_context[:2])}\"\n",
    "        \n",
    "        # Self-reflection loop\n",
    "        for i in range(max_iterations):\n",
    "            reflection = self._self_reflect(query, response, all_context)\n",
    "            result['iterations'].append({\n",
    "                'iteration': i + 1,\n",
    "                'response_preview': response[:100],\n",
    "                'reflection': reflection\n",
    "            })\n",
    "            \n",
    "            if not reflection['needs_refinement']:\n",
    "                break\n",
    "            \n",
    "            # Refine response (simplified)\n",
    "            if reflection['missing_aspects']:\n",
    "                # Retrieve more for missing aspects\n",
    "                for aspect in reflection['missing_aspects'][:2]:\n",
    "                    additional = self._retrieve(aspect, top_k=1)\n",
    "                    for key, value, score in additional:\n",
    "                        if value not in all_context:\n",
    "                            all_context.append(value)\n",
    "                \n",
    "                response = f\"Refined response based on {len(all_context)} sources: {' '.join(all_context[:3])}\"\n",
    "        \n",
    "        result['final_response'] = response\n",
    "        return result\n",
    "\n",
    "\n",
    "# Test Agentic RAG\n",
    "knowledge_base = {\n",
    "    \"pytorch\": \"PyTorch is an open-source machine learning framework developed by Meta AI. It's known for dynamic computation graphs and Pythonic design.\",\n",
    "    \"tensorflow\": \"TensorFlow is Google's open-source machine learning framework. It uses static computation graphs and has strong production deployment support.\",\n",
    "    \"transformer\": \"Transformers are neural network architectures that use self-attention mechanisms. They power models like BERT and GPT.\",\n",
    "    \"bert\": \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google.\",\n",
    "    \"gpt\": \"GPT (Generative Pre-trained Transformer) is a family of language models developed by OpenAI.\"\n",
    "}\n",
    "\n",
    "agentic_rag = AgenticRAG(knowledge_base)\n",
    "result = agentic_rag.query(\"Compare PyTorch and TensorFlow for deep learning\")\n",
    "\n",
    "print(f\"Query Route: {result['route']}\")\n",
    "print(f\"Iterations: {len(result['iterations'])}\")\n",
    "print(f\"Sources used: {len(result['sources'])}\")\n",
    "print(f\"\\nFinal Response: {result['final_response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAANG Interview Questions\n",
    "\n",
    "### Q1: How would you design a multi-agent system for a complex task like code review?\n",
    "\n",
    "**Answer:**\n",
    "I would design a hierarchical multi-agent system:\n",
    "\n",
    "1. **Supervisor Agent**: Orchestrates the review process, routes tasks, aggregates findings\n",
    "2. **Specialized Agents**:\n",
    "   - **Security Agent**: Scans for vulnerabilities (OWASP Top 10, injection risks)\n",
    "   - **Style Agent**: Checks coding standards, naming conventions\n",
    "   - **Logic Agent**: Analyzes correctness, edge cases, race conditions\n",
    "   - **Performance Agent**: Identifies bottlenecks, complexity issues\n",
    "   - **Documentation Agent**: Checks comments, docstrings, API docs\n",
    "\n",
    "3. **Communication Pattern**: Pub/sub for parallel execution, shared memory for context\n",
    "4. **Conflict Resolution**: Priority-based (security > correctness > performance > style)\n",
    "5. **Output**: Aggregated report with severity rankings and suggested fixes\n",
    "\n",
    "### Q2: What memory systems would you implement for a long-running agent?\n",
    "\n",
    "**Answer:**\n",
    "I would implement a hierarchical memory system:\n",
    "\n",
    "1. **Working Memory** (Limited, ~5 items): Current context, active task state\n",
    "2. **Short-term Memory** (Moderate, ~50 items): Recent interactions, session context\n",
    "3. **Long-term Memory** (Unlimited): Persistent knowledge, user preferences\n",
    "4. **Episodic Memory**: Specific past interactions for few-shot learning\n",
    "\n",
    "Key components:\n",
    "- **Importance scoring** for memory consolidation\n",
    "- **Vector similarity search** for retrieval\n",
    "- **Time decay** for recency bias\n",
    "- **Compression** for long-term storage efficiency\n",
    "\n",
    "### Q3: How do you ensure agent safety in production?\n",
    "\n",
    "**Answer:**\n",
    "Multiple layers of safety:\n",
    "\n",
    "1. **Action Validation**: Whitelist allowed tools, block dangerous patterns\n",
    "2. **Rate Limiting**: Token bucket algorithm to prevent runaway costs\n",
    "3. **Circuit Breaker**: Stop execution on repeated failures\n",
    "4. **Sandboxing**: Execute tool calls in isolated environments\n",
    "5. **Human-in-the-Loop**: Require approval for high-risk actions\n",
    "6. **Monitoring**: Real-time alerts for anomalous behavior\n",
    "7. **Audit Logging**: Complete trace of all actions for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **ReAct Pattern**: Reasoning and acting loop for structured problem-solving\n",
    "2. **Multi-Agent Systems**: Orchestrating specialized agents with supervisors\n",
    "3. **Memory Systems**: Hierarchical memory with working, short-term, and long-term stores\n",
    "4. **Production Patterns**: Circuit breakers, rate limiters, safety guards\n",
    "5. **Evaluation & Monitoring**: Metrics, dashboards, and alerting\n",
    "6. **Agentic RAG**: Enhanced retrieval with query decomposition and self-reflection\n",
    "\n",
    "### Key Takeaways for FAANG Interviews:\n",
    "- Agents require careful design for reliability and safety\n",
    "- Memory systems enable context retention across interactions\n",
    "- Multi-agent systems need clear communication and coordination patterns\n",
    "- Production systems require monitoring, circuit breakers, and rate limiting\n",
    "- Safety is paramount: always validate actions and maintain audit trails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
