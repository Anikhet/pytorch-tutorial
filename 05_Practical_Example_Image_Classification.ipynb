{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a465a3",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Image Classification\n",
    "\n",
    "Regression is cool, but AI is famous for **Vision**. \n",
    "\n",
    "In this notebook, we will teach a computer to \"see\" by classifying images (FashionMNIST). \n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Pixel\", \"Channel\", \"Class\", \"Logits\", and \"Softmax\"?\n",
    "- **The Intuition**: How a computer sees an image (Grid of numbers).\n",
    "- **The Practice**: Building a classifier for 10 types of clothing.\n",
    "- **The Visual**: Seeing the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e26fc",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Computer Vision has its own language. Let's learn it.\n",
    "\n",
    "### 1. Pixel\n",
    "- The smallest dot in an image.\n",
    "- Represented as a number from 0 (Black) to 255 (White).\n",
    "- In PyTorch, we usually normalize this to be between 0 and 1.\n",
    "\n",
    "### 2. Channel\n",
    "- The color layers of an image.\n",
    "- **Grayscale**: 1 Channel (Brightness).\n",
    "- **Color (RGB)**: 3 Channels (Red, Green, Blue).\n",
    "\n",
    "### 3. Class\n",
    "- The category we want to predict.\n",
    "- Example: \"T-Shirt\", \"Trouser\", \"Sneaker\".\n",
    "- We map these to numbers: 0, 1, 2...\n",
    "\n",
    "### 4. Logits\n",
    "- The raw, unnormalized scores coming out of the last layer of the model.\n",
    "- They can be any number (e.g., -5.2, 12.8).\n",
    "- Higher score = Higher confidence.\n",
    "\n",
    "### 5. Softmax\n",
    "- A function that turns Logits into Probabilities (0 to 1).\n",
    "- It makes sure all probabilities add up to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c8c93",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (The Grid)\n",
    "\n",
    "To you, an image is a picture of a shoe.\n",
    "To a computer, an image is just a **Grid of Numbers**.\n",
    "\n",
    "If you have a 28x28 image, you have 784 numbers.\n",
    "The model's job is to look at these 784 numbers and say \"That pattern looks like a Shoe\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d2ckq3oisp5",
   "source": "import numpy as np\n\n# Create a simple demonstration\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Get a sample image from FashionMNIST\nsample_img = images[0].squeeze().numpy()\n\n# 1. Original image\naxes[0, 0].imshow(sample_img, cmap='gray')\naxes[0, 0].set_title('What YOU See\\n(A Shoe)', fontsize=12, fontweight='bold')\naxes[0, 0].axis('off')\n\n# 2. Pixel values as heatmap\nim = axes[0, 1].imshow(sample_img, cmap='viridis')\naxes[0, 1].set_title('What COMPUTER Sees\\n(Grid of Numbers)', fontsize=12, fontweight='bold')\naxes[0, 1].axis('off')\nplt.colorbar(im, ax=axes[0, 1], fraction=0.046, pad=0.04)\n\n# 3. Zoomed section showing actual numbers\nzoom_size = 8\naxes[0, 2].imshow(sample_img[:zoom_size, :zoom_size], cmap='gray')\naxes[0, 2].set_title(f'Zoomed {zoom_size}x{zoom_size} Section', fontsize=12, fontweight='bold')\nfor i in range(zoom_size):\n    for j in range(zoom_size):\n        text = axes[0, 2].text(j, i, f'{sample_img[i, j]:.2f}',\n                              ha=\"center\", va=\"center\", color=\"red\", fontsize=7, fontweight='bold')\naxes[0, 2].set_xticks(range(zoom_size))\naxes[0, 2].set_yticks(range(zoom_size))\naxes[0, 2].grid(True, color='yellow', linewidth=1.5)\n\n# 4. 1D representation (flattened)\naxes[1, 0].plot(sample_img.flatten()[:200], linewidth=1)\naxes[1, 0].set_xlabel('Pixel Index', fontsize=11)\naxes[1, 0].set_ylabel('Pixel Value', fontsize=11)\naxes[1, 0].set_title('Flattened to 1D Vector\\n(First 200 pixels)', fontsize=12, fontweight='bold')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. Histogram of pixel values\naxes[1, 1].hist(sample_img.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\naxes[1, 1].set_xlabel('Pixel Value', fontsize=11)\naxes[1, 1].set_ylabel('Frequency', fontsize=11)\naxes[1, 1].set_title('Distribution of Pixel Values', fontsize=12, fontweight='bold')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\n# 6. Shape information\naxes[1, 2].axis('off')\ninfo_text = f\"\"\"\nImage Properties:\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nShape:     {sample_img.shape}\nTotal Pixels: {sample_img.size}\nData Type:    {sample_img.dtype}\nValue Range:  [{sample_img.min():.2f}, {sample_img.max():.2f}]\n\nRepresentation:\nâ€¢ 2D Array of numbers\nâ€¢ Each number = brightness\nâ€¢ 28 rows Ã— 28 columns\nâ€¢ Total: 784 input features\n\nFor the neural network:\nInput vector has 784 values!\n\"\"\"\naxes[1, 2].text(0.1, 0.5, info_text, fontsize=10, family='monospace',\n               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n               verticalalignment='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Insight:\")\nprint(\"â€¢ Humans see: shapes, patterns, objects\")\nprint(\"â€¢ Computers see: 784 numbers between 0 and 1\")\nprint(\"â€¢ Neural networks learn to map these numbers to labels!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "slpir8llke",
   "source": "### Visualization: How Computers See Images\n\nLet's visualize what an image looks like to a computer - just numbers!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6d951f98",
   "metadata": {},
   "source": [
    "## Part 3: The Data (FashionMNIST)\n",
    "\n",
    "We will use FashionMNIST. It's like \"Hello World\" for vision, but harder than digits.\n",
    "- Images: 28x28 Grayscale.\n",
    "- Classes: 10 (T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Boot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47218e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Visualize one batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f\"Image Batch Shape: {images.shape} (Batch, Channel, Height, Width)\")\n",
    "print(f\"Label Batch Shape: {labels.shape}\")\n",
    "\n",
    "# Show first image\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ecf86",
   "metadata": {},
   "source": [
    "## Part 4: The Model (Flattening)\n",
    "\n",
    "Since we are using Linear Layers (for now), we need to **Flatten** the 2D image (28x28) into a 1D vector (784).\n",
    "\n",
    "Imagine taking the image and cutting it into strips, then laying them end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28 * 28, 128) # 784 -> 128\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 10)      # 128 -> 10 Classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x) # Output Logits\n",
    "        return x\n",
    "\n",
    "model = VisionNet()\n",
    "criterion = nn.CrossEntropyLoss() # Combines Softmax + NLLLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90336c",
   "metadata": {},
   "source": [
    "## Part 5: Training (The Loop)\n",
    "\n",
    "Same loop, but now we iterate over the `trainloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418adf2",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation (Accuracy)\n",
    "\n",
    "How many did we get right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de79c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "7th5fjuurdy",
   "source": "# Comprehensive visualization of classification results\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\n# Class names for FashionMNIST\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Get predictions for test set\nall_predictions = []\nall_labels = []\nall_probs = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in testloader:\n        outputs = model(images)\n        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu.numpy())\n        all_probs.extend(probabilities.cpu().numpy())\n\nall_predictions = np.array(all_predictions)\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)\n\n# Create comprehensive dashboard\nfig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n\n# 1. Confusion Matrix\nax1 = fig.add_subplot(gs[0:2, 0:2])\ncm = confusion_matrix(all_labels, all_predictions)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n            yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})\nax1.set_xlabel('Predicted Label', fontsize=11)\nax1.set_ylabel('True Label', fontsize=11)\nax1.set_title('Confusion Matrix\\n(Diagonal = Correct Predictions)', fontsize=12, fontweight='bold')\nplt.setp(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=9)\nplt.setp(ax1.get_yticklabels(), rotation=0, fontsize=9)\n\n# 2. Per-class accuracy\nax2 = fig.add_subplot(gs[0, 2:])\nclass_correct = cm.diagonal()\nclass_total = cm.sum(axis=1)\nclass_accuracy = class_correct / class_total * 100\n\ncolors = ['green' if acc > 80 else 'orange' if acc > 70 else 'red' for acc in class_accuracy]\nbars = ax2.barh(range(10), class_accuracy, color=colors, edgecolor='black')\nax2.set_yticks(range(10))\nax2.set_yticklabels(class_names, fontsize=9)\nax2.set_xlabel('Accuracy (%)', fontsize=11)\nax2.set_title('Per-Class Accuracy', fontsize=12, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='x')\nax2.axvline(x=80, color='green', linestyle='--', linewidth=1, alpha=0.5)\n\nfor i, (bar, acc) in enumerate(zip(bars, class_accuracy)):\n    ax2.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=9, fontweight='bold')\n\n# 3. Prediction confidence distribution\nax3 = fig.add_subplot(gs[1, 2])\nmax_probs = all_probs.max(axis=1)\ncorrect_mask = all_predictions == all_labels\n\nax3.hist(max_probs[correct_mask], bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\nax3.hist(max_probs[~correct_mask], bins=50, alpha=0.7, label='Wrong', color='red', edgecolor='black')\nax3.set_xlabel('Confidence', fontsize=11)\nax3.set_ylabel('Count', fontsize=11)\nax3.set_title('Prediction Confidence\\n(Higher = More Certain)', fontsize=11, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3, axis='y')\n\n# 4. Accuracy by confidence\nax4 = fig.add_subplot(gs[1, 3])\nconfidence_bins = np.linspace(0, 1, 11)\nbin_accuracies = []\nbin_centers = []\n\nfor i in range(len(confidence_bins) - 1):\n    mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])\n    if mask.sum() > 0:\n        acc = (all_predictions[mask] == all_labels[mask]).mean() * 100\n        bin_accuracies.append(acc)\n        bin_centers.append((confidence_bins[i] + confidence_bins[i+1]) / 2)\n\nax4.plot(bin_centers, bin_accuracies, marker='o', linewidth=2, markersize=8, color='blue')\nax4.plot([0, 1], [0, 100], 'r--', linewidth=1, alpha=0.5, label='Perfect Calibration')\nax4.set_xlabel('Confidence', fontsize=11)\nax4.set_ylabel('Accuracy (%)', fontsize=11)\nax4.set_title('Calibration Curve', fontsize=11, fontweight='bold')\nax4.grid(True, alpha=0.3)\nax4.legend()\nax4.set_xlim(0, 1)\nax4.set_ylim(0, 105)\n\n# 5. Sample predictions (correct)\nax5 = fig.add_subplot(gs[2, 0:2])\nax5.axis('off')\n\n# Get some correct predictions\ncorrect_indices = np.where(correct_mask)[0][:6]\ny_pos = 0.9\n\nfor idx, test_idx in enumerate(correct_indices):\n    col = idx % 6\n    \n    # Get image\n    test_image = testset.data[test_idx].numpy()\n    \n    # Create small subplot\n    left = 0.02 + col * 0.16\n    ax_img = fig.add_axes([left, 0.05, 0.12, 0.12])\n    ax_img.imshow(test_image, cmap='gray')\n    ax_img.axis('off')\n    \n    true_label = all_labels[test_idx]\n    pred_label = all_predictions[test_idx]\n    confidence = max_probs[test_idx]\n    \n    title = f'{class_names[pred_label]}\\n{confidence*100:.0f}% âœ“'\n    ax_img.set_title(title, fontsize=9, color='green', fontweight='bold')\n\nax5.text(0.5, 0.95, 'Correct Predictions (High Confidence)', ha='center', \n        fontsize=12, fontweight='bold', transform=ax5.transAxes)\n\n# 6. Sample predictions (incorrect)\nax6 = fig.add_subplot(gs[2, 2:])\nax6.axis('off')\n\n# Get some incorrect predictions\nincorrect_indices = np.where(~correct_mask)[0][:6]\n\nfor idx, test_idx in enumerate(incorrect_indices):\n    col = idx % 6\n    \n    # Get image\n    test_image = testset.data[test_idx].numpy()\n    \n    # Create small subplot\n    left = 0.52 + col * 0.08\n    ax_img = fig.add_axes([left, 0.05, 0.06, 0.06])\n    ax_img.imshow(test_image, cmap='gray')\n    ax_img.axis('off')\n    \n    true_label = all_labels[test_idx]\n    pred_label = all_predictions[test_idx]\n    confidence = max_probs[test_idx]\n    \n    title = f'Pred: {class_names[pred_label][:8]}\\nTrue: {class_names[true_label][:8]}'\n    ax_img.set_title(title, fontsize=7, color='red', fontweight='bold')\n\nax6.text(0.5, 0.95, 'Incorrect Predictions (Model Mistakes)', ha='center', \n        fontsize=12, fontweight='bold', transform=ax6.transAxes)\n\nplt.suptitle(f'Classification Dashboard - Overall Accuracy: {correct / total * 100:.2f}%', \n            fontsize=16, fontweight='bold', y=0.98)\nplt.show()\n\nprint(\"\\nKey Insights:\")\nprint(f\"â€¢ Overall Accuracy: {correct / total * 100:.2f}%\")\nprint(f\"â€¢ Best class: {class_names[class_accuracy.argmax()]} ({class_accuracy.max():.1f}%)\")\nprint(f\"â€¢ Worst class: {class_names[class_accuracy.argmin()]} ({class_accuracy.min():.1f}%)\")\nprint(f\"â€¢ Average confidence (correct): {max_probs[correct_mask].mean()*100:.1f}%\")\nprint(f\"â€¢ Average confidence (wrong): {max_probs[~correct_mask].mean()*100:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xzduqvgorl",
   "source": "### Visualization: Model Predictions Analysis\n\nLet's create a comprehensive dashboard to analyze our model's performance.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4ceabfd1",
   "metadata": {},
   "source": "## Summary of Part 1\n\n1. **Pixel** = Input number (0-1).\n2. **Flatten** = Turning a 2D grid into a 1D vector.\n3. **Logits** = Raw scores from the model.\n4. **CrossEntropyLoss** = The standard loss for classification.\n\n**But wait!** Flattening images throws away spatial structure. This is where **Convolutional Neural Networks (CNNs)** come in."
  },
  {
   "cell_type": "markdown",
   "id": "0sex47gmqupd",
   "source": "---\n\n# PART 2: MODERN COMPUTER VISION (FAANG-Level)\n\nNow let's learn the architectures that power Google Photos, Tesla Autopilot, and Meta's image recognition.\n\n## Part 7: Convolutional Neural Networks (CNNs)\n\n### The Problem with Flattening\nWhen we flatten a 28x28 image into 784 numbers, we lose **spatial structure**. A pixel at position (5,5) is neighbors with (5,6) and (6,5), but after flattening, this relationship is destroyed.\n\n### The Solution: Convolution\nA **Convolutional Layer** slides a small filter (e.g., 3x3) across the image, preserving spatial relationships.\n\nThink of it like a \"pattern detector\":\n- One filter might detect **horizontal edges**.\n- Another might detect **circles**.\n- Another might detect **textures**.\n\n### The Math (Simplified)\nFor each position in the image, multiply the 3x3 filter with the 3x3 patch of pixels, sum them up, and put the result in the output.\n\n$$ Output[i,j] = \\sum_{m,n} Input[i+m, j+n] \\times Filter[m, n] $$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "no3lqpnoaq",
   "source": "# Simple CNN for FashionMNIST\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Convolutional Layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)  # Reduces size by half\n        \n        # Fully Connected Layers\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # After 2 poolings: 28->14->7\n        self.fc2 = nn.Linear(128, 10)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # Conv Block 1\n        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -> 14x14\n        # Conv Block 2\n        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -> 7x7\n        # Flatten\n        x = x.view(-1, 64 * 7 * 7)\n        # FC Layers\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\ncnn_model = SimpleCNN()\nprint(cnn_model)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in cnn_model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yf48hueyi1g",
   "source": "# Train the CNN (faster and more accurate than the flattened model)\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\ncnn_model = cnn_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n\nepochs = 5\nfor epoch in range(epochs):\n    cnn_model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for i, (images, labels) in enumerate(trainloader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = cnn_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}: Loss {running_loss/len(trainloader):.4f}, Train Acc: {train_acc:.2f}%\")\n\n# Test the CNN\ncnn_model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = cnn_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"\\nCNN Test Accuracy: {100 * correct / total:.2f}%\")\nprint(\"Compare this to the flattened model - CNNs are much better!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s1pxspfxg49",
   "source": "---\n\n## Part 8: ResNet - Residual Networks (FAANG Interview Favorite)\n\n### The Problem: Vanishing Gradients\nWhen you stack many layers (50, 100, 200), gradients become tiny during backprop. Deep networks fail to train.\n\n### The Breakthrough: Skip Connections (2015 - Microsoft Research)\nInstead of learning $H(x)$, learn the **residual** $F(x) = H(x) - x$.\n\n$$ y = F(x) + x $$\n\nThe \"+x\" is a **skip connection** (also called shortcut or residual connection).\n\n**Why it works:**\n- If the layer should do nothing (identity mapping), it just learns F(x) = 0.\n- Gradients can flow directly through the skip connection.\n- Won ImageNet 2015 with 152 layers (previous winners had ~20 layers).\n\n### FAANG Interview Question\n**\"Implement a ResNet block from scratch.\"** â† This is asked at Google, Meta, Nvidia!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "frj1umv0bcp",
   "source": "class ResidualBlock(nn.Module):\n    \"\"\"\n    The basic building block of ResNet.\n    \n    This is the MOST IMPORTANT interview question for Computer Vision roles.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        \n        # Main path (the F(x) part)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Shortcut path (the +x part)\n        # If dimensions change, we need a projection\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n                         stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        \n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        # Main path\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        # Add skip connection (THE KEY INNOVATION!)\n        out += self.shortcut(x)  # This is y = F(x) + x\n        out = self.relu(out)\n        \n        return out\n\n# Test the block\nblock = ResidualBlock(1, 32)\ntest_input = torch.randn(1, 1, 28, 28)\ntest_output = block(test_input)\nprint(f\"Input shape: {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")\nprint(\"âœ“ Residual block implemented correctly!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3ekky7p2sv2",
   "source": "class ResNet(nn.Module):\n    \"\"\"\n    A simplified ResNet for FashionMNIST (28x28 images).\n    In production (ImageNet), you'd use ResNet18, ResNet50, ResNet101.\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # Initial convolution\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Residual blocks\n        self.layer1 = self._make_layer(32, 32, num_blocks=2, stride=1)\n        self.layer2 = self._make_layer(32, 64, num_blocks=2, stride=2)\n        self.layer3 = self._make_layer(64, 128, num_blocks=2, stride=2)\n        \n        # Classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(128, num_classes)\n    \n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        # First block might downsample\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        # Remaining blocks\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        # Stem\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        # Residual blocks\n        x = self.layer1(x)  # 28x28\n        x = self.layer2(x)  # 14x14\n        x = self.layer3(x)  # 7x7\n        \n        # Classifier\n        x = self.avgpool(x)  # 1x1\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Create ResNet\nresnet = ResNet(num_classes=10)\nprint(resnet)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in resnet.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Test forward pass\ntest_input = torch.randn(4, 1, 28, 28)\ntest_output = resnet(test_input)\nprint(f\"\\nInput shape: {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")\nprint(\"âœ“ ResNet works!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bqxl3701qf9",
   "source": "---\n\n## Part 9: Vision Transformers (ViT) - The Future (2021-2025)\n\n### The Paradigm Shift\nIn 2020, Google Research asked: **\"What if we applied Transformers to images?\"**\n\nResult: **Vision Transformer (ViT)** beats CNNs on ImageNet (2021).\n- Used in DALL-E, Stable Diffusion, Segment Anything (SAM).\n- **Standard at Google, Meta, OpenAI in 2025.**\n\n### The Core Idea: Patches\nInstead of convolutions, we:\n1. Split the image into fixed-size **patches** (e.g., 16x16).\n2. Flatten each patch into a vector (like a \"word\" in NLP).\n3. Feed these patches to a Transformer.\n\nFor a 224x224 image with 16x16 patches:\n- Number of patches = (224/16) Ã— (224/16) = 14 Ã— 14 = 196 patches\n- Each patch is a \"token\" (like a word in a sentence)\n\n### The Architecture\n```\nImage (224x224x3)\n  â†’ Split into patches (196 patches of 16x16x3)\n  â†’ Linear projection (embed each patch)\n  â†’ Add positional embeddings\n  â†’ Transformer Encoder\n  â†’ MLP Head (classification)\n```\n\n### FAANG Interview Alert\n**\"Explain how Vision Transformers differ from CNNs\"** â† Asked at Google, Meta",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "239a57o19yr",
   "source": "class PatchEmbedding(nn.Module):\n    \"\"\"\n    Convert image into patches and embed them.\n    \"\"\"\n    def __init__(self, img_size=28, patch_size=7, in_channels=1, embed_dim=64):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Use a Conv2d to extract patches (clever trick!)\n        self.proj = nn.Conv2d(in_channels, embed_dim, \n                             kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        # x: (B, C, H, W)\n        x = self.proj(x)  # (B, embed_dim, num_patches_h, num_patches_w)\n        x = x.flatten(2)  # (B, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    Simplified Vision Transformer for FashionMNIST.\n    \"\"\"\n    def __init__(self, img_size=28, patch_size=7, in_channels=1, num_classes=10,\n                 embed_dim=64, depth=4, num_heads=4, mlp_ratio=2.0):\n        super().__init__()\n        \n        # 1. Patch Embedding\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        # 2. Class token (like [CLS] in BERT)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # 3. Positional Embedding\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        \n        # 4. Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=int(embed_dim * mlp_ratio),\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        \n        # 5. Classification Head\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        \n        # 1. Patch embedding\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n        \n        # 2. Add class token\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)\n        \n        # 3. Add positional embedding\n        x = x + self.pos_embed\n        \n        # 4. Transformer\n        x = self.transformer(x)\n        \n        # 5. Classification from [CLS] token\n        x = self.norm(x[:, 0])  # Take the CLS token\n        x = self.head(x)\n        \n        return x\n\n# Create ViT\nvit = VisionTransformer(img_size=28, patch_size=7, num_classes=10)\nprint(vit)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in vit.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\n# Test\ntest_input = torch.randn(4, 1, 28, 28)\ntest_output = vit(test_input)\nprint(f\"\\nInput shape: {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")\nprint(\"âœ“ Vision Transformer works!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p2alhmnla8",
   "source": "---\n\n## Part 10: Transfer Learning (How FAANG Actually Uses Models)\n\n### The Reality Check\nAt Google/Meta, you **never** train ImageNet from scratch. You:\n1. Download a pre-trained model (ResNet50, EfficientNet, ViT)\n2. Fine-tune it on your data\n\nThis is 100x faster and uses 100x less data.\n\n### The Strategy\n**Fine-tuning options:**\n1. **Feature Extraction**: Freeze all layers except the last one\n2. **Fine-tuning**: Unfreeze some/all layers and train with small LR\n3. **Progressive unfreezing**: Unfreeze layers gradually\n\n### FAANG Interview Question\n**\"When would you freeze layers vs fine-tune them?\"**\n\n**Answer:**\n- **Freeze** when: Small dataset, similar to ImageNet\n- **Fine-tune** when: Large dataset, different from ImageNet",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tgoyj6389np",
   "source": "# Transfer Learning Example using torchvision pretrained models\n# Note: For FashionMNIST we'll demonstrate the concept, \n# but in practice you'd use this for RGB images\n\nfrom torchvision import models\n\n# Option 1: Load pre-trained ResNet18 (trained on ImageNet)\n# pretrained_resnet = models.resnet18(pretrained=True)\n\n# For demonstration, let's show how to modify it for grayscale + 10 classes\nclass TransferResNet(nn.Module):\n    def __init__(self, num_classes=10, freeze_layers=True):\n        super().__init__()\n        # Load pretrained ResNet18\n        self.backbone = models.resnet18(weights=None)  # Would use weights=ResNet18_Weights.DEFAULT for real transfer\n        \n        # Modify first conv for grayscale (1 channel instead of 3)\n        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Freeze backbone if feature extraction mode\n        if freeze_layers:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n        \n        # Replace final FC layer for our 10 classes\n        num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Linear(num_features, num_classes)\n        \n        # Always train the new FC layer\n        for param in self.backbone.fc.parameters():\n            param.requires_grad = True\n    \n    def forward(self, x):\n        return self.backbone(x)\n\n# Create transfer learning model\ntransfer_model = TransferResNet(num_classes=10, freeze_layers=True)\n\n# Count trainable vs frozen parameters\ntrainable = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in transfer_model.parameters())\nprint(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\nprint(\"\\\\nIn feature extraction mode, we only train the last layer!\")\nprint(\"This trains 100x faster than training from scratch.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "udddptspf3r",
   "source": "---\n\n## Part 11: Data Augmentation (Making Models Robust)\n\n### The Problem\nModels overfit on small datasets. A dog facing left looks different from a dog facing right (to the model).\n\n### The Solution: Augmentation\nArtificially create variations of your data:\n- **Geometric**: Rotation, flipping, cropping, scaling\n- **Color**: Brightness, contrast, saturation\n- **Advanced**: CutOut, MixUp, CutMix, RandAugment\n\n### The Impact\n- Can improve accuracy by 5-15%\n- Standard practice at all FAANG companies\n- Required for winning Kaggle competitions\n\n### FAANG Interview Question\n**\"How do you prevent overfitting in computer vision?\"**\n\n**Answer:** \n1. Data augmentation\n2. Dropout / regularization  \n3. More data\n4. Early stopping\n5. Transfer learning",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "48if802x81w",
   "source": "# Data Augmentation Pipeline (Production-Ready)\n\n# Basic augmentation for training\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),  # Flip 50% of images\n    transforms.RandomRotation(degrees=15),    # Rotate up to 15 degrees\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Shift image\n    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),  # Random zoom\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n])\n\n# No augmentation for test (important!)\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Create augmented datasets\ntrain_augmented = torchvision.datasets.FashionMNIST(\n    root='./data', train=True, download=True, transform=train_transform\n)\ntest_normal = torchvision.datasets.FashionMNIST(\n    root='./data', train=False, download=True, transform=test_transform\n)\n\n# Visualize augmented images\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfig.suptitle(\"Data Augmentation Examples (Same Image, Different Augmentations)\")\n\n# Get one image and apply augmentation 10 times\noriginal_image, label = train_augmented.dataset.data[0], train_augmented.dataset.targets[0]\n\nfor i in range(10):\n    augmented = train_transform(transforms.ToPILImage()(original_image))\n    row = i // 5\n    col = i % 5\n    axes[row, col].imshow(augmented.squeeze(), cmap='gray')\n    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ“ Each training image is randomly transformed during training!\")\nprint(\"This creates infinite variations and prevents overfitting.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s3jtc3pqk2",
   "source": "---\n\n## Final Summary: Computer Vision for FAANG Interviews\n\n### Architecture Evolution (What You MUST Know)\n\n| Year | Architecture | Key Innovation | When Used |\n|------|--------------|----------------|-----------|\n| 2012 | **AlexNet** | First deep CNN to win ImageNet | Legacy systems |\n| 2014 | **VGG** | Deeper networks (16-19 layers) | Feature extraction |\n| 2015 | **ResNet** â­ | Skip connections | **Most interviews** |\n| 2016 | **Inception** | Multi-scale convolutions | Google products |\n| 2017 | **MobileNet** | Depthwise separable convs | Mobile/edge devices |\n| 2019 | **EfficientNet** | Compound scaling | SOTA 2019-2021 |\n| 2021 | **ViT** â­ | Transformers for images | **Future standard** |\n| 2022 | **ConvNeXt** | Modernized CNNs | Research |\n\nâ­ = Must implement from scratch for interviews\n\n### FAANG Interview Preparation Checklist\n\n#### **Must Know (100% Chance of Being Asked)**\n- âœ… Explain convolution vs fully connected\n- âœ… Implement ResNet block from scratch\n- âœ… Explain skip connections and why they work\n- âœ… Batch Normalization: what, why, when\n- âœ… Difference between CNN and ViT\n- âœ… Transfer learning strategy\n- âœ… Data augmentation techniques\n\n#### **Should Know (70% Chance)**\n- âœ… Depthwise separable convolutions (MobileNet)\n- âœ… Global Average Pooling vs Fully Connected\n- âœ… Receptive field calculation\n- âœ… Spatial Pyramid Pooling\n- âœ… Gradient flow in very deep networks\n\n#### **Nice to Know (30% Chance)**\n- âœ… Neural Architecture Search (NAS)\n- âœ… Knowledge distillation\n- âœ… Pruning and quantization\n- âœ… Attention mechanisms in CNNs (CBAM, SENet)\n\n### Common Interview Questions (Copy These!)\n\n**Q1: \"Why do ResNets work better than plain deep networks?\"**\n```\nAnswer:\n1. Skip connections allow gradients to flow directly through the network\n2. Enables learning identity mappings (if layer should do nothing, learn F(x)=0)\n3. Prevents vanishing gradients in very deep networks (50-200 layers)\n4. Allows training much deeper networks without degradation\n```\n\n**Q2: \"When would you use a CNN vs a Vision Transformer?\"**\n```\nAnswer:\nCNN:\n- Small datasets (CNNs have inductive bias)\n- Need translation equivariance\n- Limited compute\n- Edge deployment (faster inference)\n\nViT:\n- Large datasets (100M+ images)\n- Pretraining foundation models\n- Don't need strict spatial invariance\n- Have massive compute for training\n```\n\n**Q3: \"Implement a 3x3 convolution in Python/NumPy\"**\n```python\n# This is asked at Google/Meta!\ndef conv2d(input, kernel):\n    h, w = input.shape\n    kh, kw = kernel.shape\n    output = np.zeros((h - kh + 1, w - kw + 1))\n    \n    for i in range(output.shape[0]):\n        for j in range(output.shape[1]):\n            output[i, j] = np.sum(input[i:i+kh, j:j+kw] * kernel)\n    \n    return output\n```\n\n### What We Covered in This Enhanced Notebook\n\n1. âœ… **Basic Classification** (MLP on flattened images)\n2. âœ… **CNNs** (Spatial structure preservation)\n3. âœ… **ResNet** (Skip connections, deep networks)\n4. âœ… **Vision Transformers** (Patch-based, attention)\n5. âœ… **Transfer Learning** (Fine-tuning pretrained models)\n6. âœ… **Data Augmentation** (Preventing overfitting)\n\n### Next Steps for FAANG Prep\n\n1. **Implement from scratch:** ResNet-18, ViT-Tiny\n2. **Read papers:** ResNet, ViT, EfficientNet\n3. **Practice:** LeetCode-style CV problems\n4. **Build project:** Image classifier end-to-end (data â†’ training â†’ deployment)\n5. **Study:** Object detection (YOLO, Faster R-CNN)\n\n### Resources\n- Papers: [ResNet](https://arxiv.org/abs/1512.03385), [ViT](https://arxiv.org/abs/2010.11929)\n- Code: [torchvision.models](https://pytorch.org/vision/stable/models.html)\n- Practice: [Deep Learning Interviews Book](https://arxiv.org/abs/2201.00650)\n\n---\n\n**You are now ready for Computer Vision interviews at FAANG/Nvidia! ðŸš€**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2rmqdws31xm",
   "source": "---\n\n# PART 3: BEYOND CLASSIFICATION - OBJECT DETECTION & SEGMENTATION\n\nClassification answers \"What?\". Detection answers \"What and Where?\". Segmentation answers \"Where exactly, pixel-by-pixel?\".\n\n## Part 12: Object Detection (YOLO Conceptual Understanding)\n\n### The Problem\nClassification: \"This image contains a dog\" âœ“  \nDetection: \"This image contains a dog at (x1,y1,x2,y2)\" âœ“âœ“\n\n### The Paradigms\n\n**Two-Stage Detectors (R-CNN Family):**\n1. Propose regions (where objects might be)\n2. Classify each region\n\n**One-Stage Detectors (YOLO, SSD):**\n- Directly predict bounding boxes + classes in one pass\n- Much faster (real-time!)\n\n### YOLO: You Only Look Once\n\n**Core Idea:** Divide image into grid, each cell predicts bounding boxes.\n\n```\nImage â†’ Grid (7x7) â†’ Each cell predicts:\n  - Bounding boxes (x, y, w, h)\n  - Confidence scores\n  - Class probabilities\n```\n\n### FAANG Interview Question\n**\"Explain the difference between one-stage and two-stage object detectors\"**\n\n**Answer:**\n- **Two-stage** (Faster R-CNN): Propose regions â†’ Classify. Accurate but slow.\n- **One-stage** (YOLO): Direct prediction. Fast (real-time) but less accurate on small objects.\n- **Modern trend**: One-stage (YOLO-v8, DETR) catching up in accuracy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3dn09erpkh6",
   "source": "# Simplified Object Detection Head (Conceptual)\n# In practice, you'd use torchvision.models.detection\n\nclass SimpleDetectionHead(nn.Module):\n    \"\"\"\n    Simplified detection head to understand the concept.\n    Real YOLO is more complex but follows this pattern.\n    \"\"\"\n    def __init__(self, backbone_out_channels=128, num_classes=10, num_anchors=3):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        \n        # For each anchor box, predict:\n        # - 4 values for bounding box (x, y, w, h)\n        # - 1 value for objectness score\n        # - num_classes values for class probabilities\n        outputs_per_anchor = 4 + 1 + num_classes\n        \n        self.detection_head = nn.Conv2d(\n            backbone_out_channels,\n            num_anchors * outputs_per_anchor,\n            kernel_size=1\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        x: Feature map from backbone (B, C, H, W)\n        Returns: Predictions (B, H, W, num_anchors, 4+1+num_classes)\n        \"\"\"\n        batch_size = x.size(0)\n        predictions = self.detection_head(x)\n        \n        # Reshape to (B, num_anchors, 4+1+num_classes, H, W)\n        predictions = predictions.view(\n            batch_size,\n            self.num_anchors,\n            -1,\n            predictions.size(2),\n            predictions.size(3)\n        )\n        \n        # Permute to (B, H, W, num_anchors, 4+1+num_classes)\n        predictions = predictions.permute(0, 3, 4, 1, 2)\n        \n        return predictions\n\n# Example usage\nbackbone_features = torch.randn(2, 128, 7, 7)  # Features from ResNet\ndetector = SimpleDetectionHead(backbone_out_channels=128, num_classes=10)\ndetections = detector(backbone_features)\n\nprint(f\"Input features: {backbone_features.shape}\")\nprint(f\"Detection output: {detections.shape}\")\nprint(f\"Interpretation: (Batch, Grid_H, Grid_W, Anchors, [x,y,w,h,conf,class_probs])\")\nprint(f\"Each grid cell predicts {detections.size(3)} bounding boxes\")\nprint(\"\\\\nâœ“ This is the core idea behind YOLO!\")\nprint(\"\\\\nReal implementation: Use torchvision.models.detection.fasterrcnn_resnet50_fpn()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4z31faik3",
   "source": "---\n\n## Part 13: Semantic Segmentation (Pixel-Perfect Predictions)\n\n### The Problem\n- Classification: \"Dog\" âœ“\n- Detection: \"Dog at (100, 100, 300, 300)\" âœ“âœ“\n- Segmentation: \"These exact pixels are dog\" âœ“âœ“âœ“\n\n### Use Cases\n- Medical imaging (tumor segmentation)\n- Autonomous driving (road, pedestrian, car segmentation)\n- Background removal (Zoom virtual backgrounds)\n\n### U-Net Architecture (The Standard)\n\n```\nInput Image\n    â†“\nEncoder (Downsampling)\n    â†“\nBottleneck (Lowest resolution, highest channels)\n    â†“\nDecoder (Upsampling) + Skip Connections\n    â†“\nOutput Mask (same size as input)\n```\n\n**Key Innovation:** Skip connections from encoder to decoder\n- Encoder captures \"what\" (semantic info)\n- Decoder captures \"where\" (spatial info)\n- Skip connections combine both!\n\n### FAANG Interview Question\n**\"Explain the U-Net architecture and why it works for segmentation\"**\n\n**Answer:**\n1. **Symmetric encoder-decoder** structure\n2. **Skip connections** preserve spatial information lost during downsampling  \n3. **Pixel-wise prediction** - each pixel gets a class label\n4. **Works with small datasets** (important for medical imaging)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rsvrges869r",
   "source": "# U-Net Implementation (Simplified for Understanding)\n\nclass DoubleConv(nn.Module):\n    \"\"\"Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet(nn.Module):\n    \"\"\"\n    Simplified U-Net for semantic segmentation.\n    This is THE standard architecture for medical imaging and segmentation tasks.\n    \"\"\"\n    def __init__(self, in_channels=1, num_classes=2):\n        super().__init__()\n        \n        # Encoder (Downsampling path)\n        self.enc1 = DoubleConv(in_channels, 64)\n        self.enc2 = DoubleConv(64, 128)\n        self.enc3 = DoubleConv(128, 256)\n        \n        # Bottleneck\n        self.bottleneck = DoubleConv(256, 512)\n        \n        # Decoder (Upsampling path)\n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(512, 256)  # 512 because of skip connection\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(128, 64)\n        \n        # Final output layer\n        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n        \n        self.pool = nn.MaxPool2d(2, 2)\n    \n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)          # 64 channels\n        enc2 = self.enc2(self.pool(enc1))  # 128 channels\n        enc3 = self.enc3(self.pool(enc2))  # 256 channels\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(self.pool(enc3))  # 512 channels\n        \n        # Decoder with skip connections\n        dec3 = self.upconv3(bottleneck)\n        dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection!\n        dec3 = self.dec3(dec3)\n        \n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat([dec2, enc2], dim=1)  # Skip connection!\n        dec2 = self.dec2(dec2)\n        \n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat([dec1, enc1], dim=1)  # Skip connection!\n        dec1 = self.dec1(dec1)\n        \n        # Output\n        out = self.out(dec1)\n        return out\n\n# Test U-Net\nunet = UNet(in_channels=1, num_classes=2)  # Binary segmentation\ntest_input = torch.randn(1, 1, 64, 64)  # 64x64 image\ntest_output = unet(test_input)\n\nprint(f\"Input shape:  {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")\nprint(f\"\\\\nâœ“ Output has same spatial dimensions as input!\")\nprint(f\"âœ“ Each pixel gets a prediction for {test_output.size(1)} classes\")\nprint(f\"\\\\nSkip connections preserve spatial details lost during downsampling.\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in unet.parameters())\nprint(f\"\\\\nTotal parameters: {total_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}