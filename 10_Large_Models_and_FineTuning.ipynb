{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Large Models and Fine-Tuning\n",
    "\n",
    "In 2025, you rarely train models from scratch. Instead, you take a massive pre-trained model (like Llama, BERT, or ResNet) and **fine-tune** it on your data. This notebook introduces you to the world of Large Language Models (LLMs) and efficient fine-tuning.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load pre-trained models from Hugging Face\n",
    "- Understand Fine-Tuning vs Training from Scratch\n",
    "- Learn about Parameter-Efficient Fine-Tuning (PEFT/LoRA)\n",
    "- Understand Quantization (loading models in 4-bit/8-bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Note: In a real environment, you would install 'transformers' and 'peft'\n",
    "# !pip install transformers peft bitsandbytes\n",
    "print(\"Ready to explore LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Models\n",
    "\n",
    "We use the `transformers` library (by Hugging Face) to load state-of-the-art models.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a small LLM (e.g., TinyLlama)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Fine-Tuning?\n",
    "\n",
    "**Pre-training**: The model reads the entire internet to learn language (expensive, takes months).\n",
    "**Fine-tuning**: The model trains on *your* specific dataset to learn a task (cheap, takes hours).\n",
    "\n",
    "Example: Turning a generic model into a medical assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Fine-tuning a 7B parameter model requires massive GPU memory. **LoRA (Low-Rank Adaptation)** solves this.\n",
    "\n",
    "Instead of updating all weights ($W$), LoRA freezes the model and adds small trainable adapters ($A$ and $B$):\n",
    "\n",
    "$$ W_{new} = W_{frozen} + (A \\times B) $$\n",
    "\n",
    "This reduces trainable parameters by 99%!\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,             # Rank\n",
    "    lora_alpha=32,   # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Where to add adapters\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "# Output: \"trainable params: 4M || all params: 7B || trainable%: 0.06%\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization (4-bit / 8-bit)\n",
    "\n",
    "To fit large models on consumer GPUs, we reduce precision from Float32 (32 bits) to Int8 (8 bits) or even 4-bit.\n",
    "\n",
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Understanding LoRA Mathematics in Depth\n\nLoRA is one of the most important techniques for FAANG ML engineers to understand deeply. Let's dive into the mathematics."
  },
  {
   "cell_type": "markdown",
   "id": "vlsue49w0d",
   "source": "### The Core Insight: Low-Rank Decomposition\n\nFor a pre-trained weight matrix $W \\in \\mathbb{R}^{d \\times k}$, instead of updating all $d \\times k$ parameters, LoRA introduces two smaller matrices:\n\n$$W_{new} = W_{frozen} + \\Delta W = W_{frozen} + BA$$\n\nWhere:\n- $B \\in \\mathbb{R}^{d \\times r}$ (down-projection)\n- $A \\in \\mathbb{R}^{r \\times k}$ (up-projection)\n- $r$ is the **rank** (typically 4, 8, 16, or 32)\n\n**Parameter Savings**:\n- Original: $d \\times k$ parameters\n- LoRA: $(d \\times r) + (r \\times k) = r(d + k)$ parameters\n- For $d = k = 4096$ and $r = 8$: 16.7M → 65K (256x reduction!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yq2wcxrqvm",
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LoRALayer(nn.Module):\n    \"\"\"\n    Implementation of a LoRA adapter layer.\n    \n    Key insight: We decompose the weight update into two low-rank matrices.\n    This dramatically reduces trainable parameters while maintaining performance.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 8, alpha: float = 16):\n        super().__init__()\n        \n        self.original_layer = original_layer\n        self.rank = rank\n        self.alpha = alpha  # Scaling factor\n        \n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n        \n        # Freeze original weights\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n        \n        # LoRA matrices (trainable)\n        # A is initialized with Kaiming, B is initialized to zero\n        # This ensures ΔW = BA = 0 at initialization (no change initially)\n        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n        \n        # Initialize A with Kaiming\n        nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)\n        \n        # Scaling factor\n        self.scaling = self.alpha / self.rank\n    \n    def forward(self, x):\n        # Original forward pass\n        original_output = self.original_layer(x)\n        \n        # LoRA path: x @ A^T @ B^T * scaling\n        # (batch, in) @ (in, rank) @ (rank, out) = (batch, out)\n        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B)\n        \n        return original_output + lora_output * self.scaling\n\n# Demonstration\noriginal = nn.Linear(4096, 4096)\nlora_layer = LoRALayer(original, rank=8, alpha=16)\n\n# Count parameters\noriginal_params = sum(p.numel() for p in original.parameters())\ntrainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n\nprint(f\"Original parameters: {original_params:,}\")\nprint(f\"LoRA trainable parameters: {trainable_params:,}\")\nprint(f\"Reduction: {original_params / trainable_params:.1f}x\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xwryrt6483",
   "source": "### Why Zero Initialization for B?\n\nA critical design choice in LoRA is initializing $B = 0$. This ensures:\n\n1. **At initialization**: $\\Delta W = BA = 0$, so the model behaves exactly like the pre-trained model\n2. **During training**: Updates emerge gradually as $B$ learns\n3. **Stability**: Prevents catastrophic forgetting at the start of training\n\nThis is different from random initialization, which would immediately corrupt the pre-trained weights!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "0yfsw824uij",
   "source": "## 6. QLoRA: Quantized LoRA\n\nQLoRA combines quantization with LoRA for even more memory savings. Key innovations:\n\n### 4-bit NormalFloat (NF4)\nInstead of standard 4-bit integers, QLoRA uses a special data type optimized for normally distributed weights:\n- Theoretically optimal for Gaussian distributions\n- Better precision than INT4 for neural network weights\n\n### Double Quantization\nQuantizes the quantization constants themselves:\n- First quantization: weights to 4-bit\n- Second quantization: scaling factors to 8-bit\n- Saves ~0.5GB for a 7B model!\n\n### Paged Optimizers\nUses CPU RAM as overflow for GPU memory during gradient computation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1tklml93psw",
   "source": "# QLoRA configuration example\nqlora_config_example = \"\"\"\nfrom transformers import BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",              # NormalFloat4 (optimal for weights)\n    bnb_4bit_compute_dtype=torch.bfloat16,   # Compute in bfloat16\n    bnb_4bit_use_double_quant=True,          # Double quantization\n)\n\n# Load model in 4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\"\"\"\n\nprint(\"QLoRA Configuration Example:\")\nprint(qlora_config_example)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i85g9atsdz",
   "source": "## 7. Memory Calculations for Fine-Tuning\n\nUnderstanding memory requirements is CRITICAL for production ML. Let's calculate exactly what you need.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h1bsnnnq01k",
   "source": "def calculate_memory_requirements(\n    model_params_billions: float,\n    precision: str = \"fp32\",  # fp32, fp16, bf16, int8, int4\n    training: bool = True,\n    lora_rank: int = 0,  # 0 = full fine-tuning\n    batch_size: int = 1,\n    sequence_length: int = 2048,\n    hidden_size: int = 4096,\n):\n    \"\"\"\n    Calculate GPU memory requirements for model training/inference.\n    \n    Key insight: Memory is dominated by:\n    1. Model weights\n    2. Optimizer states (2x for Adam)\n    3. Gradients\n    4. Activations (for backprop)\n    \"\"\"\n    \n    bytes_per_param = {\n        \"fp32\": 4, \"fp16\": 2, \"bf16\": 2, \"int8\": 1, \"int4\": 0.5\n    }\n    \n    param_bytes = bytes_per_param[precision]\n    params = model_params_billions * 1e9\n    \n    # Model weights\n    model_memory_gb = (params * param_bytes) / 1e9\n    \n    if not training:\n        # Inference only needs model weights + KV cache\n        kv_cache_gb = (batch_size * sequence_length * hidden_size * 2 * 2 * param_bytes) / 1e9\n        total = model_memory_gb + kv_cache_gb\n        return {\n            \"model_weights_gb\": model_memory_gb,\n            \"kv_cache_gb\": kv_cache_gb,\n            \"total_gb\": total,\n        }\n    \n    # Training memory\n    if lora_rank > 0:\n        # LoRA: Only train ~0.1% of parameters\n        trainable_fraction = 0.001\n        trainable_params = params * trainable_fraction\n        \n        optimizer_memory_gb = (trainable_params * 8) / 1e9  # Adam needs 8 bytes/param\n        gradient_memory_gb = (trainable_params * param_bytes) / 1e9\n    else:\n        # Full fine-tuning\n        optimizer_memory_gb = (params * 8) / 1e9  # Adam: momentum + variance\n        gradient_memory_gb = (params * param_bytes) / 1e9\n    \n    # Activations (rough estimate)\n    activation_memory_gb = (batch_size * sequence_length * hidden_size * 4 * 2) / 1e9\n    \n    total = model_memory_gb + optimizer_memory_gb + gradient_memory_gb + activation_memory_gb\n    \n    return {\n        \"model_weights_gb\": round(model_memory_gb, 2),\n        \"optimizer_states_gb\": round(optimizer_memory_gb, 2),\n        \"gradients_gb\": round(gradient_memory_gb, 2),\n        \"activations_gb\": round(activation_memory_gb, 2),\n        \"total_gb\": round(total, 2),\n    }\n\n# Compare different configurations\nprint(\"=\" * 60)\nprint(\"Memory Requirements for Llama-2-7B\")\nprint(\"=\" * 60)\n\nconfigs = [\n    (\"Full FT (FP32)\", {\"precision\": \"fp32\", \"lora_rank\": 0}),\n    (\"Full FT (FP16)\", {\"precision\": \"fp16\", \"lora_rank\": 0}),\n    (\"LoRA (FP16)\", {\"precision\": \"fp16\", \"lora_rank\": 16}),\n    (\"QLoRA (INT4)\", {\"precision\": \"int4\", \"lora_rank\": 16}),\n]\n\nfor name, config in configs:\n    result = calculate_memory_requirements(7, **config)\n    print(f\"\\n{name}:\")\n    for k, v in result.items():\n        print(f\"  {k}: {v} GB\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"GPU Recommendations:\")\nprint(\"=\" * 60)\nprint(\"Full Fine-Tuning (7B): 4x A100-80GB or 8x A100-40GB\")\nprint(\"LoRA Fine-Tuning (7B): 1x A100-40GB or 2x RTX 4090\")\nprint(\"QLoRA Fine-Tuning (7B): 1x RTX 4090 (24GB) or 1x RTX 3090\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r9ge9m9kig",
   "source": "## 8. Other PEFT Techniques\n\nLoRA isn't the only option. Here's a comparison of PEFT methods:\n\n### Comparison of PEFT Methods\n\n| Method | Parameters | Memory | Quality | Use Case |\n|--------|-----------|--------|---------|----------|\n| **Full Fine-Tuning** | 100% | Highest | Best | Large compute budget |\n| **LoRA** | ~0.1% | Low | Near-SOTA | General purpose |\n| **QLoRA** | ~0.1% | Lowest | Good | Consumer GPUs |\n| **Prefix Tuning** | <0.1% | Very Low | Moderate | NLU tasks |\n| **Prompt Tuning** | <0.01% | Minimal | Lower | Simple classification |\n| **Adapter Layers** | ~2% | Medium | Good | Multi-task learning |\n\n### When to Use What\n\n1. **LoRA**: Default choice for most fine-tuning tasks\n2. **QLoRA**: When GPU memory is limited (consumer hardware)\n3. **Prefix Tuning**: When you need multiple tasks from one model\n4. **Full Fine-Tuning**: When you have the compute and need best quality\n5. **Adapter Layers**: When adding new capabilities incrementally",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ktczvbpo5f",
   "source": "## 9. Production Fine-Tuning Script Template",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kijrd5pnnsq",
   "source": "fine_tuning_script = \"\"\"\n# Production Fine-Tuning Script with LoRA\n# Tested on: RTX 4090 (24GB), Llama-2-7B\n\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\n\n# 1. Load Model with Quantization\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nfrom transformers import BitsAndBytesConfig\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# 2. Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 3. Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch = 16\n    learning_rate=2e-4,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=100,\n    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n    gradient_checkpointing=True,  # Trade compute for memory\n)\n\n# 4. Save LoRA Weights (small file!)\n# model.save_pretrained(\"./lora_adapter\")\n\"\"\"\n\nprint(\"Production Fine-Tuning Script:\")\nprint(fine_tuning_script)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gkjkilozy7l",
   "source": "## 10. FAANG Interview Questions\n\n### Q1: Explain how LoRA reduces memory requirements while maintaining model quality.\n\n**Answer**: LoRA freezes the pre-trained weights $W$ and introduces a low-rank decomposition for the weight update: $\\Delta W = BA$ where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$.\n\nMemory savings come from:\n1. **Fewer trainable parameters**: Instead of $d \\times k$ parameters, we only train $r(d + k)$ where $r \\ll \\min(d, k)$\n2. **No gradient storage for frozen weights**: Only LoRA parameters need gradients\n3. **Reduced optimizer states**: Adam stores momentum/variance only for trainable params\n\nQuality is maintained because:\n1. Pre-trained weights contain most of the knowledge\n2. Task-specific adaptation is low-rank in nature\n3. The scaling factor $\\alpha/r$ controls adaptation strength\n\n---\n\n### Q2: What are the key hyperparameters in LoRA and how do you tune them?\n\n**Answer**:\n1. **Rank (r)**: Higher = more capacity, more memory. Start with 8-16, increase if underfitting.\n2. **Alpha (α)**: Scaling factor. Common: $\\alpha = 2r$. Higher = stronger adaptation.\n3. **Target modules**: Which layers to adapt. Attention layers (q, k, v, o) are most impactful.\n4. **Dropout**: 0.05-0.1 for regularization.\n\nTuning strategy:\n- Start with r=8, α=16, all attention layers\n- If quality is low: increase r, add more target modules\n- If overfitting: increase dropout, decrease r\n\n---\n\n### Q3: How does quantization affect model quality? What's the trade-off between INT4 and INT8?\n\n**Answer**:\n\n| Precision | Memory | Quality Loss | Speed |\n|-----------|--------|--------------|-------|\n| FP16 | 2x smaller | Minimal | Same |\n| INT8 | 4x smaller | ~1% perplexity | Faster on CPU |\n| INT4 | 8x smaller | ~3-5% perplexity | Depends on hardware |\n\nKey considerations:\n- **INT8**: Negligible quality loss, 4x memory savings. Safe default.\n- **INT4**: More quality degradation, but NF4 (NormalFloat4) reduces this significantly.\n- **Calibration**: Post-training quantization needs representative data for accurate scaling.\n\n---\n\n### Q4: What's the difference between full fine-tuning, LoRA, and prompt tuning?\n\n**Answer**:\n\n| Method | Updates | Use Case |\n|--------|---------|----------|\n| **Full Fine-Tuning** | All weights | Max quality, large compute budget |\n| **LoRA** | Low-rank adapters (~0.1%) | Standard fine-tuning with limited compute |\n| **Prompt Tuning** | Prepended embeddings only | Simple classification, multiple tasks |\n\nDecision framework:\n1. **Production with quality focus**: Full fine-tuning if budget allows, LoRA otherwise\n2. **Consumer GPU**: QLoRA (4-bit + LoRA)\n3. **Many tasks, one model**: Prefix tuning or prompt tuning\n\n---\n\n### Q5: How do you handle catastrophic forgetting during fine-tuning?\n\n**Answer**: Catastrophic forgetting occurs when the model \"forgets\" pre-trained knowledge while learning new tasks.\n\nMitigation strategies:\n1. **Lower learning rate**: 1e-5 to 2e-4 (10-100x lower than pre-training)\n2. **LoRA/PEFT**: Freeze most weights, limiting what can be \"forgotten\"\n3. **Regularization**: L2 penalty toward original weights\n4. **Rehearsal**: Mix in pre-training data during fine-tuning\n5. **Learning rate warmup**: Gradual increase prevents early weight destruction\n\nLoRA naturally prevents forgetting because the original weights are frozen!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2zhezw1wttl",
   "source": "## Key Takeaways\n\n1. **Don't train from scratch**: Use pre-trained models.\n2. **Fine-tuning**: Adapts a general model to your specific data.\n3. **LoRA**: Allows fine-tuning huge models on small GPUs by using low-rank decomposition.\n4. **QLoRA**: Combines 4-bit quantization with LoRA for even more memory savings.\n5. **Memory math**: Understanding GPU memory requirements is critical for production ML.\n6. **PEFT landscape**: Choose the right method based on compute budget and quality requirements.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}