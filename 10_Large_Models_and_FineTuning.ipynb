{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Large Models and Fine-Tuning\n",
    "\n",
    "In 2025, you rarely train models from scratch. Instead, you take a massive pre-trained model (like Llama, BERT, or ResNet) and **fine-tune** it on your data. This notebook introduces you to the world of Large Language Models (LLMs) and efficient fine-tuning.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load pre-trained models from Hugging Face\n",
    "- Understand Fine-Tuning vs Training from Scratch\n",
    "- Learn about Parameter-Efficient Fine-Tuning (PEFT/LoRA)\n",
    "- Understand Quantization (loading models in 4-bit/8-bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Note: In a real environment, you would install 'transformers' and 'peft'\n",
    "# !pip install transformers peft bitsandbytes\n",
    "print(\"Ready to explore LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Models\n",
    "\n",
    "We use the `transformers` library (by Hugging Face) to load state-of-the-art models.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a small LLM (e.g., TinyLlama)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Fine-Tuning?\n",
    "\n",
    "**Pre-training**: The model reads the entire internet to learn language (expensive, takes months).\n",
    "**Fine-tuning**: The model trains on *your* specific dataset to learn a task (cheap, takes hours).\n",
    "\n",
    "Example: Turning a generic model into a medical assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Fine-tuning a 7B parameter model requires massive GPU memory. **LoRA (Low-Rank Adaptation)** solves this.\n",
    "\n",
    "Instead of updating all weights ($W$), LoRA freezes the model and adds small trainable adapters ($A$ and $B$):\n",
    "\n",
    "$$ W_{new} = W_{frozen} + (A \\times B) $$\n",
    "\n",
    "This reduces trainable parameters by 99%!\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,             # Rank\n",
    "    lora_alpha=32,   # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Where to add adapters\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "# Output: \"trainable params: 4M || all params: 7B || trainable%: 0.06%\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization (4-bit / 8-bit)\n",
    "\n",
    "To fit large models on consumer GPUs, we reduce precision from Float32 (32 bits) to Int8 (8 bits) or even 4-bit.\n",
    "\n",
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Don't train from scratch**: Use pre-trained models.\n",
    "2. **Fine-tuning**: Adapts a general model to your specific data.\n",
    "3. **LoRA**: Allows fine-tuning huge models on small GPUs.\n",
    "4. **Quantization**: Reduces memory usage significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
