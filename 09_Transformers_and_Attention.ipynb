{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Tutorial: Sequence Models - RNNs to Transformers\n\nFrom simple RNNs to modern Transformers, this notebook covers the evolution of sequence modeling that powers ChatGPT, Google Translate, and Siri.\n\n## Learning Objectives\n- **Part 1**: Understand RNN, LSTM, GRU (The foundations - still asked in interviews!)\n- **Part 2**: Implement Seq2Seq models (Translation, summarization)\n- **Part 3**: Deep dive into Self-Attention and Transformers\n- **Part 4**: Positional Encodings and why they matter\n- **Part 5**: GPT vs BERT architectures"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ozsw1579dkg",
   "source": "---\n\n# PART 2: Transformers and Self-Attention\n\n## From RNN to Transformers: The Paradigm Shift (2017)\n\n**The Problem with RNNs/LSTMs:**\n- Sequential processing (can't parallelize)\n- Still struggle with very long sequences\n- Slow to train\n\n**The Solution: Self-Attention (2017 - \"Attention Is All You Need\")**\n- Process entire sequence in parallel\n- Every token attends to every other token directly\n- **10x faster to train than LSTMs**\n\n---\n\n## 1. Self-Attention: The Core Mechanism\n\nIn a sentence like \"The animal didn't cross the street because **it** was too tired\", what does \"it\" refer to? The street or the animal?\n\nSelf-attention allows the model to look at ALL other words simultaneously to figure this out.\n\n### The Formula\n$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n\nWhere:\n- **Q (Query)**: What I'm looking for (\"Who does 'it' refer to?\")\n- **K (Key)**: What I have to offer (\"I'm the word 'animal'\")\n- **V (Value)**: What I actually contain (embedding of 'animal')\n\n### The Intuition\n1. Each word asks a question (Q): \"Which words are relevant to me?\"\n2. Each word offers information (K): \"I'm a noun/verb/etc\"\n3. Calculate compatibility: Q ¬∑ K (dot product)\n4. Attend to relevant words: softmax(scores) √ó V\n\n### FAANG Interview Question\n**\"Implement scaled dot-product attention from scratch\"** ‚Üê Asked at all FAANG!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y5zper7fq3m",
   "source": "class SimpleRNNCell(nn.Module):\n    \"\"\"\n    A single RNN cell (one timestep).\n    This is the #1 interview question for sequence modeling!\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # Weights for input -> hidden\n        self.W_xh = nn.Linear(input_size, hidden_size)\n        # Weights for hidden -> hidden (the recurrent part!)\n        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n        \n        self.tanh = nn.Tanh()\n    \n    def forward(self, x, h_prev):\n        \"\"\"\n        x: input at current timestep (batch_size, input_size)\n        h_prev: hidden state from previous timestep (batch_size, hidden_size)\n        \"\"\"\n        h_t = self.tanh(self.W_xh(x) + self.W_hh(h_prev))\n        return h_t\n\n# Test the RNN cell\ninput_size = 10\nhidden_size = 20\nbatch_size = 4\n\nrnn_cell = SimpleRNNCell(input_size, hidden_size)\n\n# Initialize hidden state\nh = torch.zeros(batch_size, hidden_size)\n\n# Process a sequence of 5 timesteps\nsequence_length = 5\nfor t in range(sequence_length):\n    x_t = torch.randn(batch_size, input_size)\n    h = rnn_cell(x_t, h)\n    print(f\"Timestep {t}: hidden state shape = {h.shape}\")\n\nprint(\"\\\\n‚úì RNN processes sequence step-by-step, maintaining hidden state!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zwy2uw68c6",
   "source": "---\n\n## 2. Positional Encodings: Why and How\n\n### The Problem\nSelf-attention has NO concept of order!\n- \"Dog bites man\" and \"Man bites dog\" look identical to attention.\n- RNNs have built-in order (process sequentially), but Transformers don't.\n\n### The Solution: Positional Encodings\nAdd position information to each token embedding.\n\n$$\n\\begin{align*}\nPE_{(pos, 2i)} &= \\sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos, 2i+1)} &= \\cos(pos / 10000^{2i/d_{model}})\n\\end{align*}\n$$\n\nWhere:\n- $pos$: Position in sequence (0, 1, 2, ...)\n- $i$: Dimension index\n- $d_{model}$: Embedding dimension\n\n### Why Sinusoidal?\n1. **Unique** for each position\n2. **Relative** positions have consistent patterns\n3. **Generalizes** to longer sequences than seen in training\n\n### Modern Alternatives (2024+)\n- **Learned PE**: Train positional embeddings like word embeddings\n- **RoPE** (Rotary): Used in Llama, PaLM (multiplies instead of adds)\n- **ALiBi**: Attention bias based on distance\n\n### FAANG Interview Question\n**\"Why do Transformers need positional encodings?\"** ‚Üê Asked at Google, OpenAI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6fjjrrmwf3h",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Sinusoidal positional encoding (from \"Attention Is All You Need\").\n    This is THE standard implementation asked in interviews!\n    \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        # Create matrix of shape (max_len, d_model)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # div_term = 10000^(2i/d_model)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        # Apply sin to even indices, cos to odd indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Add batch dimension: (1, max_len, d_model)\n        pe = pe.unsqueeze(0)\n        \n        # Register as buffer (not a parameter, but saved with model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        \"\"\"\n        x: (batch, seq_len, d_model)\n        \"\"\"\n        # Add positional encoding to input\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\n# Visualize positional encodings\nd_model = 128\nmax_len = 100\n\npos_enc = PositionalEncoding(d_model, max_len)\npe_matrix = pos_enc.pe.squeeze(0).numpy()\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu', interpolation='nearest')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.title('Positional Encoding Matrix')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.plot(pe_matrix[:, :4])\nplt.xlabel('Position')\nplt.ylabel('Value')\nplt.title('First 4 Dimensions')\nplt.legend([f'Dim {i}' for i in range(4)])\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úì Each position gets a unique encoding!\")\nprint(\"‚úì Sinusoidal pattern allows model to learn relative positions.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "okqsd93q8l9",
   "source": "## The Problem with RNNs: Vanishing Gradients\n\nWhen sequences get long (100+ tokens), gradients vanish during backprop:\n- Gradient flows: Output ‚Üí h_100 ‚Üí h_99 ‚Üí ... ‚Üí h_1\n- Each step multiplies by a matrix\n- Repeated multiplication makes gradients explode or vanish\n\n**Result:** RNNs can't remember long-term dependencies.\n\n---\n\n## LSTM: Long Short-Term Memory (1997, Still Used Today!)\n\n### The Breakthrough\nAdd explicit **memory cells** with **gates** that control:\n1. **Forget gate**: What to remove from memory\n2. **Input gate**: What new information to add\n3. **Output gate**: What to expose as hidden state\n\n### The Equations (Don't Panic!)\n$$\n\\begin{align*}\nf_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad &\\text{(Forget gate)} \\\\\ni_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad &\\text{(Input gate)} \\\\\n\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad &\\text{(Candidate values)} \\\\\nC_t &= f_t * C_{t-1} + i_t * \\tilde{C}_t \\quad &\\text{(Update cell state)} \\\\\no_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad &\\text{(Output gate)} \\\\\nh_t &= o_t * \\tanh(C_t) \\quad &\\text{(Final hidden state)}\n\\end{align*}\n$$\n\n### The Intuition\n- **Cell state** ($C_t$): The \"highway\" where information flows unmodified\n- **Gates**: Traffic lights that decide what flows through\n\n### FAANG Interview Alert\n**\"Explain the difference between RNN and LSTM\"** ‚Üê Top 5 most asked question!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "d6r3vcwk9ij",
   "source": "class LSTMCell(nn.Module):\n    \"\"\"\n    LSTM Cell from scratch - asked at Google, Meta, Amazon!\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # All 4 gates can be computed together (efficiency trick!)\n        # Input: [h_{t-1}; x_t] concatenated\n        self.gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n    \n    def forward(self, x, states):\n        \"\"\"\n        x: (batch, input_size)\n        states: (h_prev, c_prev) where each is (batch, hidden_size)\n        \"\"\"\n        h_prev, c_prev = states\n        \n        # Concatenate input and hidden state\n        combined = torch.cat([x, h_prev], dim=1)\n        \n        # Compute all gates at once, then split\n        gates = self.gates(combined)\n        i, f, g, o = gates.chunk(4, dim=1)\n        \n        # Apply activations\n        i = torch.sigmoid(i)  # Input gate\n        f = torch.sigmoid(f)  # Forget gate\n        g = torch.tanh(g)     # Candidate values\n        o = torch.sigmoid(o)  # Output gate\n        \n        # Update cell state (the memory!)\n        c_t = f * c_prev + i * g\n        \n        # Update hidden state\n        h_t = o * torch.tanh(c_t)\n        \n        return h_t, c_t\n\n# Test LSTM\nlstm_cell = LSTMCell(input_size=10, hidden_size=20)\n\nh = torch.zeros(4, 20)\nc = torch.zeros(4, 20)\n\nprint(\"LSTM processing sequence:\")\nfor t in range(5):\n    x_t = torch.randn(4, 10)\n    h, c = lstm_cell(x_t, (h, c))\n    print(f\"Step {t}: h={h.shape}, c={c.shape}\")\n\nprint(\"\\\\n‚úì LSTM maintains TWO states: hidden (h) and cell (c)!\")\nprint(\"The cell state (c) is the 'memory highway' that solves vanishing gradients.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2q6aj8bsp6o",
   "source": "---\n\n## Seq2Seq: Sequence-to-Sequence Models (The Bridge to Transformers)\n\n### The Problem\n- RNN/LSTM can process one sequence ‚Üí one output\n- But what about sequence ‚Üí sequence? (Translation, Summarization)\n  - Input: \"Hello world\" ‚Üí Output: \"Hola mundo\"\n\n### The Solution: Encoder-Decoder Architecture (2014)\n\n```\nEncoder (LSTM)                Decoder (LSTM)\n    ‚Üì                              ‚Üì\nInput Sequence  ‚Üí  Context Vector  ‚Üí  Output Sequence\n\"Hello world\"       (fixed size)      \"Hola mundo\"\n```\n\n**How it works:**\n1. **Encoder**: Reads input sequence, compresses to fixed-size vector (context)\n2. **Decoder**: Generates output sequence from context vector\n\n### The Problem with Seq2Seq\n**Bottleneck**: All information compressed into ONE vector!\n- Long sequences lose information\n- This is what **Attention** was invented to solve (2015)\n\n### FAANG Interview Question\n**\"Explain the Seq2Seq architecture and its limitation\"** ‚Üê Asked at Google, Meta\n\n**Answer:**\n1. Encoder processes input ‚Üí context vector\n2. Decoder generates output from context\n3. **Limitation**: Fixed-size context is a bottleneck for long sequences\n4. **Solution**: Attention mechanism (attend to different encoder states)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o2qrnhdjgco",
   "source": "# Seq2Seq Implementation (Simplified)\n\nclass Encoder(nn.Module):\n    \"\"\"Encodes input sequence into a context vector\"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n    \n    def forward(self, x):\n        # x: (batch, seq_len) - token IDs\n        embedded = self.embedding(x)  # (batch, seq_len, hidden_size)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        # Return final hidden state as context\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    \"\"\"Generates output sequence from context vector\"\"\"\n    def __init__(self, output_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden, cell):\n        # x: (batch, 1) - one token at a time\n        embedded = self.embedding(x)  # (batch, 1, hidden_size)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))  # (batch, output_size)\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    \"\"\"\n    Complete Seq2Seq model for sequence-to-sequence tasks.\n    Used in early machine translation systems (pre-Transformer).\n    \"\"\"\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        \"\"\"\n        src: source sequence (batch, src_len)\n        trg: target sequence (batch, trg_len)\n        teacher_forcing_ratio: probability of using true token vs predicted\n        \"\"\"\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.fc.out_features\n        \n        # Store outputs\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n        \n        # Encode entire input sequence\n        hidden, cell = self.encoder(src)\n        \n        # First input to decoder is <SOS> token\n        input_token = trg[:, 0].unsqueeze(1)\n        \n        # Generate output sequence one token at a time\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input_token, hidden, cell)\n            outputs[:, t, :] = output\n            \n            # Teacher forcing: use true token or predicted token?\n            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1).unsqueeze(1)\n            input_token = trg[:, t].unsqueeze(1) if use_teacher_forcing else top1\n        \n        return outputs\n\n# Example\nvocab_size = 1000\nhidden_size = 256\n\nencoder = Encoder(vocab_size, hidden_size)\ndecoder = Decoder(vocab_size, hidden_size)\nseq2seq = Seq2Seq(encoder, decoder)\n\n# Dummy data\nsrc = torch.randint(0, vocab_size, (4, 10))  # Batch of 4, length 10\ntrg = torch.randint(0, vocab_size, (4, 12))  # Batch of 4, length 12\n\noutputs = seq2seq(src, trg)\nprint(f\"Input shape: {src.shape}\")\nprint(f\"Target shape: {trg.shape}\")\nprint(f\"Output shape: {outputs.shape}\")\nprint(\"\\\\n‚úì Seq2Seq translates variable-length sequences!\")\nprint(\"\\\\nThis was state-of-the-art for translation (2014-2017)\")\nprint(\"Then Transformers came and changed everything...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Self-Attention\n",
    "\n",
    "In a sentence like \"The animal didn't cross the street because **it** was too tired\", what does \"it\" refer to? The street or the animal?\n",
    "\n",
    "Self-attention allows the model to look at other words in the sentence to figure this out. It computes a weighted sum of all other words.\n",
    "\n",
    "Formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What I'm looking for\n",
    "- **K (Key)**: What I have to offer\n",
    "- **V (Value)**: What I actually contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n# PART 3: GPT vs BERT Architectures (Must Know!)\n\nThis is **THE most important interview topic for NLP roles in 2025**.\n\n## The Two Paradigms\n\n### BERT: Bidirectional Encoder (Google, 2018)\n- **Architecture**: Encoder-only\n- **Training**: Masked Language Modeling (predict masked words)\n- **Use case**: Understanding (classification, NER, Q&A)\n- **Example**: \"The [MASK] is shining\" ‚Üí predict \"sun\"\n- **Key**: Sees ENTIRE sentence (bidirectional context)\n\n### GPT: Autoregressive Decoder (OpenAI, 2018)\n- **Architecture**: Decoder-only\n- **Training**: Next token prediction (left-to-right)\n- **Use case**: Generation (text completion, chat)\n- **Example**: \"The sun is\" ‚Üí predict \"shining\"\n- **Key**: Sees ONLY previous tokens (causal/autoregressive)\n\n---\n\n## The Critical Difference: Attention Masking\n\n### BERT (Bidirectional)\n```\nAttention weights for \"is\":\n  The  sun  is shining\n  ‚úì    ‚úì   ‚úì    ‚úì      (can attend to ALL words)\n```\n\n### GPT (Causal)  \n```\nAttention weights for \"is\":\n  The  sun  is shining\n  ‚úì    ‚úì   ‚úì    ‚úó      (can ONLY attend to past, not future!)\n```\n\n---\n\n## FAANG Interview Questions\n\n**Q1: \"When would you use BERT vs GPT?\"**\n```\nBERT:\n- Classification (sentiment, spam detection)\n- Named Entity Recognition\n- Question Answering\n- Sentence embeddings\n\nGPT:\n- Text generation\n- Chatbots\n- Code completion\n- Creative writing\n```\n\n**Q2: \"How do you implement causal masking?\"**\n```python\n# This creates a lower-triangular matrix\n# Future tokens are masked with -inf\nmask = torch.tril(torch.ones(seq_len, seq_len))\nmask = mask.masked_fill(mask == 0, float('-inf'))\nscores = scores + mask  # Before softmax\n```\n\n**Q3: \"Why can't BERT generate text?\"**\n```\nAnswer: BERT is trained to predict masked words using \nbidirectional context. It's not trained for autoregressive \ngeneration (predicting next token). You'd need to fine-tune \nit differently or use a decoder.\n```\n\n---\n\n## Architecture Comparison Table\n\n| Feature | BERT | GPT |\n|---------|------|-----|\n| **Type** | Encoder | Decoder |\n| **Attention** | Bidirectional | Causal (masked) |\n| **Training** | MLM (predict masks) | Next token prediction |\n| **Input** | [CLS] text [SEP] | text |\n| **Output** | Token embeddings | Next token logits |\n| **Best for** | Understanding | Generation |\n| **Examples** | RoBERTa, ALBERT | GPT-3, ChatGPT |\n\n---\n\n## What Powers ChatGPT?\n- **Architecture**: Decoder-only (GPT)\n- **Size**: 175B parameters (GPT-3) ‚Üí 1.76T parameters (GPT-4 rumored)\n- **Training**: Causal language modeling + RLHF\n- **Context**: 8K tokens (GPT-3) ‚Üí 128K tokens (GPT-4 Turbo)\n\n---\n\n## Key Takeaways\n\n1. **BERT = Encoder = Understanding**\n2. **GPT = Decoder = Generation**\n3. **Causal mask** is the key difference\n4. **Modern trend**: Decoder-only models (Llama, PaLM, GPT) dominate\n5. **Why?**: Decoders can do both understanding AND generation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's Transformer Modules\n",
    "\n",
    "PyTorch provides optimized implementations so you don't have to write everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Multi-Head Attention Layer\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)\n",
    "\n",
    "# Create dummy input: (Batch, Seq_Len, Embed_Dim)\n",
    "x = torch.randn(32, 10, 256)\n",
    "\n",
    "# Self-attention: Q=x, K=x, V=x\n",
    "attn_output, _ = multihead_attn(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {attn_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PART 4: Tokenization Deep Dive (Critical for LLMs)\n\n## The Problem\nNeural networks work with numbers, not text. How do we convert text ‚Üí numbers?\n\n### Naive Approaches (Don't Use!)\n1. **Character-level**: \"Hello\" ‚Üí [H, e, l, l, o] ‚Üí Too long, loses word meaning\n2. **Word-level**: \"Hello\" ‚Üí [Hello] ‚Üí Huge vocabulary (1M+ words)\n\n### Modern Solutions: Subword Tokenization\n\n## BPE (Byte Pair Encoding) - Used in GPT\n\n**Idea:** Start with characters, merge most frequent pairs\n\n```\nStep 1: \"lower\" ‚Üí [\"l\", \"o\", \"w\", \"e\", \"r\"]\nStep 2: Merge \"e\"+\"r\" ‚Üí [\"l\", \"o\", \"w\", \"er\"]  (if most frequent)\nStep 3: Merge \"l\"+\"o\" ‚Üí [\"lo\", \"w\", \"er\"]\n...\n```\n\n**Result:** \n- Common words: single token (\"the\" ‚Üí [the])\n- Rare words: multiple subtokens (\"chatbot\" ‚Üí [\"chat\", \"bot\"])\n- Unknown words: character-level fallback\n\n## WordPiece - Used in BERT\n\nSimilar to BPE but uses likelihood-based merging.\n\n## SentencePiece - Used in Llama, T5\n\nLanguage-agnostic, works directly on Unicode.\n\n---\n\n## FAANG Interview Questions\n\n**Q1: \"Why use subword tokenization instead of word-level?\"**\n```\nAdvantages:\n1. Fixed vocabulary size (32K-100K vs 1M+ words)\n2. Handles rare/unknown words (no <UNK> token)\n3. Captures morphology (\"play\", \"playing\", \"played\" share \"play\")\n4. Language-agnostic\n```\n\n**Q2: \"What's the difference between BPE and WordPiece?\"**\n```\nBPE: Merge most frequent pairs\nWordPiece: Merge pairs that maximize likelihood on training data\nBoth achieve similar results in practice.\n```\n\n**Q3: \"How does GPT tokenize 'ü§ñ'?\"**\n```\nEmojis, special characters: Usually single or multi-byte tokens\nGPT uses UTF-8 bytes ‚Üí BPE\nMost modern tokenizers handle Unicode natively\n```\n\n---\n\n## Tokenization in Practice\n\n```python\nfrom transformers import AutoTokenizer\n\n# Load GPT-2 tokenizer (BPE)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntext = \"Hello, world!\"\ntokens = tokenizer.tokenize(text)\nids = tokenizer.encode(text)\n\nprint(f\"Text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"IDs: {ids}\")\n\n# Decode back\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n```\n\n---\n\n## Key Concepts\n\n### Special Tokens\n- `[CLS]`: Classification token (BERT)\n- `[SEP]`: Separator (BERT)\n- `<BOS>`: Beginning of sequence (GPT)\n- `<EOS>`: End of sequence\n- `<PAD>`: Padding (for batching)\n- `<UNK>`: Unknown word (rare in BPE)\n\n### Vocabulary Size Impact\n- **Small vocab (8K)**: More tokens per sentence, longer sequences\n- **Large vocab (100K)**: Fewer tokens per sentence, but larger embedding matrix\n- **Sweet spot**: 32K-50K for most LLMs\n\n---\n\n## Tokenization Challenges\n\n1. **Numbers**: \"1234\" might be [\"1\", \"23\", \"4\"] - loses semantic meaning\n2. **Code**: Indentation, brackets often split poorly\n3. **Multilingual**: English-centric tokenizers waste tokens on other languages\n4. **Trailing spaces**: \"Hello\" vs \"Hello \" are different tokens!\n\n---\n\n## Modern Trends (2024-2025)\n\n- **Larger vocabs**: GPT-4 likely uses 100K+ (unconfirmed)\n- **Multimodal**: Image patches as \"visual tokens\" (CLIP, Flamingo)\n- **Byte-level**: Direct UTF-8 bytes (more universal)\n\n---\n\n**Tokenization is the \"dark art\" of NLP - small changes have huge impacts!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "output = transformer_encoder(x)\n",
    "print(f\"Encoder Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (Concept)\n",
    "\n",
    "Transformers don't understand text; they understand numbers. Tokenization converts text to numbers.\n",
    "\n",
    "1. Text: \"I love AI\"\n",
    "2. Tokens: [\"I\", \"love\", \"AI\"]\n",
    "3. IDs: [101, 204, 505]\n",
    "\n",
    "*(In practice, we use libraries like `huggingface/tokenizers`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Self-Attention**: The mechanism that relates different positions of a sequence.\n",
    "2. **Q, K, V**: The three projections used to compute attention.\n",
    "3. **Multi-Head Attention**: Running multiple attention mechanisms in parallel.\n",
    "4. **Transformer**: A stack of attention and feed-forward layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}