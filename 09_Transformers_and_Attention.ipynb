{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Transformers and Attention\n",
    "\n",
    "The Transformer architecture has revolutionized AI, powering everything from ChatGPT to Stable Diffusion. In this notebook, we'll understand the core mechanism behind it: **Self-Attention**.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Self-Attention mechanism\n",
    "- Implement a single Self-Attention head from scratch\n",
    "- Use PyTorch's built-in Transformer modules\n",
    "- Understand Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Self-Attention\n",
    "\n",
    "In a sentence like \"The animal didn't cross the street because **it** was too tired\", what does \"it\" refer to? The street or the animal?\n",
    "\n",
    "Self-attention allows the model to look at other words in the sentence to figure this out. It computes a weighted sum of all other words.\n",
    "\n",
    "Formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What I'm looking for\n",
    "- **K (Key)**: What I have to offer\n",
    "- **V (Value)**: What I actually contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    d_k = query.size(-1)\n",
    "    # 1. Compute scores: Q @ K.T\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. Apply Softmax to get probabilities\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 3. Multiply by V\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example\n",
    "d_model = 4  # Embedding dimension\n",
    "seq_len = 3  # \"I love AI\"\n",
    "\n",
    "# Random embeddings for Q, K, V\n",
    "q = torch.randn(1, seq_len, d_model)\n",
    "k = torch.randn(1, seq_len, d_model)\n",
    "v = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(weights)\n",
    "print(\"\\nOutput:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's Transformer Modules\n",
    "\n",
    "PyTorch provides optimized implementations so you don't have to write everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Multi-Head Attention Layer\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)\n",
    "\n",
    "# Create dummy input: (Batch, Seq_Len, Embed_Dim)\n",
    "x = torch.randn(32, 10, 256)\n",
    "\n",
    "# Self-attention: Q=x, K=x, V=x\n",
    "attn_output, _ = multihead_attn(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {attn_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Transformer Encoder\n",
    "\n",
    "A Transformer Encoder consists of multiple layers of Self-Attention + Feed Forward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "output = transformer_encoder(x)\n",
    "print(f\"Encoder Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (Concept)\n",
    "\n",
    "Transformers don't understand text; they understand numbers. Tokenization converts text to numbers.\n",
    "\n",
    "1. Text: \"I love AI\"\n",
    "2. Tokens: [\"I\", \"love\", \"AI\"]\n",
    "3. IDs: [101, 204, 505]\n",
    "\n",
    "*(In practice, we use libraries like `huggingface/tokenizers`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Self-Attention**: The mechanism that relates different positions of a sequence.\n",
    "2. **Q, K, V**: The three projections used to compute attention.\n",
    "3. **Multi-Head Attention**: Running multiple attention mechanisms in parallel.\n",
    "4. **Transformer**: A stack of attention and feed-forward layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
