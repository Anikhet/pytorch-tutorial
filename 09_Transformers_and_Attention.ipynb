{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Tutorial: Sequence Models - RNNs to Transformers\n\nFrom simple RNNs to modern Transformers, this notebook covers the evolution of sequence modeling that powers ChatGPT, Google Translate, and Siri.\n\n## Learning Objectives\n- **Part 1**: Understand RNN, LSTM, GRU (The foundations - still asked in interviews!)\n- **Part 2**: Implement Seq2Seq models (Translation, summarization)\n- **Part 3**: Deep dive into Self-Attention and Transformers\n- **Part 4**: Positional Encodings and why they matter\n- **Part 5**: GPT vs BERT architectures"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ozsw1579dkg",
   "source": "---\n\n# PART 2: Transformers and Self-Attention\n\n## From RNN to Transformers: The Paradigm Shift (2017)\n\n**The Problem with RNNs/LSTMs:**\n- Sequential processing (can't parallelize)\n- Still struggle with very long sequences\n- Slow to train\n\n**The Solution: Self-Attention (2017 - \"Attention Is All You Need\")**\n- Process entire sequence in parallel\n- Every token attends to every other token directly\n- **10x faster to train than LSTMs**\n\n---\n\n## 1. Self-Attention: The Core Mechanism\n\nIn a sentence like \"The animal didn't cross the street because **it** was too tired\", what does \"it\" refer to? The street or the animal?\n\nSelf-attention allows the model to look at ALL other words simultaneously to figure this out.\n\n### The Formula\n$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n\nWhere:\n- **Q (Query)**: What I'm looking for (\"Who does 'it' refer to?\")\n- **K (Key)**: What I have to offer (\"I'm the word 'animal'\")\n- **V (Value)**: What I actually contain (embedding of 'animal')\n\n### The Intuition\n1. Each word asks a question (Q): \"Which words are relevant to me?\"\n2. Each word offers information (K): \"I'm a noun/verb/etc\"\n3. Calculate compatibility: Q ¬∑ K (dot product)\n4. Attend to relevant words: softmax(scores) √ó V\n\n### FAANG Interview Question\n**\"Implement scaled dot-product attention from scratch\"** ‚Üê Asked at all FAANG!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7k9kao26m9l",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Implement attention from scratch for visualization\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    The EXACT function asked in FAANG interviews!\n    \n    Args:\n        Q: Query matrix (batch, seq_len, d_k)\n        K: Key matrix (batch, seq_len, d_k)\n        V: Value matrix (batch, seq_len, d_v)\n        mask: Optional mask (batch, seq_len, seq_len)\n    \n    Returns:\n        output, attention_weights\n    \"\"\"\n    d_k = Q.size(-1)\n    \n    # Step 1: Compute attention scores (Q @ K^T)\n    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n    \n    # Step 2: Scale by sqrt(d_k)\n    scores = scores / math.sqrt(d_k)\n    \n    # Step 3: Apply mask (for causal attention)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    \n    # Step 4: Apply softmax\n    attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)\n    \n    # Step 5: Multiply by V\n    output = torch.matmul(attention_weights, V)  # (batch, seq_len, d_v)\n    \n    return output, attention_weights\n\n# Create a simple example sentence\nsentence = \"The animal didn't cross the street because it was tired\"\ntokens = sentence.split()\nseq_len = len(tokens)\n\n# Create dummy embeddings (normally from embedding layer)\nd_model = 8\ntorch.manual_seed(42)\nembeddings = torch.randn(1, seq_len, d_model)\n\n# Create Q, K, V projections (simplified - normally learned)\nW_q = nn.Linear(d_model, d_model, bias=False)\nW_k = nn.Linear(d_model, d_model, bias=False)\nW_v = nn.Linear(d_model, d_model, bias=False)\n\nQ = W_q(embeddings)\nK = W_k(embeddings)\nV = W_v(embeddings)\n\n# Compute attention\noutput, attention_weights = scaled_dot_product_attention(Q, K, V)\n\n# Visualize\nfig = plt.figure(figsize=(16, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.4)\n\n# 1. Attention heatmap\nax1 = fig.add_subplot(gs[0:2, 0:2])\nattn_numpy = attention_weights[0].detach().numpy()\nsns.heatmap(attn_numpy, annot=True, fmt='.2f', cmap='YlOrRd', \n            xticklabels=tokens, yticklabels=tokens, ax=ax1, \n            cbar_kws={'label': 'Attention Weight'}, vmin=0, vmax=1)\nax1.set_xlabel('Key (What others offer)', fontsize=12)\nax1.set_ylabel('Query (What I\\'m looking for)', fontsize=12)\nax1.set_title('Self-Attention Heatmap\\n(Row = Query token, Col = Attended token)', \n             fontsize=13, fontweight='bold')\nplt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n\n# Highlight \"it\" row\nit_idx = tokens.index('it')\nax1.add_patch(plt.Rectangle((0, it_idx), len(tokens), 1, \n                            fill=False, edgecolor='blue', linewidth=3))\nax1.text(-1, it_idx + 0.5, '‚Üí', fontsize=20, color='blue', \n        ha='right', va='center', fontweight='bold')\n\n# 2. Focus on \"it\" token\nax2 = fig.add_subplot(gs[0, 2])\nit_attention = attn_numpy[it_idx]\ncolors = ['red' if i == tokens.index('animal') else 'blue' for i in range(seq_len)]\nbars = ax2.barh(range(seq_len), it_attention, color=colors, edgecolor='black')\nax2.set_yticks(range(seq_len))\nax2.set_yticklabels(tokens, fontsize=10)\nax2.set_xlabel('Attention Weight', fontsize=11)\nax2.set_title(f'What does \"it\" attend to?', fontsize=12, fontweight='bold')\nax2.axvline(x=0.1, color='gray', linestyle='--', alpha=0.5)\nax2.grid(True, alpha=0.3, axis='x')\n\n# Add values\nfor i, (bar, val) in enumerate(zip(bars, it_attention)):\n    ax2.text(val + 0.005, i, f'{val:.3f}', va='center', fontsize=9)\n\n# 3. Step-by-step breakdown\nax3 = fig.add_subplot(gs[1, 2])\nax3.axis('off')\nsteps_text = \"\"\"\nAttention Steps for \"it\":\n\n1Ô∏è‚É£ Query: What is \"it\"?\n   Q_it = W_q √ó emb_it\n   \n2Ô∏è‚É£ Keys: What do others offer?\n   K_i = W_k √ó emb_i for all i\n   \n3Ô∏è‚É£ Scores: How relevant?\n   score_i = Q_it ¬∑ K_i / ‚àöd_k\n   \n4Ô∏è‚É£ Softmax: Normalize to [0,1]\n   attn_i = softmax(scores)\n   \n5Ô∏è‚É£ Output: Weighted sum\n   out_it = Œ£ attn_i √ó V_i\n\nResult: \"it\" attends most to\n\"animal\" (not \"street\")!\n\"\"\"\nax3.text(0.05, 0.5, steps_text, fontsize=10, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n        verticalalignment='center')\n\n# 4. QKV matrix visualization\nax4 = fig.add_subplot(gs[2, 0])\nax4.imshow(Q[0, :3, :].detach().numpy(), cmap='Blues', aspect='auto')\nax4.set_yticks(range(3))\nax4.set_yticklabels(tokens[:3])\nax4.set_xlabel('Dimension', fontsize=10)\nax4.set_title('Query Matrix (Q)\\n(First 3 tokens)', fontsize=11, fontweight='bold')\nax4.set_xticks([])\n\nax5 = fig.add_subplot(gs[2, 1])\nax5.imshow(K[0, :3, :].detach().numpy(), cmap='Greens', aspect='auto')\nax5.set_yticks(range(3))\nax5.set_yticklabels(tokens[:3])\nax5.set_xlabel('Dimension', fontsize=10)\nax5.set_title('Key Matrix (K)\\n(First 3 tokens)', fontsize=11, fontweight='bold')\nax5.set_xticks([])\n\nax6 = fig.add_subplot(gs[2, 2])\nax6.imshow(V[0, :3, :].detach().numpy(), cmap='Oranges', aspect='auto')\nax6.set_yticks(range(3))\nax6.set_yticklabels(tokens[:3])\nax6.set_xlabel('Dimension', fontsize=10)\nax6.set_title('Value Matrix (V)\\n(First 3 tokens)', fontsize=11, fontweight='bold')\nax6.set_xticks([])\n\nplt.suptitle('Self-Attention Mechanism: Complete Breakdown', \n            fontsize=16, fontweight='bold', y=0.98)\nplt.show()\n\nprint(\"üîç Key Insights:\")\nprint(f\"‚Ä¢ 'it' attends most to '{tokens[np.argmax(it_attention)]}' (weight: {np.max(it_attention):.3f})\")\nprint(f\"‚Ä¢ This is learned automatically during training!\")\nprint(f\"‚Ä¢ Each token can attend to ALL other tokens simultaneously\")\nprint(f\"\\nüí° FAANG Interview Tip:\")\nprint(\"Be ready to implement scaled_dot_product_attention() on a whiteboard!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wj6rpikhufg",
   "source": "### Visualization: How Self-Attention Works\n\nLet's visualize the attention mechanism step-by-step with a concrete example.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y5zper7fq3m",
   "source": "class SimpleRNNCell(nn.Module):\n    \"\"\"\n    A single RNN cell (one timestep).\n    This is the #1 interview question for sequence modeling!\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # Weights for input -> hidden\n        self.W_xh = nn.Linear(input_size, hidden_size)\n        # Weights for hidden -> hidden (the recurrent part!)\n        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n        \n        self.tanh = nn.Tanh()\n    \n    def forward(self, x, h_prev):\n        \"\"\"\n        x: input at current timestep (batch_size, input_size)\n        h_prev: hidden state from previous timestep (batch_size, hidden_size)\n        \"\"\"\n        h_t = self.tanh(self.W_xh(x) + self.W_hh(h_prev))\n        return h_t\n\n# Test the RNN cell\ninput_size = 10\nhidden_size = 20\nbatch_size = 4\n\nrnn_cell = SimpleRNNCell(input_size, hidden_size)\n\n# Initialize hidden state\nh = torch.zeros(batch_size, hidden_size)\n\n# Process a sequence of 5 timesteps\nsequence_length = 5\nfor t in range(sequence_length):\n    x_t = torch.randn(batch_size, input_size)\n    h = rnn_cell(x_t, h)\n    print(f\"Timestep {t}: hidden state shape = {h.shape}\")\n\nprint(\"\\\\n‚úì RNN processes sequence step-by-step, maintaining hidden state!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zwy2uw68c6",
   "source": "---\n\n## 2. Positional Encodings: Why and How\n\n### The Problem\nSelf-attention has NO concept of order!\n- \"Dog bites man\" and \"Man bites dog\" look identical to attention.\n- RNNs have built-in order (process sequentially), but Transformers don't.\n\n### The Solution: Positional Encodings\nAdd position information to each token embedding.\n\n$$\n\\begin{align*}\nPE_{(pos, 2i)} &= \\sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos, 2i+1)} &= \\cos(pos / 10000^{2i/d_{model}})\n\\end{align*}\n$$\n\nWhere:\n- $pos$: Position in sequence (0, 1, 2, ...)\n- $i$: Dimension index\n- $d_{model}$: Embedding dimension\n\n### Why Sinusoidal?\n1. **Unique** for each position\n2. **Relative** positions have consistent patterns\n3. **Generalizes** to longer sequences than seen in training\n\n### Modern Alternatives (2024+)\n- **Learned PE**: Train positional embeddings like word embeddings\n- **RoPE** (Rotary): Used in Llama, PaLM (multiplies instead of adds)\n- **ALiBi**: Attention bias based on distance\n\n### FAANG Interview Question\n**\"Why do Transformers need positional encodings?\"** ‚Üê Asked at Google, OpenAI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6fjjrrmwf3h",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Sinusoidal positional encoding (from \"Attention Is All You Need\").\n    This is THE standard implementation asked in interviews!\n    \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        # Create matrix of shape (max_len, d_model)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # div_term = 10000^(2i/d_model)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        # Apply sin to even indices, cos to odd indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Add batch dimension: (1, max_len, d_model)\n        pe = pe.unsqueeze(0)\n        \n        # Register as buffer (not a parameter, but saved with model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        \"\"\"\n        x: (batch, seq_len, d_model)\n        \"\"\"\n        # Add positional encoding to input\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\n# Visualize positional encodings\nd_model = 128\nmax_len = 100\n\npos_enc = PositionalEncoding(d_model, max_len)\npe_matrix = pos_enc.pe.squeeze(0).numpy()\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu', interpolation='nearest')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.title('Positional Encoding Matrix')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.plot(pe_matrix[:, :4])\nplt.xlabel('Position')\nplt.ylabel('Value')\nplt.title('First 4 Dimensions')\nplt.legend([f'Dim {i}' for i in range(4)])\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úì Each position gets a unique encoding!\")\nprint(\"‚úì Sinusoidal pattern allows model to learn relative positions.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "okqsd93q8l9",
   "source": "## The Problem with RNNs: Vanishing Gradients\n\nWhen sequences get long (100+ tokens), gradients vanish during backprop:\n- Gradient flows: Output ‚Üí h_100 ‚Üí h_99 ‚Üí ... ‚Üí h_1\n- Each step multiplies by a matrix\n- Repeated multiplication makes gradients explode or vanish\n\n**Result:** RNNs can't remember long-term dependencies.\n\n---\n\n## LSTM: Long Short-Term Memory (1997, Still Used Today!)\n\n### The Breakthrough\nAdd explicit **memory cells** with **gates** that control:\n1. **Forget gate**: What to remove from memory\n2. **Input gate**: What new information to add\n3. **Output gate**: What to expose as hidden state\n\n### The Equations (Don't Panic!)\n$$\n\\begin{align*}\nf_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad &\\text{(Forget gate)} \\\\\ni_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad &\\text{(Input gate)} \\\\\n\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad &\\text{(Candidate values)} \\\\\nC_t &= f_t * C_{t-1} + i_t * \\tilde{C}_t \\quad &\\text{(Update cell state)} \\\\\no_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad &\\text{(Output gate)} \\\\\nh_t &= o_t * \\tanh(C_t) \\quad &\\text{(Final hidden state)}\n\\end{align*}\n$$\n\n### The Intuition\n- **Cell state** ($C_t$): The \"highway\" where information flows unmodified\n- **Gates**: Traffic lights that decide what flows through\n\n### FAANG Interview Alert\n**\"Explain the difference between RNN and LSTM\"** ‚Üê Top 5 most asked question!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "d6r3vcwk9ij",
   "source": "class LSTMCell(nn.Module):\n    \"\"\"\n    LSTM Cell from scratch - asked at Google, Meta, Amazon!\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # All 4 gates can be computed together (efficiency trick!)\n        # Input: [h_{t-1}; x_t] concatenated\n        self.gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n    \n    def forward(self, x, states):\n        \"\"\"\n        x: (batch, input_size)\n        states: (h_prev, c_prev) where each is (batch, hidden_size)\n        \"\"\"\n        h_prev, c_prev = states\n        \n        # Concatenate input and hidden state\n        combined = torch.cat([x, h_prev], dim=1)\n        \n        # Compute all gates at once, then split\n        gates = self.gates(combined)\n        i, f, g, o = gates.chunk(4, dim=1)\n        \n        # Apply activations\n        i = torch.sigmoid(i)  # Input gate\n        f = torch.sigmoid(f)  # Forget gate\n        g = torch.tanh(g)     # Candidate values\n        o = torch.sigmoid(o)  # Output gate\n        \n        # Update cell state (the memory!)\n        c_t = f * c_prev + i * g\n        \n        # Update hidden state\n        h_t = o * torch.tanh(c_t)\n        \n        return h_t, c_t\n\n# Test LSTM\nlstm_cell = LSTMCell(input_size=10, hidden_size=20)\n\nh = torch.zeros(4, 20)\nc = torch.zeros(4, 20)\n\nprint(\"LSTM processing sequence:\")\nfor t in range(5):\n    x_t = torch.randn(4, 10)\n    h, c = lstm_cell(x_t, (h, c))\n    print(f\"Step {t}: h={h.shape}, c={c.shape}\")\n\nprint(\"\\\\n‚úì LSTM maintains TWO states: hidden (h) and cell (c)!\")\nprint(\"The cell state (c) is the 'memory highway' that solves vanishing gradients.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2q6aj8bsp6o",
   "source": "---\n\n## Seq2Seq: Sequence-to-Sequence Models (The Bridge to Transformers)\n\n### The Problem\n- RNN/LSTM can process one sequence ‚Üí one output\n- But what about sequence ‚Üí sequence? (Translation, Summarization)\n  - Input: \"Hello world\" ‚Üí Output: \"Hola mundo\"\n\n### The Solution: Encoder-Decoder Architecture (2014)\n\n```\nEncoder (LSTM)                Decoder (LSTM)\n    ‚Üì                              ‚Üì\nInput Sequence  ‚Üí  Context Vector  ‚Üí  Output Sequence\n\"Hello world\"       (fixed size)      \"Hola mundo\"\n```\n\n**How it works:**\n1. **Encoder**: Reads input sequence, compresses to fixed-size vector (context)\n2. **Decoder**: Generates output sequence from context vector\n\n### The Problem with Seq2Seq\n**Bottleneck**: All information compressed into ONE vector!\n- Long sequences lose information\n- This is what **Attention** was invented to solve (2015)\n\n### FAANG Interview Question\n**\"Explain the Seq2Seq architecture and its limitation\"** ‚Üê Asked at Google, Meta\n\n**Answer:**\n1. Encoder processes input ‚Üí context vector\n2. Decoder generates output from context\n3. **Limitation**: Fixed-size context is a bottleneck for long sequences\n4. **Solution**: Attention mechanism (attend to different encoder states)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o2qrnhdjgco",
   "source": "# Seq2Seq Implementation (Simplified)\n\nclass Encoder(nn.Module):\n    \"\"\"Encodes input sequence into a context vector\"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n    \n    def forward(self, x):\n        # x: (batch, seq_len) - token IDs\n        embedded = self.embedding(x)  # (batch, seq_len, hidden_size)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        # Return final hidden state as context\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    \"\"\"Generates output sequence from context vector\"\"\"\n    def __init__(self, output_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden, cell):\n        # x: (batch, 1) - one token at a time\n        embedded = self.embedding(x)  # (batch, 1, hidden_size)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))  # (batch, output_size)\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    \"\"\"\n    Complete Seq2Seq model for sequence-to-sequence tasks.\n    Used in early machine translation systems (pre-Transformer).\n    \"\"\"\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        \"\"\"\n        src: source sequence (batch, src_len)\n        trg: target sequence (batch, trg_len)\n        teacher_forcing_ratio: probability of using true token vs predicted\n        \"\"\"\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.fc.out_features\n        \n        # Store outputs\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n        \n        # Encode entire input sequence\n        hidden, cell = self.encoder(src)\n        \n        # First input to decoder is <SOS> token\n        input_token = trg[:, 0].unsqueeze(1)\n        \n        # Generate output sequence one token at a time\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input_token, hidden, cell)\n            outputs[:, t, :] = output\n            \n            # Teacher forcing: use true token or predicted token?\n            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1).unsqueeze(1)\n            input_token = trg[:, t].unsqueeze(1) if use_teacher_forcing else top1\n        \n        return outputs\n\n# Example\nvocab_size = 1000\nhidden_size = 256\n\nencoder = Encoder(vocab_size, hidden_size)\ndecoder = Decoder(vocab_size, hidden_size)\nseq2seq = Seq2Seq(encoder, decoder)\n\n# Dummy data\nsrc = torch.randint(0, vocab_size, (4, 10))  # Batch of 4, length 10\ntrg = torch.randint(0, vocab_size, (4, 12))  # Batch of 4, length 12\n\noutputs = seq2seq(src, trg)\nprint(f\"Input shape: {src.shape}\")\nprint(f\"Target shape: {trg.shape}\")\nprint(f\"Output shape: {outputs.shape}\")\nprint(\"\\\\n‚úì Seq2Seq translates variable-length sequences!\")\nprint(\"\\\\nThis was state-of-the-art for translation (2014-2017)\")\nprint(\"Then Transformers came and changed everything...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9m0mckzb7f7",
   "source": "# Visualize Multi-Head Attention and Causal Masking\nfig = plt.figure(figsize=(18, 10))\ngs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.35)\n\n# Simpler sentence for clearer visualization\nsentence2 = \"I love deep learning\"\ntokens2 = sentence2.split()\nseq_len2 = len(tokens2)\n\ntorch.manual_seed(42)\nembeddings2 = torch.randn(1, seq_len2, 16)\n\n# 1. Multi-Head Attention - Different heads learn different patterns\nax1 = fig.add_subplot(gs[0, 0])\nax1.axis('off')\nax1.text(0.5, 0.9, 'Multi-Head Attention', ha='center', fontsize=14, \n        fontweight='bold', transform=ax1.transAxes)\n\n# Simulate 3 different attention heads\nnum_heads = 3\nhead_names = ['Head 1:\\nSyntax', 'Head 2:\\nSemantics', 'Head 3:\\nContext']\n\nfor head_idx in range(num_heads):\n    # Create different attention patterns for each head\n    torch.manual_seed(40 + head_idx)\n    W_q_head = nn.Linear(16, 8, bias=False)\n    W_k_head = nn.Linear(16, 8, bias=False)\n    W_v_head = nn.Linear(16, 8, bias=False)\n    \n    Q_head = W_q_head(embeddings2)\n    K_head = W_k_head(embeddings2)\n    V_head = W_v_head(embeddings2)\n    \n    _, attn_head = scaled_dot_product_attention(Q_head, K_head, V_head)\n    \n    # Create subplot for this head\n    left = 0.05 + head_idx * 0.3\n    ax_head = fig.add_axes([left, 0.55, 0.22, 0.25])\n    \n    sns.heatmap(attn_head[0].detach().numpy(), annot=True, fmt='.2f', \n                cmap='YlOrRd', xticklabels=tokens2, yticklabels=tokens2,\n                ax=ax_head, cbar=False, vmin=0, vmax=1, linewidths=0.5)\n    ax_head.set_title(head_names[head_idx], fontsize=11, fontweight='bold')\n    ax_head.set_xlabel('', fontsize=9)\n    ax_head.set_ylabel('', fontsize=9)\n    plt.setp(ax_head.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n    plt.setp(ax_head.get_yticklabels(), fontsize=9)\n\n# 2. BERT: Bidirectional Attention (no mask)\nax2 = fig.add_subplot(gs[1, 0])\ntorch.manual_seed(42)\nW_q2 = nn.Linear(16, 16, bias=False)\nW_k2 = nn.Linear(16, 16, bias=False)\nW_v2 = nn.Linear(16, 16, bias=False)\n\nQ2 = W_q2(embeddings2)\nK2 = W_k2(embeddings2)\nV2 = W_v2(embeddings2)\n\n_, attn_bidirectional = scaled_dot_product_attention(Q2, K2, V2, mask=None)\nsns.heatmap(attn_bidirectional[0].detach().numpy(), annot=True, fmt='.2f',\n            cmap='Greens', xticklabels=tokens2, yticklabels=tokens2,\n            ax=ax2, cbar_kws={'label': 'Weight'}, vmin=0, vmax=1)\nax2.set_title('BERT: Bidirectional Attention\\n(Can see ALL tokens)', \n             fontsize=12, fontweight='bold', color='darkgreen')\nax2.set_xlabel('Key', fontsize=10)\nax2.set_ylabel('Query', fontsize=10)\nplt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n\n# 3. GPT: Causal Attention (with mask)\nax3 = fig.add_subplot(gs[1, 1])\n\n# Create causal mask (lower triangular)\ncausal_mask = torch.tril(torch.ones(seq_len2, seq_len2))\n_, attn_causal = scaled_dot_product_attention(Q2, K2, V2, mask=causal_mask)\n\nsns.heatmap(attn_causal[0].detach().numpy(), annot=True, fmt='.2f',\n            cmap='Blues', xticklabels=tokens2, yticklabels=tokens2,\n            ax=ax3, cbar_kws={'label': 'Weight'}, vmin=0, vmax=1, mask=(causal_mask == 0).numpy())\nax3.set_title('GPT: Causal Attention\\n(Can only see PAST tokens)', \n             fontsize=12, fontweight='bold', color='darkblue')\nax3.set_xlabel('Key', fontsize=10)\nax3.set_ylabel('Query', fontsize=10)\nplt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n\n# Add gray for masked regions\nfor i in range(seq_len2):\n    for j in range(i+1, seq_len2):\n        ax3.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, \n                                    facecolor='gray', alpha=0.7, edgecolor='white'))\n        ax3.text(j+0.5, i+0.5, 'X', ha='center', va='center', \n                color='white', fontweight='bold', fontsize=12)\n\n# 4. Comparison explanation\nax4 = fig.add_subplot(gs[1, 2])\nax4.axis('off')\n\ncomparison_text = \"\"\"\nBERT vs GPT Attention:\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nBERT (Encoder):\n‚úì Bidirectional\n‚úì Sees entire sentence\n‚úì Best for: Classification,\n  NER, Q&A, embeddings\n  \nExample:\n\"I love [MASK] learning\"\n‚Üí Can see both sides!\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nGPT (Decoder):\n‚úì Causal (autoregressive)\n‚úì Only sees past tokens\n‚úì Best for: Generation,\n  completion, chat\n  \nExample:\n\"I love deep\" ‚Üí predict next\n‚Üí Cannot see \"learning\"!\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nThe Mask:\n‚Ä¢ Gray X = -‚àû before softmax\n‚Ä¢ Prevents \"cheating\"\n‚Ä¢ Critical for generation!\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nFAANG Interview:\n\"Implement causal mask\"\n‚Üí torch.tril() creates\n   lower-triangular matrix\n\"\"\"\n\nax4.text(0.05, 0.5, comparison_text, fontsize=9.5, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9,\n                 edgecolor='black', linewidth=2),\n        verticalalignment='center')\n\nplt.suptitle('Multi-Head Attention & Causal Masking (BERT vs GPT)', \n            fontsize=16, fontweight='bold')\nplt.show()\n\nprint(\"üéØ Key Takeaways for FAANG Interviews:\")\nprint(\"\\n1. Multi-Head Attention:\")\nprint(\"   ‚Ä¢ Run attention multiple times in parallel\")\nprint(\"   ‚Ä¢ Each head learns different patterns (syntax, semantics, etc.)\")\nprint(\"   ‚Ä¢ Concat all heads then project: Concat(head1, head2, ...) @ W_o\")\n\nprint(\"\\n2. BERT vs GPT:\")\nprint(\"   ‚Ä¢ BERT: Bidirectional (no mask) ‚Üí Understanding tasks\")\nprint(\"   ‚Ä¢ GPT: Causal (with mask) ‚Üí Generation tasks\")\nprint(\"   ‚Ä¢ Mask = torch.tril() creates lower triangular matrix\")\n\nprint(\"\\n3. Why Causal Masking?\")\nprint(\"   ‚Ä¢ Prevents the model from 'cheating' during generation\")\nprint(\"   ‚Ä¢ At inference, we generate token-by-token (can't see future)\")\nprint(\"   ‚Ä¢ Training must match inference conditions!\")\n\nprint(\"\\nüí° Interview Question:\")\nprint('Q: \"Why does GPT use causal masking?\"')\nprint('A: \"To match training and inference. During generation, we predict')\nprint('    one token at a time without seeing future tokens, so training')\nprint('    must enforce the same constraint.\"')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b3itjnot8t",
   "source": "### Visualization: Multi-Head Attention & Causal Masking\n\nLet's visualize how multi-head attention works and the critical difference between BERT and GPT.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Self-Attention\n",
    "\n",
    "In a sentence like \"The animal didn't cross the street because **it** was too tired\", what does \"it\" refer to? The street or the animal?\n",
    "\n",
    "Self-attention allows the model to look at other words in the sentence to figure this out. It computes a weighted sum of all other words.\n",
    "\n",
    "Formula:\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What I'm looking for\n",
    "- **K (Key)**: What I have to offer\n",
    "- **V (Value)**: What I actually contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n# PART 3: GPT vs BERT Architectures (Must Know!)\n\nThis is **THE most important interview topic for NLP roles in 2025**.\n\n## The Two Paradigms\n\n### BERT: Bidirectional Encoder (Google, 2018)\n- **Architecture**: Encoder-only\n- **Training**: Masked Language Modeling (predict masked words)\n- **Use case**: Understanding (classification, NER, Q&A)\n- **Example**: \"The [MASK] is shining\" ‚Üí predict \"sun\"\n- **Key**: Sees ENTIRE sentence (bidirectional context)\n\n### GPT: Autoregressive Decoder (OpenAI, 2018)\n- **Architecture**: Decoder-only\n- **Training**: Next token prediction (left-to-right)\n- **Use case**: Generation (text completion, chat)\n- **Example**: \"The sun is\" ‚Üí predict \"shining\"\n- **Key**: Sees ONLY previous tokens (causal/autoregressive)\n\n---\n\n## The Critical Difference: Attention Masking\n\n### BERT (Bidirectional)\n```\nAttention weights for \"is\":\n  The  sun  is shining\n  ‚úì    ‚úì   ‚úì    ‚úì      (can attend to ALL words)\n```\n\n### GPT (Causal)  \n```\nAttention weights for \"is\":\n  The  sun  is shining\n  ‚úì    ‚úì   ‚úì    ‚úó      (can ONLY attend to past, not future!)\n```\n\n---\n\n## FAANG Interview Questions\n\n**Q1: \"When would you use BERT vs GPT?\"**\n```\nBERT:\n- Classification (sentiment, spam detection)\n- Named Entity Recognition\n- Question Answering\n- Sentence embeddings\n\nGPT:\n- Text generation\n- Chatbots\n- Code completion\n- Creative writing\n```\n\n**Q2: \"How do you implement causal masking?\"**\n```python\n# This creates a lower-triangular matrix\n# Future tokens are masked with -inf\nmask = torch.tril(torch.ones(seq_len, seq_len))\nmask = mask.masked_fill(mask == 0, float('-inf'))\nscores = scores + mask  # Before softmax\n```\n\n**Q3: \"Why can't BERT generate text?\"**\n```\nAnswer: BERT is trained to predict masked words using \nbidirectional context. It's not trained for autoregressive \ngeneration (predicting next token). You'd need to fine-tune \nit differently or use a decoder.\n```\n\n---\n\n## Architecture Comparison Table\n\n| Feature | BERT | GPT |\n|---------|------|-----|\n| **Type** | Encoder | Decoder |\n| **Attention** | Bidirectional | Causal (masked) |\n| **Training** | MLM (predict masks) | Next token prediction |\n| **Input** | [CLS] text [SEP] | text |\n| **Output** | Token embeddings | Next token logits |\n| **Best for** | Understanding | Generation |\n| **Examples** | RoBERTa, ALBERT | GPT-3, ChatGPT |\n\n---\n\n## What Powers ChatGPT?\n- **Architecture**: Decoder-only (GPT)\n- **Size**: 175B parameters (GPT-3) ‚Üí 1.76T parameters (GPT-4 rumored)\n- **Training**: Causal language modeling + RLHF\n- **Context**: 8K tokens (GPT-3) ‚Üí 128K tokens (GPT-4 Turbo)\n\n---\n\n## Key Takeaways\n\n1. **BERT = Encoder = Understanding**\n2. **GPT = Decoder = Generation**\n3. **Causal mask** is the key difference\n4. **Modern trend**: Decoder-only models (Llama, PaLM, GPT) dominate\n5. **Why?**: Decoders can do both understanding AND generation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's Transformer Modules\n",
    "\n",
    "PyTorch provides optimized implementations so you don't have to write everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Multi-Head Attention Layer\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)\n",
    "\n",
    "# Create dummy input: (Batch, Seq_Len, Embed_Dim)\n",
    "x = torch.randn(32, 10, 256)\n",
    "\n",
    "# Self-attention: Q=x, K=x, V=x\n",
    "attn_output, _ = multihead_attn(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {attn_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# PART 4: Tokenization Deep Dive (Critical for LLMs)\n\n## The Problem\nNeural networks work with numbers, not text. How do we convert text ‚Üí numbers?\n\n### Naive Approaches (Don't Use!)\n1. **Character-level**: \"Hello\" ‚Üí [H, e, l, l, o] ‚Üí Too long, loses word meaning\n2. **Word-level**: \"Hello\" ‚Üí [Hello] ‚Üí Huge vocabulary (1M+ words)\n\n### Modern Solutions: Subword Tokenization\n\n## BPE (Byte Pair Encoding) - Used in GPT\n\n**Idea:** Start with characters, merge most frequent pairs\n\n```\nStep 1: \"lower\" ‚Üí [\"l\", \"o\", \"w\", \"e\", \"r\"]\nStep 2: Merge \"e\"+\"r\" ‚Üí [\"l\", \"o\", \"w\", \"er\"]  (if most frequent)\nStep 3: Merge \"l\"+\"o\" ‚Üí [\"lo\", \"w\", \"er\"]\n...\n```\n\n**Result:** \n- Common words: single token (\"the\" ‚Üí [the])\n- Rare words: multiple subtokens (\"chatbot\" ‚Üí [\"chat\", \"bot\"])\n- Unknown words: character-level fallback\n\n## WordPiece - Used in BERT\n\nSimilar to BPE but uses likelihood-based merging.\n\n## SentencePiece - Used in Llama, T5\n\nLanguage-agnostic, works directly on Unicode.\n\n---\n\n## FAANG Interview Questions\n\n**Q1: \"Why use subword tokenization instead of word-level?\"**\n```\nAdvantages:\n1. Fixed vocabulary size (32K-100K vs 1M+ words)\n2. Handles rare/unknown words (no <UNK> token)\n3. Captures morphology (\"play\", \"playing\", \"played\" share \"play\")\n4. Language-agnostic\n```\n\n**Q2: \"What's the difference between BPE and WordPiece?\"**\n```\nBPE: Merge most frequent pairs\nWordPiece: Merge pairs that maximize likelihood on training data\nBoth achieve similar results in practice.\n```\n\n**Q3: \"How does GPT tokenize 'ü§ñ'?\"**\n```\nEmojis, special characters: Usually single or multi-byte tokens\nGPT uses UTF-8 bytes ‚Üí BPE\nMost modern tokenizers handle Unicode natively\n```\n\n---\n\n## Tokenization in Practice\n\n```python\nfrom transformers import AutoTokenizer\n\n# Load GPT-2 tokenizer (BPE)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntext = \"Hello, world!\"\ntokens = tokenizer.tokenize(text)\nids = tokenizer.encode(text)\n\nprint(f\"Text: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"IDs: {ids}\")\n\n# Decode back\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n```\n\n---\n\n## Key Concepts\n\n### Special Tokens\n- `[CLS]`: Classification token (BERT)\n- `[SEP]`: Separator (BERT)\n- `<BOS>`: Beginning of sequence (GPT)\n- `<EOS>`: End of sequence\n- `<PAD>`: Padding (for batching)\n- `<UNK>`: Unknown word (rare in BPE)\n\n### Vocabulary Size Impact\n- **Small vocab (8K)**: More tokens per sentence, longer sequences\n- **Large vocab (100K)**: Fewer tokens per sentence, but larger embedding matrix\n- **Sweet spot**: 32K-50K for most LLMs\n\n---\n\n## Tokenization Challenges\n\n1. **Numbers**: \"1234\" might be [\"1\", \"23\", \"4\"] - loses semantic meaning\n2. **Code**: Indentation, brackets often split poorly\n3. **Multilingual**: English-centric tokenizers waste tokens on other languages\n4. **Trailing spaces**: \"Hello\" vs \"Hello \" are different tokens!\n\n---\n\n## Modern Trends (2024-2025)\n\n- **Larger vocabs**: GPT-4 likely uses 100K+ (unconfirmed)\n- **Multimodal**: Image patches as \"visual tokens\" (CLIP, Flamingo)\n- **Byte-level**: Direct UTF-8 bytes (more universal)\n\n---\n\n**Tokenization is the \"dark art\" of NLP - small changes have huge impacts!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "output = transformer_encoder(x)\n",
    "print(f\"Encoder Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (Concept)\n",
    "\n",
    "Transformers don't understand text; they understand numbers. Tokenization converts text to numbers.\n",
    "\n",
    "1. Text: \"I love AI\"\n",
    "2. Tokens: [\"I\", \"love\", \"AI\"]\n",
    "3. IDs: [101, 204, 505]\n",
    "\n",
    "*(In practice, we use libraries like `huggingface/tokenizers`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Self-Attention**: The mechanism that relates different positions of a sequence.\n",
    "2. **Q, K, V**: The three projections used to compute attention.\n",
    "3. **Multi-Head Attention**: Running multiple attention mechanisms in parallel.\n",
    "4. **Transformer**: A stack of attention and feed-forward layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}