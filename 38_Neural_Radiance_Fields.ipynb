{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Neural Radiance Fields (NeRF)\n",
    "\n",
    "Neural Radiance Fields (NeRF) represent 3D scenes as neural networks that map 3D coordinates to color and density. Given a set of 2D images, NeRF can synthesize novel views of a scene.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand implicit neural representations\n",
    "- Implement positional encoding (Fourier features)\n",
    "- Build a NeRF MLP architecture\n",
    "- Understand volume rendering\n",
    "- Learn about ray marching and sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: 3D Scene Representation\n",
    "\n",
    "Traditional 3D representations:\n",
    "- **Meshes**: Vertices + faces (discrete, hard to optimize)\n",
    "- **Voxels**: 3D grids (memory intensive)\n",
    "- **Point clouds**: Unstructured points (sparse)\n",
    "\n",
    "**NeRF's approach**: Represent the scene as a continuous function:\n",
    "$$F: (x, y, z, \\theta, \\phi) \\rightarrow (r, g, b, \\sigma)$$\n",
    "\n",
    "Where:\n",
    "- $(x, y, z)$: 3D position\n",
    "- $(\\theta, \\phi)$: Viewing direction\n",
    "- $(r, g, b)$: Color\n",
    "- $\\sigma$: Volume density (opacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the concept\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Traditional representations\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "# Simple mesh (cube)\n",
    "vertices = np.array([[0,0,0], [1,0,0], [1,1,0], [0,1,0],\n",
    "                     [0,0,1], [1,0,1], [1,1,1], [0,1,1]])\n",
    "for i in range(4):\n",
    "    ax1.plot3D(*zip(vertices[i], vertices[(i+1)%4]), 'b-')\n",
    "    ax1.plot3D(*zip(vertices[i+4], vertices[(i+1)%4+4]), 'b-')\n",
    "    ax1.plot3D(*zip(vertices[i], vertices[i+4]), 'b-')\n",
    "ax1.set_title('Mesh')\n",
    "\n",
    "# Voxels\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "voxels = np.random.rand(5, 5, 5) > 0.7\n",
    "ax2.voxels(voxels, alpha=0.5)\n",
    "ax2.set_title('Voxels')\n",
    "\n",
    "# NeRF concept (continuous)\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "u = np.linspace(0, 2*np.pi, 50)\n",
    "v = np.linspace(0, np.pi, 25)\n",
    "x = np.outer(np.cos(u), np.sin(v))\n",
    "y = np.outer(np.sin(u), np.sin(v))\n",
    "z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "ax3.plot_surface(x, y, z, alpha=0.7, cmap='viridis')\n",
    "ax3.set_title('NeRF (Continuous)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"NeRF represents scenes as continuous functions, enabling:\")\n",
    "print(\"- Arbitrary resolution rendering\")\n",
    "print(\"- Smooth interpolation between views\")\n",
    "print(\"- Compact representation (just network weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding\n",
    "\n",
    "MLPs struggle to learn high-frequency functions. **Positional encoding** maps inputs to higher dimensions using sinusoidal functions:\n",
    "\n",
    "$$\\gamma(p) = [\\sin(2^0\\pi p), \\cos(2^0\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p)]$$\n",
    "\n",
    "This is the key insight that makes NeRF work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier feature positional encoding.\n",
    "    Maps low-dimensional input to higher dimensions using sinusoids.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_frequencies: int = 10, include_input: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.include_input = include_input\n",
    "        \n",
    "        # Frequency bands: 2^0, 2^1, ..., 2^(L-1)\n",
    "        self.register_buffer(\n",
    "            'frequency_bands',\n",
    "            2.0 ** torch.linspace(0, num_frequencies - 1, num_frequencies)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [..., input_dim]\n",
    "        Returns: [..., input_dim * (2 * num_frequencies + include_input)]\n",
    "        \"\"\"\n",
    "        encodings = []\n",
    "        \n",
    "        if self.include_input:\n",
    "            encodings.append(x)\n",
    "        \n",
    "        for freq in self.frequency_bands:\n",
    "            encodings.append(torch.sin(freq * np.pi * x))\n",
    "            encodings.append(torch.cos(freq * np.pi * x))\n",
    "        \n",
    "        return torch.cat(encodings, dim=-1)\n",
    "    \n",
    "    def output_dim(self, input_dim: int) -> int:\n",
    "        return input_dim * (2 * self.num_frequencies + int(self.include_input))\n",
    "\n",
    "\n",
    "# Demonstrate why positional encoding helps\n",
    "def visualize_positional_encoding():\n",
    "    pe = PositionalEncoding(num_frequencies=6)\n",
    "    \n",
    "    # 1D input\n",
    "    x = torch.linspace(-1, 1, 200).unsqueeze(-1)\n",
    "    encoded = pe(x)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Original input\n",
    "    axes[0].plot(x.numpy())\n",
    "    axes[0].set_title('Original Input (1D)')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    \n",
    "    # Encoded (show first few dimensions)\n",
    "    for i in range(min(8, encoded.shape[-1])):\n",
    "        axes[1].plot(encoded[:, i].numpy(), alpha=0.7, label=f'dim {i}')\n",
    "    axes[1].set_title('Positional Encoding (First 8 Dimensions)')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Input dimension: 1\")\n",
    "    print(f\"Output dimension: {encoded.shape[-1]}\")\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NeRF MLP Architecture\n",
    "\n",
    "The NeRF network architecture:\n",
    "1. Encode position $(x, y, z)$ with positional encoding\n",
    "2. Pass through MLP layers\n",
    "3. Output density $\\sigma$\n",
    "4. Concatenate with encoded viewing direction\n",
    "5. Output color $(r, g, b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Radiance Field network.\n",
    "    \n",
    "    Input: 3D position (x, y, z) and viewing direction (theta, phi)\n",
    "    Output: RGB color and volume density (sigma)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_encoding_freqs: int = 10,\n",
    "        dir_encoding_freqs: int = 4,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 8,\n",
    "        skip_connection: int = 4,  # Layer to add skip connection\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(pos_encoding_freqs)\n",
    "        self.dir_encoder = PositionalEncoding(dir_encoding_freqs)\n",
    "        \n",
    "        pos_dim = self.pos_encoder.output_dim(3)  # 3D position\n",
    "        dir_dim = self.dir_encoder.output_dim(3)  # 3D direction (unit vector)\n",
    "        \n",
    "        self.skip_connection = skip_connection\n",
    "        \n",
    "        # Position-dependent layers\n",
    "        self.pos_layers = nn.ModuleList()\n",
    "        self.pos_layers.append(nn.Linear(pos_dim, hidden_dim))\n",
    "        \n",
    "        for i in range(1, num_layers):\n",
    "            if i == skip_connection:\n",
    "                # Skip connection: concat original positional encoding\n",
    "                self.pos_layers.append(nn.Linear(hidden_dim + pos_dim, hidden_dim))\n",
    "            else:\n",
    "                self.pos_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Density output (no view dependence)\n",
    "        self.density_layer = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Feature vector for color prediction\n",
    "        self.feature_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # View-dependent color layers\n",
    "        self.color_layer1 = nn.Linear(hidden_dim + dir_dim, hidden_dim // 2)\n",
    "        self.color_layer2 = nn.Linear(hidden_dim // 2, 3)\n",
    "        \n",
    "        self.pos_dim = pos_dim\n",
    "    \n",
    "    def forward(self, pos: torch.Tensor, direction: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        pos: [..., 3] - 3D positions\n",
    "        direction: [..., 3] - viewing directions (unit vectors)\n",
    "        \n",
    "        Returns:\n",
    "            rgb: [..., 3] - colors\n",
    "            sigma: [..., 1] - densities\n",
    "        \"\"\"\n",
    "        # Encode inputs\n",
    "        pos_encoded = self.pos_encoder(pos)\n",
    "        dir_encoded = self.dir_encoder(direction)\n",
    "        \n",
    "        # Position network with skip connection\n",
    "        h = pos_encoded\n",
    "        for i, layer in enumerate(self.pos_layers):\n",
    "            if i == self.skip_connection:\n",
    "                h = torch.cat([h, pos_encoded], dim=-1)\n",
    "            h = F.relu(layer(h))\n",
    "        \n",
    "        # Density (ReLU ensures non-negative)\n",
    "        sigma = F.relu(self.density_layer(h))\n",
    "        \n",
    "        # Feature for color\n",
    "        feature = self.feature_layer(h)\n",
    "        \n",
    "        # View-dependent color\n",
    "        h = torch.cat([feature, dir_encoded], dim=-1)\n",
    "        h = F.relu(self.color_layer1(h))\n",
    "        rgb = torch.sigmoid(self.color_layer2(h))  # [0, 1] for colors\n",
    "        \n",
    "        return rgb, sigma\n",
    "\n",
    "\n",
    "# Test the network\n",
    "nerf = NeRF().to(device)\n",
    "test_pos = torch.randn(100, 3).to(device)  # 100 random 3D positions\n",
    "test_dir = F.normalize(torch.randn(100, 3), dim=-1).to(device)  # Unit directions\n",
    "\n",
    "rgb, sigma = nerf(test_pos, test_dir)\n",
    "print(f\"Input position shape: {test_pos.shape}\")\n",
    "print(f\"Input direction shape: {test_dir.shape}\")\n",
    "print(f\"Output RGB shape: {rgb.shape}\")\n",
    "print(f\"Output sigma shape: {sigma.shape}\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in nerf.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Volume Rendering\n",
    "\n",
    "To render an image, we cast rays through each pixel and integrate color along the ray:\n",
    "\n",
    "$$C(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\cdot \\sigma(\\mathbf{r}(t)) \\cdot \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) \\, dt$$\n",
    "\n",
    "Where:\n",
    "- $T(t) = \\exp(-\\int_{t_n}^{t} \\sigma(\\mathbf{r}(s)) \\, ds)$ is transmittance\n",
    "- $\\sigma$ is density\n",
    "- $\\mathbf{c}$ is color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_rendering(\n",
    "    rgb: torch.Tensor,\n",
    "    sigma: torch.Tensor,\n",
    "    z_vals: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    white_background: bool = False,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Volume rendering equation.\n",
    "    \n",
    "    Args:\n",
    "        rgb: [batch, num_samples, 3] - colors at sample points\n",
    "        sigma: [batch, num_samples, 1] - densities at sample points\n",
    "        z_vals: [batch, num_samples] - depth values along rays\n",
    "        rays_d: [batch, 3] - ray directions\n",
    "        white_background: whether to use white background\n",
    "    \n",
    "    Returns:\n",
    "        rgb_map: [batch, 3] - rendered colors\n",
    "        depth_map: [batch] - rendered depths\n",
    "        weights: [batch, num_samples] - integration weights\n",
    "    \"\"\"\n",
    "    # Compute distances between adjacent samples\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, torch.full_like(dists[..., :1], 1e10)], dim=-1)\n",
    "    \n",
    "    # Multiply by ray direction magnitude for actual distance\n",
    "    dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "    \n",
    "    # Alpha values: 1 - exp(-sigma * delta)\n",
    "    alpha = 1.0 - torch.exp(-sigma.squeeze(-1) * dists)\n",
    "    \n",
    "    # Transmittance: cumulative product of (1 - alpha)\n",
    "    # T_i = prod_{j=1}^{i-1} (1 - alpha_j)\n",
    "    transmittance = torch.cumprod(\n",
    "        torch.cat([torch.ones_like(alpha[..., :1]), 1.0 - alpha + 1e-10], dim=-1),\n",
    "        dim=-1\n",
    "    )[..., :-1]\n",
    "    \n",
    "    # Weights for each sample\n",
    "    weights = alpha * transmittance\n",
    "    \n",
    "    # Rendered color: weighted sum\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "    \n",
    "    # Rendered depth: weighted sum of z values\n",
    "    depth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "    \n",
    "    # Handle background\n",
    "    if white_background:\n",
    "        acc_map = torch.sum(weights, dim=-1)\n",
    "        rgb_map = rgb_map + (1.0 - acc_map[..., None])\n",
    "    \n",
    "    return rgb_map, depth_map, weights\n",
    "\n",
    "\n",
    "# Demonstrate volume rendering\n",
    "print(\"Volume Rendering Steps:\")\n",
    "print(\"1. Sample points along each ray\")\n",
    "print(\"2. Query NeRF for (rgb, sigma) at each point\")\n",
    "print(\"3. Compute alpha = 1 - exp(-sigma * delta)\")\n",
    "print(\"4. Compute transmittance T = cumulative product of (1 - alpha)\")\n",
    "print(\"5. Compute weights w = alpha * T\")\n",
    "print(\"6. Final color = sum(w * rgb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ray Generation\n",
    "\n",
    "To render from a camera, we need to generate rays for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal: float,\n",
    "    camera_to_world: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate rays for each pixel.\n",
    "    \n",
    "    Args:\n",
    "        height, width: image dimensions\n",
    "        focal: focal length\n",
    "        camera_to_world: [4, 4] camera pose matrix\n",
    "    \n",
    "    Returns:\n",
    "        rays_o: [H, W, 3] - ray origins\n",
    "        rays_d: [H, W, 3] - ray directions\n",
    "    \"\"\"\n",
    "    # Create pixel coordinates\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(width, dtype=torch.float32),\n",
    "        torch.arange(height, dtype=torch.float32),\n",
    "        indexing='xy'\n",
    "    )\n",
    "    \n",
    "    # Convert to camera coordinates (pinhole camera model)\n",
    "    # Normalized device coordinates: center at (0, 0)\n",
    "    directions = torch.stack([\n",
    "        (i - width / 2) / focal,\n",
    "        -(j - height / 2) / focal,  # Flip y\n",
    "        -torch.ones_like(i),  # Looking down -z axis\n",
    "    ], dim=-1)\n",
    "    \n",
    "    # Transform to world coordinates\n",
    "    rays_d = torch.sum(\n",
    "        directions[..., None, :] * camera_to_world[:3, :3],\n",
    "        dim=-1\n",
    "    )\n",
    "    \n",
    "    # Ray origins = camera position\n",
    "    rays_o = camera_to_world[:3, 3].expand(rays_d.shape)\n",
    "    \n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_along_rays(\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    near: float,\n",
    "    far: float,\n",
    "    num_samples: int,\n",
    "    perturb: bool = True,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample points along rays.\n",
    "    \n",
    "    Args:\n",
    "        rays_o: [..., 3] - ray origins\n",
    "        rays_d: [..., 3] - ray directions\n",
    "        near, far: near and far bounds\n",
    "        num_samples: number of samples per ray\n",
    "        perturb: whether to add noise to sample positions\n",
    "    \n",
    "    Returns:\n",
    "        pts: [..., num_samples, 3] - sample positions\n",
    "        z_vals: [..., num_samples] - depth values\n",
    "    \"\"\"\n",
    "    # Uniform samples between near and far\n",
    "    t_vals = torch.linspace(0.0, 1.0, num_samples, device=rays_o.device)\n",
    "    z_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "    \n",
    "    # Expand to match ray batch shape\n",
    "    z_vals = z_vals.expand(list(rays_o.shape[:-1]) + [num_samples])\n",
    "    \n",
    "    # Perturb samples (stratified sampling)\n",
    "    if perturb:\n",
    "        mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        upper = torch.cat([mids, z_vals[..., -1:]], dim=-1)\n",
    "        lower = torch.cat([z_vals[..., :1], mids], dim=-1)\n",
    "        t_rand = torch.rand_like(z_vals)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "    \n",
    "    # Compute 3D sample positions: o + t * d\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "    \n",
    "    return pts, z_vals\n",
    "\n",
    "\n",
    "# Visualize ray sampling\n",
    "camera_pose = torch.eye(4)\n",
    "camera_pose[2, 3] = 4.0  # Camera at z=4\n",
    "\n",
    "rays_o, rays_d = get_rays(height=4, width=4, focal=2.0, camera_to_world=camera_pose)\n",
    "pts, z_vals = sample_along_rays(rays_o, rays_d, near=2.0, far=6.0, num_samples=8)\n",
    "\n",
    "print(f\"Rays origin shape: {rays_o.shape}\")\n",
    "print(f\"Rays direction shape: {rays_d.shape}\")\n",
    "print(f\"Sample points shape: {pts.shape}\")\n",
    "print(f\"Z values shape: {z_vals.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rendering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(\n",
    "    model: NeRF,\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    near: float,\n",
    "    far: float,\n",
    "    num_samples: int = 64,\n",
    "    perturb: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Render colors for a batch of rays.\n",
    "    \n",
    "    Args:\n",
    "        model: NeRF network\n",
    "        rays_o: [batch, 3] ray origins\n",
    "        rays_d: [batch, 3] ray directions\n",
    "        near, far: rendering bounds\n",
    "        num_samples: samples per ray\n",
    "        perturb: stratified sampling\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with rgb_map, depth_map, etc.\n",
    "    \"\"\"\n",
    "    # Sample points along rays\n",
    "    pts, z_vals = sample_along_rays(rays_o, rays_d, near, far, num_samples, perturb)\n",
    "    \n",
    "    # Flatten for network input\n",
    "    batch_size = rays_o.shape[0]\n",
    "    pts_flat = pts.reshape(-1, 3)\n",
    "    \n",
    "    # Viewing directions (same for all points on a ray)\n",
    "    dirs = F.normalize(rays_d, dim=-1)\n",
    "    dirs_flat = dirs[:, None, :].expand(-1, num_samples, -1).reshape(-1, 3)\n",
    "    \n",
    "    # Query network\n",
    "    rgb, sigma = model(pts_flat, dirs_flat)\n",
    "    \n",
    "    # Reshape\n",
    "    rgb = rgb.reshape(batch_size, num_samples, 3)\n",
    "    sigma = sigma.reshape(batch_size, num_samples, 1)\n",
    "    \n",
    "    # Volume rendering\n",
    "    rgb_map, depth_map, weights = volume_rendering(rgb, sigma, z_vals, rays_d)\n",
    "    \n",
    "    return {\n",
    "        'rgb_map': rgb_map,\n",
    "        'depth_map': depth_map,\n",
    "        'weights': weights,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test rendering\n",
    "nerf = NeRF().to(device)\n",
    "test_rays_o = torch.randn(16, 3).to(device)\n",
    "test_rays_d = F.normalize(torch.randn(16, 3), dim=-1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = render_rays(nerf, test_rays_o, test_rays_d, near=2.0, far=6.0)\n",
    "\n",
    "print(f\"Rendered RGB shape: {result['rgb_map'].shape}\")\n",
    "print(f\"Rendered depth shape: {result['depth_map'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical Sampling\n",
    "\n",
    "NeRF uses a **coarse-to-fine** strategy:\n",
    "1. **Coarse network**: Uniform sampling to identify important regions\n",
    "2. **Fine network**: Importance sampling in high-density regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(\n",
    "    bins: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    num_samples: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse transform sampling from a piecewise-constant PDF.\n",
    "    \n",
    "    Args:\n",
    "        bins: [batch, num_bins + 1] - bin edges\n",
    "        weights: [batch, num_bins] - weights for each bin\n",
    "        num_samples: number of samples to draw\n",
    "    \n",
    "    Returns:\n",
    "        samples: [batch, num_samples] - sampled positions\n",
    "    \"\"\"\n",
    "    # Normalize weights to get PDF\n",
    "    weights = weights + 1e-5  # Prevent division by zero\n",
    "    pdf = weights / torch.sum(weights, dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute CDF\n",
    "    cdf = torch.cumsum(pdf, dim=-1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n",
    "    \n",
    "    # Sample uniform values\n",
    "    u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=bins.device)\n",
    "    u = u.contiguous()\n",
    "    \n",
    "    # Invert CDF\n",
    "    inds = torch.searchsorted(cdf, u, right=True)\n",
    "    below = torch.clamp(inds - 1, min=0)\n",
    "    above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "    \n",
    "    # Gather CDF values\n",
    "    cdf_below = torch.gather(cdf, -1, below)\n",
    "    cdf_above = torch.gather(cdf, -1, above)\n",
    "    \n",
    "    # Gather bin edges\n",
    "    bins_below = torch.gather(bins, -1, below)\n",
    "    bins_above = torch.gather(bins, -1, above)\n",
    "    \n",
    "    # Linear interpolation\n",
    "    denom = cdf_above - cdf_below\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u - cdf_below) / denom\n",
    "    samples = bins_below + t * (bins_above - bins_below)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "print(\"Hierarchical Sampling Strategy:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Coarse pass: 64 uniform samples along ray\")\n",
    "print(\"2. Evaluate coarse network -> get weights\")\n",
    "print(\"3. Use weights as PDF for importance sampling\")\n",
    "print(\"4. Fine pass: 128 additional samples (importance sampled)\")\n",
    "print(\"5. Evaluate fine network with all samples\")\n",
    "print(\"\\nThis focuses compute on regions with high density!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modern Improvements: Hash Encoding (Instant-NGP)\n",
    "\n",
    "Hash encoding enables much faster training by using a multi-resolution hash table instead of deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified multi-resolution hash encoding (Instant-NGP style).\n",
    "    Maps 3D positions to features using hash tables at multiple resolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_levels: int = 16,\n",
    "        features_per_level: int = 2,\n",
    "        log2_hashmap_size: int = 19,\n",
    "        base_resolution: int = 16,\n",
    "        finest_resolution: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_levels = num_levels\n",
    "        self.features_per_level = features_per_level\n",
    "        self.hashmap_size = 2 ** log2_hashmap_size\n",
    "        \n",
    "        # Compute resolution at each level (geometric progression)\n",
    "        b = np.exp((np.log(finest_resolution) - np.log(base_resolution)) / (num_levels - 1))\n",
    "        self.resolutions = [int(base_resolution * (b ** i)) for i in range(num_levels)]\n",
    "        \n",
    "        # Hash tables for each level\n",
    "        self.hash_tables = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(self.hashmap_size, features_per_level) * 0.01)\n",
    "            for _ in range(num_levels)\n",
    "        ])\n",
    "        \n",
    "        # Large primes for hashing\n",
    "        self.primes = torch.tensor([1, 2654435761, 805459861], dtype=torch.long)\n",
    "    \n",
    "    def hash_function(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Spatial hash function.\"\"\"\n",
    "        # XOR hashing with prime numbers\n",
    "        result = torch.zeros(coords.shape[0], dtype=torch.long, device=coords.device)\n",
    "        for i in range(3):\n",
    "            result = result ^ (coords[:, i].long() * self.primes[i].to(coords.device))\n",
    "        return result % self.hashmap_size\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, 3] - 3D positions in [0, 1]^3\n",
    "        Returns: [batch, num_levels * features_per_level]\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for level, (resolution, hash_table) in enumerate(zip(self.resolutions, self.hash_tables)):\n",
    "            # Scale position to grid resolution\n",
    "            scaled = x * resolution\n",
    "            \n",
    "            # Get integer grid coordinates (for simplicity, just floor)\n",
    "            coords = torch.floor(scaled).long()\n",
    "            coords = torch.clamp(coords, 0, resolution - 1)\n",
    "            \n",
    "            # Hash and lookup\n",
    "            indices = self.hash_function(coords)\n",
    "            level_features = hash_table[indices]\n",
    "            features.append(level_features)\n",
    "        \n",
    "        return torch.cat(features, dim=-1)\n",
    "    \n",
    "    def output_dim(self) -> int:\n",
    "        return self.num_levels * self.features_per_level\n",
    "\n",
    "\n",
    "# Compare encodings\n",
    "pos_enc = PositionalEncoding(num_frequencies=10)\n",
    "hash_enc = HashEncoding(num_levels=16, features_per_level=2)\n",
    "\n",
    "test_input = torch.rand(1000, 3)  # Random 3D positions\n",
    "\n",
    "pos_output = pos_enc(test_input)\n",
    "hash_output = hash_enc(test_input)\n",
    "\n",
    "print(\"Encoding Comparison:\")\n",
    "print(f\"  Positional encoding output dim: {pos_output.shape[-1]}\")\n",
    "print(f\"  Hash encoding output dim: {hash_output.shape[-1]}\")\n",
    "print(f\"\\nHash encoding advantages:\")\n",
    "print(\"  - Learnable features (adapts to scene)\")\n",
    "print(\"  - Multi-resolution (coarse + fine details)\")\n",
    "print(\"  - Much faster training (minutes vs hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. FAANG Interview Questions\n",
    "\n",
    "### Q1: What is NeRF and how does it represent 3D scenes?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "NeRF (Neural Radiance Field) represents a 3D scene as a continuous function implemented by an MLP:\n",
    "\n",
    "$$F: (x, y, z, \\theta, \\phi) \\rightarrow (r, g, b, \\sigma)$$\n",
    "\n",
    "**Inputs**:\n",
    "- $(x, y, z)$: 3D position\n",
    "- $(\\theta, \\phi)$: Viewing direction\n",
    "\n",
    "**Outputs**:\n",
    "- $(r, g, b)$: View-dependent color (captures specular effects)\n",
    "- $\\sigma$: View-independent density (geometry)\n",
    "\n",
    "**Key insight**: Density depends only on position (geometry is view-independent), but color depends on viewing direction (specular highlights vary with viewpoint).\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Why is positional encoding important for NeRF?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Problem**: MLPs are biased toward learning low-frequency functions. Without positional encoding, NeRF produces blurry results.\n",
    "\n",
    "**Solution**: Map inputs to higher dimensions using sinusoids:\n",
    "$$\\gamma(p) = [\\sin(2^0\\pi p), \\cos(2^0\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p)]$$\n",
    "\n",
    "**Why it works**:\n",
    "- Sinusoids at different frequencies capture different detail levels\n",
    "- Network can now represent high-frequency variations\n",
    "- Similar to Fourier features / random Fourier features\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: Explain volume rendering in NeRF.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "Volume rendering integrates color along each ray:\n",
    "\n",
    "$$C(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\cdot \\sigma(t) \\cdot \\mathbf{c}(t) \\, dt$$\n",
    "\n",
    "**Components**:\n",
    "- $T(t)$: Transmittance (probability ray hasn't hit anything yet)\n",
    "- $\\sigma(t)$: Density at point $t$\n",
    "- $\\mathbf{c}(t)$: Color at point $t$\n",
    "\n",
    "**Discrete approximation**:\n",
    "1. Sample N points along ray\n",
    "2. Compute $\\alpha_i = 1 - \\exp(-\\sigma_i \\cdot \\delta_i)$\n",
    "3. Compute $T_i = \\prod_{j<i}(1-\\alpha_j)$\n",
    "4. Compute weights $w_i = \\alpha_i \\cdot T_i$\n",
    "5. Final color = $\\sum_i w_i \\cdot c_i$\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: What is hierarchical sampling and why is it used?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Problem**: Uniform sampling wastes compute on empty space.\n",
    "\n",
    "**Solution**: Two-pass sampling:\n",
    "\n",
    "1. **Coarse pass**: \n",
    "   - Uniform samples (e.g., 64)\n",
    "   - Evaluate coarse network\n",
    "   - Get density weights\n",
    "\n",
    "2. **Fine pass**:\n",
    "   - Use weights as PDF\n",
    "   - Importance sample additional points (e.g., 128)\n",
    "   - Sample more in high-density regions\n",
    "   - Evaluate fine network\n",
    "\n",
    "**Benefits**:\n",
    "- Focus compute on surfaces/objects\n",
    "- Better quality with same sample count\n",
    "- Or same quality with fewer samples\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: How does Instant-NGP improve on original NeRF?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Original NeRF problems**:\n",
    "- Slow training (hours/days)\n",
    "- Deep networks needed\n",
    "- Fixed positional encoding\n",
    "\n",
    "**Instant-NGP improvements**:\n",
    "\n",
    "1. **Hash encoding**:\n",
    "   - Multi-resolution hash tables instead of sinusoids\n",
    "   - Learnable features (adapt to scene)\n",
    "   - O(1) lookup instead of network forward pass\n",
    "\n",
    "2. **Smaller network**:\n",
    "   - Hash features are more expressive\n",
    "   - Only need 2-3 layer MLP\n",
    "\n",
    "3. **Fully-fused CUDA kernels**:\n",
    "   - Optimized GPU implementation\n",
    "\n",
    "**Result**: Training in seconds/minutes instead of hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "1. **NeRF** represents 3D scenes as continuous neural functions: (x,y,z,dir) -> (rgb, sigma)\n",
    "2. **Positional encoding** enables MLPs to learn high-frequency details\n",
    "3. **Volume rendering** integrates color along rays using density-weighted sums\n",
    "4. **Hierarchical sampling** focuses compute on important regions\n",
    "5. **View-dependent color** enables specular/reflective effects\n",
    "6. **Hash encoding** (Instant-NGP) dramatically speeds up training\n",
    "7. NeRF spawned many variants: mip-NeRF, NeRF-W, D-NeRF (dynamic), etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
