{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Performance Engineering (Triton & Profiling)\n",
    "\n",
    "In FAANG, making a model 10% faster can save millions of dollars. This chapter moves beyond \"making it work\" to \"making it fast\".\n",
    "\n",
    "## Learning Objectives\n",
    "- **Profile** your code to find bottlenecks.\n",
    "- Use **`torch.compile`** for free speedups.\n",
    "- Write a custom GPU kernel using **Triton**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "- **Latency**: Time per request.\n",
    "- **Throughput**: Requests per second.\n",
    "- **Kernel**: A function that runs on the GPU.\n",
    "- **Fusion**: Combining multiple operations (Add + Multiply) into one kernel to save memory bandwidth.\n",
    "- **Triton**: A language from OpenAI to write GPU kernels in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Profiling with `torch.profiler`\n",
    "\n",
    "Stop guessing where your code is slow. Measure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_computation(x):\n",
    "    return torch.matmul(x, x) + torch.relu(x)\n",
    "\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    heavy_computation(x)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `torch.compile` (PyTorch 2.0)\n",
    "\n",
    "The easiest way to speed up PyTorch code. It fuses operations automatically.\n",
    "\n",
    "```python\n",
    "model = MyModel()\n",
    "opt_model = torch.compile(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def fast_computation(x):\n",
    "    return torch.sin(x) + torch.cos(x)\n",
    "\n",
    "# First run compiles (might be slow)\n",
    "start = time.time()\n",
    "fast_computation(x)\n",
    "print(f\"First run (compilation): {time.time() - start:.4f}s\")\n",
    "\n",
    "# Second run is fast\n",
    "start = time.time()\n",
    "fast_computation(x)\n",
    "print(f\"Second run (cached): {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Custom Kernels with Triton\n",
    "\n",
    "When `torch.compile` isn't enough, you write your own kernels. Triton makes this accessible to Python engineers.\n",
    "\n",
    "*(Note: This requires a GPU to run)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "def triton_add(x: torch.Tensor, y: torch.Tensor):\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     x = torch.randn(1000, device='cuda')\n",
    "#     y = torch.randn(1000, device='cuda')\n",
    "#     out = triton_add(x, y)\n",
    "#     print(\"Triton add successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Profile first**: Don't optimize blindly.\n",
    "2. **Use `torch.compile`**: It's free speed.\n",
    "3. **Triton**: The secret weapon for custom high-performance layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
