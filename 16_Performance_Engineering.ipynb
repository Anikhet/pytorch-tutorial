{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Performance Engineering (Triton & Profiling)\n",
    "\n",
    "In FAANG, making a model 10% faster can save millions of dollars. This chapter moves beyond \"making it work\" to \"making it fast\".\n",
    "\n",
    "## Learning Objectives\n",
    "- **Profile** your code to find bottlenecks.\n",
    "- Use **`torch.compile`** for free speedups.\n",
    "- Write a custom GPU kernel using **Triton**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Vocabulary First\n\n- **Latency**: Time per request. How long a single inference takes.\n- **Throughput**: Requests per second. How many inferences you can do in parallel.\n- **Kernel**: A function that runs on the GPU. Every PyTorch operation (matmul, relu, add) launches one or more GPU kernels.\n- **Fusion**: Combining multiple operations (Add + Multiply) into one kernel to save memory bandwidth.\n- **Triton**: A language from OpenAI to write GPU kernels in Python (instead of CUDA C++).\n\n### Compute-Bound vs Memory-Bound (The Most Important Concept)\n\nEvery GPU operation is bottlenecked by one of two things:\n\n**Compute-bound**: The GPU's math units (FLOPS) are the bottleneck.\n- Large matrix multiplications (e.g., `torch.matmul` on big tensors)\n- Convolutions with many channels\n- The GPU cores are doing math as fast as they can\n\n**Memory-bound**: Moving data between GPU memory (HBM) and compute cores is the bottleneck.\n- Element-wise operations (ReLU, Add, LayerNorm)\n- Small matrix operations\n- The compute cores are **idle**, waiting for data to arrive\n\n**Why this matters**: Most PyTorch operations are memory-bound. The GPU can compute faster than it can read data. This is why **operator fusion** is so powerful — it eliminates unnecessary reads/writes.\n\n### The Roofline Model (How to Think About Performance)\n\n```\nThroughput\n    │\n    │         ╱ Compute ceiling (max FLOPS)\n    │        ╱\n    │       ╱───────────────── Compute-bound region\n    │      ╱\n    │     ╱\n    │    ╱  Memory-bound region\n    │   ╱\n    │──╱───────────────────────\n    └────────────────────────── Arithmetic Intensity (FLOPS/byte)\n```\n\n- **Left side**: Operations with low arithmetic intensity are memory-bound\n- **Right side**: Operations with high arithmetic intensity are compute-bound\n- **Goal**: Move operations to the right (higher arithmetic intensity) via fusion\n\n### What is Operator Fusion?\n\nWithout fusion (3 separate kernel launches, 3 memory round-trips):\n```\nRead X from HBM → Compute matmul → Write Y to HBM\nRead Y from HBM → Compute ReLU  → Write Z to HBM\nRead Z from HBM → Compute Add   → Write W to HBM\n```\n\nWith fusion (1 kernel launch, 1 memory round-trip):\n```\nRead X from HBM → Compute (matmul + ReLU + Add) → Write W to HBM\n```\n\nThis is exactly what `torch.compile` does automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Profiling with `torch.profiler`\n\nStop guessing where your code is slow. Measure it.\n\n### How to Read Profiler Output\n\nThe profiler shows you:\n- **Self CPU time**: Time spent in the operation itself (not counting sub-operations)\n- **CPU total**: Total time including sub-operations\n- **CUDA time**: Time spent on GPU (if applicable)\n- **# Calls**: How many times the operation was called\n\n**Reading strategy**: Sort by `cpu_time_total` or `cuda_time_total`. The top entries are your bottlenecks. If an operation shows high CPU time but low CUDA time, you may have a CPU bottleneck (data preprocessing, Python overhead). If CUDA time dominates, optimize the GPU operations.\n\n### Common Bottlenecks You'll Find\n\n1. **Data loading**: CPU can't feed data fast enough. Fix: increase `num_workers` in DataLoader.\n2. **CPU-GPU sync**: Operations like `.item()`, `print(tensor)`, or `if tensor > 0` force the GPU to wait. Fix: batch these operations.\n3. **Small kernel launches**: Many tiny GPU operations. Fix: use `torch.compile` for fusion.\n4. **Memory copies**: Frequent `.to(device)` calls. Fix: move data to GPU once, keep it there."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_computation(x):\n",
    "    return torch.matmul(x, x) + torch.relu(x)\n",
    "\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    heavy_computation(x)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. `torch.compile` (PyTorch 2.0)\n\nThe easiest way to speed up PyTorch code. It fuses operations automatically.\n\n### How `torch.compile` Works Under the Hood\n\n1. **Tracing**: PyTorch captures the computation graph (what operations are called and in what order).\n2. **Graph optimization**: The compiler identifies fusible operations and eliminates redundant memory accesses.\n3. **Code generation**: It generates optimized Triton kernels (or uses pre-existing ones) for the fused operations.\n4. **Caching**: The compiled code is cached, so subsequent calls are fast.\n\n### Compilation Modes\n\n```python\n# Default: Good balance of compile time and speed\nmodel = torch.compile(model)\n\n# Maximum optimization (slower compile, faster execution)\nmodel = torch.compile(model, mode=\"max-autotune\")\n\n# Fastest compile time (less optimization)\nmodel = torch.compile(model, mode=\"reduce-overhead\")\n```\n\n### When `torch.compile` Doesn't Help\n\n- **Dynamic shapes**: If tensor sizes change every iteration, the compiler recompiles each time (slow). Use `dynamic=True` to mitigate.\n- **Data-dependent control flow**: `if x.sum() > 0` breaks the graph because the compiler can't predict the branch at compile time.\n- **Custom C extensions**: The compiler can only optimize pure PyTorch operations.\n\n### Typical Speedups\n\n| Workload | Speedup with `torch.compile` |\n|----------|------|\n| Transformer training | 1.3-1.5x |\n| CNN inference | 1.2-1.4x |\n| Element-wise ops (memory-bound) | 2-3x |\n| Already-optimized code (cuBLAS matmuls) | ~1x (no gain) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def fast_computation(x):\n",
    "    return torch.sin(x) + torch.cos(x)\n",
    "\n",
    "# First run compiles (might be slow)\n",
    "start = time.time()\n",
    "fast_computation(x)\n",
    "print(f\"First run (compilation): {time.time() - start:.4f}s\")\n",
    "\n",
    "# Second run is fast\n",
    "start = time.time()\n",
    "fast_computation(x)\n",
    "print(f\"Second run (cached): {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Writing Custom Kernels with Triton\n\nWhen `torch.compile` isn't enough, you write your own kernels. Triton makes this accessible to Python engineers.\n\n### Why Triton Over CUDA?\n\n| Aspect | CUDA C++ | Triton (Python) |\n|--------|----------|-----------------|\n| Language | C++ | Python |\n| Learning curve | Months | Days |\n| Memory management | Manual (shared memory, tiling) | Automatic |\n| Performance | Maximum (hand-tuned) | 90-95% of CUDA |\n| Portability | NVIDIA only | NVIDIA (AMD support improving) |\n| Used by | NVIDIA engineers | ML researchers, PyTorch team |\n\n**Key insight**: Triton auto-tunes memory tiling and shared memory usage — the hardest parts of GPU programming. You focus on the algorithm; Triton handles the hardware details.\n\n### Flash Attention (The Most Important Custom Kernel)\n\nStandard attention computes:\n```\nQ, K, V are [batch, heads, seq_len, dim]\nAttention = softmax(Q @ K.T / sqrt(d)) @ V\n```\n\nThe problem: `Q @ K.T` creates an `[seq_len, seq_len]` matrix. For seq_len=8192, that's 256MB per head per batch — enormous memory usage.\n\n**Flash Attention** fuses the entire attention computation into one kernel:\n- Never materializes the full `[seq_len, seq_len]` attention matrix\n- Processes attention in tiles (blocks) that fit in fast SRAM\n- Reduces memory from O(N^2) to O(N)\n- 2-4x faster than standard attention\n\n```python\n# Using Flash Attention in PyTorch (built-in since 2.0)\nfrom torch.nn.functional import scaled_dot_product_attention\noutput = scaled_dot_product_attention(Q, K, V)  # Automatically uses Flash Attention\n```\n\n*(Note: The Triton kernel below requires a GPU to run)*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "def triton_add(x: torch.Tensor, y: torch.Tensor):\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     x = torch.randn(1000, device='cuda')\n",
    "#     y = torch.randn(1000, device='cuda')\n",
    "#     out = triton_add(x, y)\n",
    "#     print(\"Triton add successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Profile first**: Don't optimize blindly. Use `torch.profiler` to find actual bottlenecks.\n2. **Understand the bottleneck type**: Is it compute-bound or memory-bound? The fix is different for each.\n3. **Use `torch.compile`**: It's free speed for most workloads — automatic operator fusion without code changes.\n4. **Flash Attention**: The single most impactful optimization for Transformers. Use `scaled_dot_product_attention` in PyTorch 2.0+.\n5. **Triton**: The secret weapon for custom high-performance layers when you need to go beyond what `torch.compile` offers.\n\n### Performance Optimization Checklist\n\n```\n1. Profile your code (find the actual bottleneck)\n   ↓\n2. Is it data loading? → Increase num_workers, use pin_memory=True\n   ↓\n3. Is it many small GPU ops? → Use torch.compile for fusion\n   ↓\n4. Is it attention? → Use scaled_dot_product_attention (Flash Attention)\n   ↓\n5. Is it a custom operation? → Write a Triton kernel\n   ↓\n6. Still slow? → Consider mixed precision (torch.autocast), quantization, or better hardware\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}