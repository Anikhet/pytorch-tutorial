{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Graph Neural Networks (GNNs)\n",
    "\n",
    "Graphs are everywhere: Social Networks (Facebook), Knowledge Graphs (Google), and Molecule Structures (Drug Discovery). **Graph Neural Networks (GNNs)** are the state-of-the-art for learning on this non-Euclidean data.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **Graphs**: Nodes, Edges, and Adjacency Matrices.\n",
    "- Understand **Message Passing**: How nodes talk to neighbors.\n",
    "- Implement a **GCN (Graph Convolutional Network)** using `torch_geometric`.\n",
    "- Solve a **Link Prediction** task (Recommendation System)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Vocabulary First\n\n- **Node (Vertex)**: An entity (e.g., a User, a Product).\n- **Edge (Link)**: A connection (e.g., \"Friend of\", \"Bought\").\n- **Feature Matrix ($X$)**: Attributes of each node (e.g., Age, Location).\n- **Adjacency Matrix ($A$)**: A grid showing who is connected to whom.\n- **Message Passing**: Aggregating information from neighbors to update a node's embedding.\n\n### Why Graphs Matter at Scale\n\nGraphs are the natural data structure for relationships. Unlike images (grid) or text (sequence), graphs have **no fixed structure** — each node can have any number of neighbors.\n\n**Real-world applications at top tech companies:**\n- **Pinterest (PinSage)**: Recommends pins by learning embeddings of 3 billion nodes on a graph of pins and boards\n- **Google (Knowledge Graph)**: Powers search results by reasoning over entities and relationships\n- **Uber (Fraud Detection)**: Detects coordinated fraud rings by analyzing transaction graphs\n- **Drug Discovery (DeepMind)**: Predicts molecular properties by treating molecules as graphs (atoms = nodes, bonds = edges)\n- **Twitter/X**: Detects bot networks through graph structure analysis\n\n### The Message Passing Intuition\n\nThink of a party where you can only talk to people standing next to you:\n\n1. **Round 1**: You hear what your direct neighbors say (1-hop information)\n2. **Round 2**: Your neighbors relay what *their* neighbors said (2-hop information)\n3. **Round 3**: Information from 3 hops away reaches you\n\nEach GNN layer = one round of conversation. After L layers, each node has heard from neighbors up to L hops away.\n\n**The Over-Smoothing Problem**: If you stack too many layers (too many rounds of conversation), every node ends up with the same information — like a game of telephone where everyone converges to the same message. This is why most GNNs use only 2-4 layers, unlike deep CNNs with 100+ layers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "print(\"Ready for Graphs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Simple Graph\n",
    "\n",
    "Let's create a small social network with 3 people.\n",
    "- Node 0 is friends with Node 1.\n",
    "- Node 1 is friends with Node 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Index (COO format): [Source Nodes, Target Nodes]\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 1, 2],\n",
    "    [1, 0, 2, 1]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Node Features (e.g., Age, Activity Level) - 2 features per node\n",
    "x = torch.tensor([\n",
    "    [-1, 0], # Node 0\n",
    "    [ 0, 1], # Node 1\n",
    "    [ 1, 0]  # Node 2\n",
    "], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Graph Convolutional Network (GCN)\n\nA GCN layer updates a node's representation by averaging its neighbors' features.\n\n$$ h_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\frac{1}{c_{uv}} W^{(l)} h_u^{(l)} \\right) $$\n\n### GCN vs GAT vs GraphSAGE (The Three Major Architectures)\n\n| Architecture | How It Aggregates Neighbors | Strengths | Weaknesses |\n|-------------|---------------------------|-----------|------------|\n| **GCN** | Weighted average (fixed weights based on degree) | Simple, fast, good baseline | Treats all neighbors equally |\n| **GAT** (Graph Attention) | Learns attention weights per neighbor | Can prioritize important neighbors | More parameters, slower |\n| **GraphSAGE** | Samples a fixed number of neighbors, then aggregates | Scales to huge graphs (billions of nodes) | Sampling introduces noise |\n\n**GCN** is the simplest: it averages neighbor features weighted by node degree (popular nodes contribute less per connection). This is analogous to normalized averaging.\n\n**GAT** adds attention: \"Not all friends are equally important.\" It learns a weight for each edge that determines how much each neighbor contributes. Like Transformer self-attention, but on graph edges.\n\n**GraphSAGE** is built for scale: instead of aggregating ALL neighbors (impossible for a node with 1 million connections), it randomly samples a fixed number (e.g., 25) per layer. This makes it practical for production graphs with billions of nodes.\n\n### Transductive vs Inductive Learning\n\n- **Transductive** (GCN): Trained on a fixed graph. If a new node appears, you must retrain. Good for: static graphs (knowledge graphs, citation networks).\n- **Inductive** (GraphSAGE, GAT): Can generalize to unseen nodes. Good for: dynamic graphs where new users/items appear constantly (social networks, e-commerce)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(2, 16) # Input: 2 features -> Hidden: 16\n",
    "        self.conv2 = GCNConv(16, 2) # Hidden: 16 -> Output: 2 (Embedding)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Link Prediction (Recommendation)\n\nTo recommend a friend, we check if the dot product of two node embeddings is high.\n\n$$ Score(u, v) = h_u \\cdot h_v $$\n\n### Why Dot Product Works for Recommendations\n\nThe embedding space is learned so that **connected nodes are close together**. The dot product measures similarity in this space:\n- **High score**: Nodes are similar (likely to connect)\n- **Low score**: Nodes are dissimilar (unlikely to connect)\n\nThis is the same principle behind collaborative filtering in recommendation systems — but GNNs can also incorporate node features (content-based) alongside graph structure (collaborative), giving you the best of both worlds.\n\n### The Three Main GNN Tasks\n\n1. **Node Classification**: Predict a label for each node (e.g., \"Is this user a bot?\")\n2. **Link Prediction**: Predict whether an edge should exist (e.g., \"Should we recommend user A follow user B?\")\n3. **Graph Classification**: Predict a label for an entire graph (e.g., \"Is this molecule toxic?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get embeddings\n",
    "embeddings = model(data)\n",
    "\n",
    "# Predict link between Node 0 and Node 2 (who are NOT friends yet)\n",
    "node_0 = embeddings[0]\n",
    "node_2 = embeddings[2]\n",
    "\n",
    "score = torch.matmul(node_0, node_2)\n",
    "prob = torch.sigmoid(score)\n",
    "\n",
    "print(f\"Probability of connection between 0 and 2: {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Graphs** model relationships — they're the natural data structure for social networks, molecules, knowledge bases, and fraud detection.\n2. **GNNs** learn embeddings through message passing — each layer aggregates information from neighbors, building richer representations.\n3. **Over-smoothing limits depth** — unlike CNNs, GNNs work best with 2-4 layers. Too many layers cause all node embeddings to converge.\n4. **Architecture choice depends on scale**: GCN for simplicity, GAT for importance-weighted neighbors, GraphSAGE for billion-node production graphs.\n5. **Transductive vs Inductive**: If new nodes appear regularly (e-commerce, social media), use an inductive method like GraphSAGE.\n6. **Link Prediction** is the basis of modern recommendation systems — learned embeddings capture both content features and graph structure simultaneously."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}