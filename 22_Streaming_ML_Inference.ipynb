{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 22: Streaming ML Inference (Real-Time AI Systems)\n",
    "\n",
    "**The Challenge**: Your model needs to make predictions on **millions of events per second** as they happen - fraud detection, recommendation engines, content moderation, real-time bidding.\n",
    "\n",
    "**Traditional Approach**: Batch processing every few hours ‚ùå\n",
    "**Modern Approach**: Stream processing with sub-second latency ‚úÖ\n",
    "\n",
    "In 2025, streaming ML infrastructure is the standard for production systems at scale. This notebook teaches you:\n",
    "- How to build real-time inference pipelines with **Kafka + PyTorch**\n",
    "- What **feature stores** are and why they matter\n",
    "- How to prevent **training-serving skew** (the #1 production ML bug!)\n",
    "- Two streaming patterns: **Embedded** vs **Enricher**\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the **streaming ML architecture**\n",
    "2. Build a real-time inference system with **Apache Kafka**\n",
    "3. Integrate **feature stores** for consistent features\n",
    "4. Avoid **training-serving skew**\n",
    "5. Deploy streaming ML on **production infrastructure**\n",
    "\n",
    "---"
   ],
   "id": "cell_00"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vocabulary & Core Concepts\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Stream Processing**: Processing data **as it arrives** (not in batches)\n",
    "- **Apache Kafka**: Distributed event streaming platform (the de facto standard)\n",
    "- **Apache Flink**: Stream processing framework for real-time analytics\n",
    "- **Feature Store**: Centralized repository for ML features (online + offline)\n",
    "- **Training-Serving Skew**: When training and inference use different feature calculations\n",
    "- **Latency SLA**: Service Level Agreement (e.g., 95% of requests < 100ms)\n",
    "- **Event-Driven Architecture**: Systems that react to events rather than polling\n",
    "\n",
    "### Why Streaming ML?\n",
    "\n",
    "**Batch Inference Problems**:\n",
    "- ‚ùå Stale predictions (hours/days old)\n",
    "- ‚ùå Can't react to real-time events\n",
    "- ‚ùå Wastes compute on unchanged data\n",
    "\n",
    "**Streaming Inference Benefits**:\n",
    "- ‚úÖ Sub-second latency\n",
    "- ‚úÖ React to events immediately\n",
    "- ‚úÖ Only compute when needed\n",
    "- ‚úÖ Better user experience\n",
    "\n",
    "### Real-World Use Cases\n",
    "- **Fraud Detection**: Flag suspicious transactions in <50ms\n",
    "- **Personalized Recommendations**: Update recommendations as user browses\n",
    "- **Content Moderation**: Filter harmful content before it's visible\n",
    "- **Real-Time Bidding**: Predict ad click probability in <10ms\n",
    "- **Anomaly Detection**: Detect system failures as they happen"
   ],
   "id": "cell_01"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Streaming Architecture Patterns\n",
    "\n",
    "### Pattern 1: Embedded Model\n",
    "Model is **embedded directly** into the streaming application.\n",
    "\n",
    "```\n",
    "Kafka Stream ‚Üí Flink App (with model loaded) ‚Üí Predictions ‚Üí Output Stream\n",
    "```\n",
    "\n",
    "**Pros**: Lowest latency, simple deployment\n",
    "**Cons**: Hard to update model, duplicates model across instances\n",
    "\n",
    "### Pattern 2: Enricher (Model Service)\n",
    "Streaming app **calls a separate ML service** via gRPC/REST.\n",
    "\n",
    "```\n",
    "Kafka Stream ‚Üí Enricher App ‚Üí [gRPC call] ‚Üí Model Service ‚Üí Response ‚Üí Output Stream\n",
    "```\n",
    "\n",
    "**Pros**: Easy model updates, shared service, version control\n",
    "**Cons**: Extra network hop (~5-10ms latency)\n",
    "\n",
    "### Which to Use?\n",
    "| Requirement | Pattern |\n",
    "|-------------|----------|\n",
    "| Ultra-low latency (< 10ms) | Embedded |\n",
    "| Frequent model updates | Enricher |\n",
    "| Multiple models/versions | Enricher |\n",
    "| Simple model, stable | Embedded |"
   ],
   "id": "cell_02"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Real-Time Fraud Detection System\n",
    "\n",
    "Let's build a simplified fraud detection pipeline:\n",
    "1. Kafka receives transaction events\n",
    "2. PyTorch model scores each transaction\n",
    "3. Output flagged transactions to another Kafka topic\n",
    "\n",
    "### Step 1: Setup (Conceptual - requires Kafka)\n",
    "\n",
    "```bash\n",
    "# Install dependencies\n",
    "pip install kafka-python torch\n",
    "\n",
    "# Start Kafka (Docker)\n",
    "docker run -d -p 9092:9092 apache/kafka\n",
    "```"
   ],
   "id": "cell_03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Simulated Fraud Detection Model\n",
    "class FraudDetectionModel(nn.Module):\n",
    "    def __init__(self, input_dim=10):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output: fraud probability [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Load pretrained model (in production, load from S3/GCS)\n",
    "model = FraudDetectionModel()\n",
    "model.eval()  # Inference mode\n",
    "\n",
    "print(\"‚úÖ Fraud detection model loaded\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Feature Engineering (in production, use Feature Store)\n",
    "def extract_features(transaction: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert raw transaction to ML features.\n",
    "    In production, fetch from Feature Store (Feast, Tecton, etc.)\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        transaction['amount'],\n",
    "        transaction['merchant_risk_score'],\n",
    "        transaction['user_velocity_1h'],  # Transactions in last hour\n",
    "        transaction['user_velocity_24h'],\n",
    "        transaction['amount_vs_avg_ratio'],\n",
    "        transaction['time_since_last_txn_minutes'],\n",
    "        transaction['is_foreign'],  # Boolean ‚Üí 0 or 1\n",
    "        transaction['device_trust_score'],\n",
    "        transaction['merchant_category_risk'],\n",
    "        transaction['hour_of_day'] / 24.0  # Normalize\n",
    "    ]\n",
    "    return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "# Test feature extraction\n",
    "sample_transaction = {\n",
    "    'transaction_id': 'txn_12345',\n",
    "    'amount': 450.00,\n",
    "    'merchant_risk_score': 0.3,\n",
    "    'user_velocity_1h': 2,\n",
    "    'user_velocity_24h': 15,\n",
    "    'amount_vs_avg_ratio': 2.1,\n",
    "    'time_since_last_txn_minutes': 45,\n",
    "    'is_foreign': 0,\n",
    "    'device_trust_score': 0.85,\n",
    "    'merchant_category_risk': 0.2,\n",
    "    'hour_of_day': 14\n",
    "}\n",
    "\n",
    "features = extract_features(sample_transaction)\n",
    "print(f\"Extracted features: {features}\")\n",
    "print(f\"Feature shape: {features.shape}\")"
   ],
   "id": "cell_05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Inference Loop (Simulated)\n",
    "class StreamingInferenceEngine:\n",
    "    def __init__(self, model, fraud_threshold=0.7):\n",
    "        self.model = model\n",
    "        self.fraud_threshold = fraud_threshold\n",
    "        self.stats = {'processed': 0, 'fraudulent': 0, 'avg_latency_ms': 0}\n",
    "    \n",
    "    def process_event(self, transaction: Dict) -> Dict:\n",
    "        \"\"\"Process a single transaction event.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # 1. Extract features\n",
    "        features = extract_features(transaction).unsqueeze(0)  # Add batch dim\n",
    "        \n",
    "        # 2. Run inference\n",
    "        with torch.no_grad():\n",
    "            fraud_score = self.model(features).item()\n",
    "        \n",
    "        # 3. Make decision\n",
    "        is_fraud = fraud_score > self.fraud_threshold\n",
    "        \n",
    "        latency = (time.time() - start) * 1000  # Convert to ms\n",
    "        \n",
    "        # 4. Update stats\n",
    "        self.stats['processed'] += 1\n",
    "        if is_fraud:\n",
    "            self.stats['fraudulent'] += 1\n",
    "        self.stats['avg_latency_ms'] = (\n",
    "            (self.stats['avg_latency_ms'] * (self.stats['processed'] - 1) + latency)\n",
    "            / self.stats['processed']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'transaction_id': transaction['transaction_id'],\n",
    "            'fraud_score': fraud_score,\n",
    "            'is_fraud': is_fraud,\n",
    "            'latency_ms': latency\n",
    "        }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return self.stats\n",
    "\n",
    "# Initialize engine\n",
    "engine = StreamingInferenceEngine(model, fraud_threshold=0.7)\n",
    "\n",
    "# Simulate streaming events\n",
    "import random\n",
    "\n",
    "print(\"\\nSimulating real-time transaction stream:\\n\")\n",
    "for i in range(10):\n",
    "    # Simulate transaction (in production, this comes from Kafka)\n",
    "    txn = {\n",
    "        'transaction_id': f'txn_{i+1:05d}',\n",
    "        'amount': random.uniform(10, 1000),\n",
    "        'merchant_risk_score': random.random(),\n",
    "        'user_velocity_1h': random.randint(0, 10),\n",
    "        'user_velocity_24h': random.randint(0, 50),\n",
    "        'amount_vs_avg_ratio': random.uniform(0.5, 3.0),\n",
    "        'time_since_last_txn_minutes': random.randint(1, 120),\n",
    "        'is_foreign': random.choice([0, 1]),\n",
    "        'device_trust_score': random.random(),\n",
    "        'merchant_category_risk': random.random(),\n",
    "        'hour_of_day': random.randint(0, 23)\n",
    "    }\n",
    "    \n",
    "    result = engine.process_event(txn)\n",
    "    \n",
    "    status = \"üö® FRAUD\" if result['is_fraud'] else \"‚úÖ Clean\"\n",
    "    print(f\"{status} | {result['transaction_id']} | Score: {result['fraud_score']:.3f} | {result['latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "stats = engine.get_stats()\n",
    "print(f\"Processed: {stats['processed']} transactions\")\n",
    "print(f\"Fraudulent: {stats['fraudulent']} ({stats['fraudulent']/stats['processed']*100:.1f}%)\")\n",
    "print(f\"Average latency: {stats['avg_latency_ms']:.2f}ms\")"
   ],
   "id": "cell_06"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Stores - The Missing Piece\n",
    "\n",
    "### The Problem: Training-Serving Skew\n",
    "\n",
    "**Scenario**: You train a model using Spark (batch), but serve it in real-time (streaming).\n",
    "\n",
    "**What goes wrong**:\n",
    "```python\n",
    "# Training (Spark)\n",
    "user_avg_txn = df.groupBy('user_id').agg(avg('amount'))  # Pandas/Spark\n",
    "\n",
    "# Serving (Python)\n",
    "user_avg_txn = sum(amounts) / len(amounts)  # Native Python\n",
    "```\n",
    "\n",
    "**Result**: Slight differences in float precision, rounding, or logic ‚Üí **model performs worse in production!**\n",
    "\n",
    "### The Solution: Feature Store\n",
    "\n",
    "A Feature Store provides:\n",
    "1. **Single source of truth** for feature definitions\n",
    "2. **Offline store** (historical data for training)\n",
    "3. **Online store** (low-latency lookup for inference)\n",
    "4. **Automatic sync** between offline and online\n",
    "\n",
    "### Popular Feature Stores\n",
    "- **Feast** (Open Source, most popular)\n",
    "- **Tecton** (Enterprise)\n",
    "- **AWS SageMaker Feature Store**\n",
    "- **Databricks Feature Store**\n",
    "- **Hopsworks**"
   ],
   "id": "cell_07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Example: Using Feast Feature Store\n",
    "\n",
    "feast_example = '''\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Feast\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# Define features once\n",
    "entity_rows = [\n",
    "    {\"user_id\": \"user_123\", \"event_timestamp\": datetime.now()}\n",
    "]\n",
    "\n",
    "# Get features for inference (from online store - Redis/DynamoDB)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"user_features:avg_transaction_amount\",\n",
    "        \"user_features:transaction_count_24h\",\n",
    "        \"user_features:fraud_history_score\"\n",
    "    ],\n",
    "    entity_rows=entity_rows\n",
    ").to_dict()\n",
    "\n",
    "# These are THE SAME features used during training!\n",
    "# No training-serving skew!\n",
    "'''\n",
    "\n",
    "print(\"Feast Feature Store Usage:\")\n",
    "print(feast_example)\n",
    "print(\"\\n‚úÖ Key benefit: Training and serving use IDENTICAL feature logic!\")"
   ],
   "id": "cell_08"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Production Kafka Integration\n",
    "\n",
    "### Real Kafka Consumer Example"
   ],
   "id": "cell_09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready Kafka consumer with PyTorch inference\n",
    "# (Requires: pip install kafka-python)\n",
    "\n",
    "kafka_consumer_code = '''\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Initialize Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'transactions',  # Input topic\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='fraud-detection-service'\n",
    ")\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda m: json.dumps(m).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Load model once at startup\n",
    "model = FraudDetectionModel()\n",
    "model.eval()\n",
    "\n",
    "print(\"üöÄ Streaming inference service started...\")\n",
    "\n",
    "# Main processing loop\n",
    "for message in consumer:\n",
    "    transaction = message.value\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(transaction).unsqueeze(0)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        fraud_score = model(features).item()\n",
    "    \n",
    "    # Publish result to output topic\n",
    "    result = {\n",
    "        'transaction_id': transaction['transaction_id'],\n",
    "        'fraud_score': fraud_score,\n",
    "        'is_fraud': fraud_score > 0.7,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    producer.send('fraud-scores', value=result)\n",
    "    \n",
    "    if result['is_fraud']:\n",
    "        print(f\"üö® Fraud detected: {result['transaction_id']} (score: {fraud_score:.3f})\")\n",
    "'''\n",
    "\n",
    "print(\"Production Kafka + PyTorch Integration:\")\n",
    "print(kafka_consumer_code)\n",
    "print(\"\\n‚ö° This processes millions of transactions per day in real-time!\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Monitoring & Observability\n",
    "\n",
    "### Critical Metrics for Streaming ML\n",
    "\n",
    "1. **Latency Metrics**\n",
    "   - P50, P95, P99 latency\n",
    "   - Time-to-first-byte (TTFB)\n",
    "   \n",
    "2. **Throughput**\n",
    "   - Events/second processed\n",
    "   - Backlog/lag (messages waiting)\n",
    "   \n",
    "3. **Model Quality**\n",
    "   - Prediction distribution drift\n",
    "   - Feature distribution drift\n",
    "   - Online accuracy (when labels arrive)\n",
    "   \n",
    "4. **System Health**\n",
    "   - Consumer lag (Kafka)\n",
    "   - Error rate\n",
    "   - Memory/CPU utilization\n",
    "\n",
    "### Example: Prometheus Metrics"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example monitoring setup (requires prometheus_client)\n",
    "\n",
    "monitoring_code = '''\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "\n",
    "# Define metrics\n",
    "transactions_processed = Counter(\n",
    "    'fraud_detection_transactions_total',\n",
    "    'Total transactions processed'\n",
    ")\n",
    "\n",
    "fraud_detected = Counter(\n",
    "    'fraud_detection_fraud_total',\n",
    "    'Total fraudulent transactions detected'\n",
    ")\n",
    "\n",
    "inference_latency = Histogram(\n",
    "    'fraud_detection_latency_seconds',\n",
    "    'Inference latency in seconds',\n",
    "    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    ")\n",
    "\n",
    "fraud_score_dist = Histogram(\n",
    "    'fraud_detection_score_distribution',\n",
    "    'Distribution of fraud scores',\n",
    "    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    ")\n",
    "\n",
    "# Start metrics server\n",
    "start_http_server(8000)  # Expose metrics at :8000/metrics\n",
    "\n",
    "# In your inference loop:\n",
    "with inference_latency.time():\n",
    "    fraud_score = model(features).item()\n",
    "\n",
    "transactions_processed.inc()\n",
    "fraud_score_dist.observe(fraud_score)\n",
    "\n",
    "if fraud_score > 0.7:\n",
    "    fraud_detected.inc()\n",
    "'''\n",
    "\n",
    "print(\"Production Monitoring Setup:\")\n",
    "print(monitoring_code)\n",
    "print(\"\\nüìä Grafana dashboards can visualize these metrics in real-time!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Best Practices & Anti-Patterns\n",
    "\n",
    "### ‚úÖ DO\n",
    "1. **Use feature stores** to avoid training-serving skew\n",
    "2. **Monitor model drift** - retrain when performance degrades\n",
    "3. **Set latency SLAs** and alert when violated\n",
    "4. **Implement circuit breakers** - fallback if model fails\n",
    "5. **Version your models** - enable rollbacks\n",
    "6. **Load models once** at startup, not per request\n",
    "7. **Batch micro-batches** (e.g., 10ms window) for throughput\n",
    "\n",
    "### ‚ùå DON'T\n",
    "1. **Don't recompute features** differently in training vs serving\n",
    "2. **Don't ignore backpressure** - handle slow consumers\n",
    "3. **Don't deploy without monitoring** - you'll be blind\n",
    "4. **Don't use batch pipelines** for real-time use cases\n",
    "5. **Don't skip model validation** before deploying\n",
    "6. **Don't ignore data quality** - garbage in, garbage out\n",
    "\n",
    "### Handling Edge Cases\n",
    "- **Missing features**: Have default values or skip prediction\n",
    "- **Model errors**: Return safe default (e.g., flag as suspicious)\n",
    "- **Kafka downtime**: Buffer locally, implement retry logic\n",
    "- **Feature store lag**: Cache last known values (with TTL)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Streaming ML Stack (2025)\n",
    "\n",
    "### Architecture Components\n",
    "```\n",
    "Data Sources ‚Üí Kafka ‚Üí [Streaming App + PyTorch Model] ‚Üí Kafka ‚Üí Downstream Systems\n",
    "                         ‚Üì (feature lookup)\n",
    "                    Feature Store (Redis/DynamoDB)\n",
    "                         ‚Üì (metrics)\n",
    "                   Prometheus ‚Üí Grafana\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "- **Messaging**: Apache Kafka (industry standard)\n",
    "- **Processing**: Flink, Kafka Streams, or custom Python\n",
    "- **Feature Store**: Feast, Tecton, or cloud-native\n",
    "- **Monitoring**: Prometheus + Grafana\n",
    "- **Model Serving**: Embedded or separate service\n",
    "\n",
    "### Performance Targets\n",
    "- **Latency**: < 50ms for P95\n",
    "- **Throughput**: 10,000+ events/second per instance\n",
    "- **Availability**: 99.9% uptime\n",
    "- **Model freshness**: < 1 hour lag from training\n",
    "\n",
    "### What FAANG Expects You to Know\n",
    "‚úÖ Difference between batch and streaming inference\n",
    "‚úÖ How to prevent training-serving skew\n",
    "‚úÖ What feature stores are and why they matter\n",
    "‚úÖ Kafka integration patterns\n",
    "‚úÖ How to monitor streaming ML systems\n",
    "‚úÖ When to use embedded vs enricher pattern\n",
    "‚úÖ Latency optimization techniques\n",
    "\n",
    "### Further Reading\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Feast Feature Store](https://feast.dev/)\n",
    "- [Real-time ML with Kafka and Flink](https://www.kai-waehner.de/blog/2024/10/01/real-time-model-inference-with-apache-kafka-and-flink-for-predictive-ai-and-genai/)\n",
    "\n",
    "**You now understand production streaming ML infrastructure! üöÄ**"
   ],
   "id": "cell_14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}