{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: RAG and Agents\n",
    "\n",
    "The modern AI stack isn't just about training models; it's about **using** them. Two massive trends in 2025 are **RAG (Retrieval Augmented Generation)** and **Agents**.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **RAG**: Giving LLMs access to your private data.\n",
    "- Understand **Embeddings**: Converting text to numbers for search.\n",
    "- Build a simple **Agent**: An LLM that can use tools (like a calculator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "- **Embedding**: A vector (list of numbers) representing the *meaning* of text. Similar meanings = close vectors.\n",
    "- **Vector Database**: A database optimized to store and search embeddings.\n",
    "- **Context Window**: The limit on how much text an LLM can read at once.\n",
    "- **RAG**: Retrieve relevant docs -> Paste into Context -> Ask LLM to answer.\n",
    "- **Agent**: An LLM loop that decides *what to do next* (Think -> Act -> Observe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG: The \"Open Book\" Exam\n",
    "\n",
    "Imagine asking an LLM \"What is my company's vacation policy?\". It doesn't know. RAG fixes this.\n",
    "\n",
    "### Step A: Create Embeddings (Mock)\n",
    "In production, you'd use `OpenAI` or `HuggingFace` embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's pretend these are 3-dimensional embeddings for simplicity\n",
    "# In reality, they are 768 or 1536 dimensions\n",
    "documents = {\n",
    "    \"doc1\": torch.tensor([0.1, 0.2, 0.9]), # Represents \"Vacation policy\"\n",
    "    \"doc2\": torch.tensor([0.8, 0.1, 0.1]), # Represents \"Coffee machine manual\"\n",
    "    \"doc3\": torch.tensor([0.2, 0.9, 0.1])  # Represents \"Meeting notes\"\n",
    "}\n",
    "\n",
    "query = torch.tensor([0.15, 0.25, 0.85]) # User asks: \"How many vacation days?\"\n",
    "\n",
    "print(\"Database created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step B: Retrieval (Cosine Similarity)\n",
    "We find the document closest to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, docs):\n",
    "    best_doc = None\n",
    "    max_score = -1\n",
    "    \n",
    "    for name, doc_vec in docs.items():\n",
    "        # Cosine Similarity: (A . B) / (|A| * |B|)\n",
    "        score = F.cosine_similarity(query.unsqueeze(0), doc_vec.unsqueeze(0))\n",
    "        print(f\"Score for {name}: {score.item():.4f}\")\n",
    "        \n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_doc = name\n",
    "            \n",
    "    return best_doc\n",
    "\n",
    "result = retrieve(query, documents)\n",
    "print(f\"\\nMost relevant document: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agents: LLMs with Tools\n",
    "\n",
    "An agent is a loop:\n",
    "1. **Thought**: LLM analyzes the request.\n",
    "2. **Action**: LLM decides to call a function (e.g., `calculator(5, 5)`).\n",
    "3. **Observation**: The function returns a result (`25`).\n",
    "4. **Response**: LLM uses the result to answer the user.\n",
    "\n",
    "This is how ChatGPT Plugins work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Agent Loop\n",
    "def calculator(a, b, op):\n",
    "    if op == 'add': return a + b\n",
    "    if op == 'mul': return a * b\n",
    "\n",
    "user_request = \"What is 5 * 5?\"\n",
    "\n",
    "# 1. LLM decides to use a tool (Mocked)\n",
    "print(f\"User: {user_request}\")\n",
    "print(\"Agent: I need to calculate 5 * 5. Calling tool 'calculator'...\")\n",
    "\n",
    "# 2. Tool Execution\n",
    "result = calculator(5, 5, 'mul')\n",
    "\n",
    "# 3. Final Answer\n",
    "print(f\"Agent: The result is {result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RAG** connects LLMs to your data using embeddings.\n",
    "2. **Agents** give LLMs \"hands\" to perform actions.\n",
    "3. This is the future of Application AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
