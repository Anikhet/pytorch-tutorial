{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI and Fairness\n",
    "\n",
    "## Overview\n",
    "Essential knowledge for enterprise ML and FAANG interviews:\n",
    "- **Bias Detection**: Identifying unfair model behavior\n",
    "- **Fairness Metrics**: Demographic parity, equalized odds, calibration\n",
    "- **Explainability**: SHAP, LIME, attention visualization\n",
    "- **Compliance**: EU AI Act, GDPR, model documentation\n",
    "\n",
    "## Why This Matters\n",
    "- **Legal**: EU AI Act mandates fairness and explainability\n",
    "- **Reputation**: Biased AI causes PR disasters\n",
    "- **Business**: Unfair models lose customers and face lawsuits\n",
    "- **Ethics**: Building AI that serves everyone fairly\n",
    "\n",
    "## FAANG Interview Focus\n",
    "- How do you detect bias in ML models?\n",
    "- Explain the trade-off between different fairness metrics\n",
    "- How do you make model predictions explainable?\n",
    "- Design a fairness monitoring system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# pip install aif360 fairlearn shap lime scikit-learn pandas numpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Responsible AI & Fairness - FAANG Interview Prep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Bias in ML\n",
    "\n",
    "### Types of Bias\n",
    "1. **Historical Bias**: Training data reflects past discrimination\n",
    "2. **Representation Bias**: Underrepresentation of certain groups\n",
    "3. **Measurement Bias**: Features measured differently across groups\n",
    "4. **Aggregation Bias**: One model doesn't fit all subgroups\n",
    "5. **Evaluation Bias**: Test data doesn't represent production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasType(Enum):\n",
    "    HISTORICAL = \"historical\"  # Past discrimination in data\n",
    "    REPRESENTATION = \"representation\"  # Underrepresented groups\n",
    "    MEASUREMENT = \"measurement\"  # Inconsistent feature collection\n",
    "    AGGREGATION = \"aggregation\"  # Model doesn't fit subgroups\n",
    "    EVALUATION = \"evaluation\"  # Test data not representative\n",
    "    DEPLOYMENT = \"deployment\"  # Different behavior in production\n",
    "\n",
    "@dataclass\n",
    "class FairnessReport:\n",
    "    \"\"\"Standardized fairness evaluation report.\"\"\"\n",
    "    dataset_name: str\n",
    "    protected_attribute: str\n",
    "    privileged_group: str\n",
    "    unprivileged_group: str\n",
    "    metrics: Dict[str, float]\n",
    "    violations: List[str]\n",
    "    recommendations: List[str]\n",
    "\n",
    "def create_synthetic_biased_data(n_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic dataset with intentional bias for demonstration.\n",
    "    \n",
    "    Scenario: Loan approval with gender bias.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Demographics\n",
    "    gender = np.random.choice(['male', 'female'], n_samples, p=[0.6, 0.4])\n",
    "    age = np.random.normal(35, 10, n_samples).clip(18, 65).astype(int)\n",
    "    \n",
    "    # Features\n",
    "    income = np.random.normal(50000, 20000, n_samples).clip(20000, 150000)\n",
    "    credit_score = np.random.normal(650, 100, n_samples).clip(300, 850).astype(int)\n",
    "    \n",
    "    # Introduce bias: males get higher credit scores on average\n",
    "    credit_score = np.where(gender == 'male', credit_score + 30, credit_score).clip(300, 850)\n",
    "    \n",
    "    # Target: Loan approval (biased toward males)\n",
    "    base_prob = 0.3 + (credit_score - 500) / 1000 + (income - 30000) / 200000\n",
    "    # Add gender bias to approval\n",
    "    approval_prob = np.where(gender == 'male', base_prob + 0.1, base_prob - 0.05)\n",
    "    approval_prob = approval_prob.clip(0.1, 0.9)\n",
    "    approved = np.random.binomial(1, approval_prob)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'gender': gender,\n",
    "        'age': age,\n",
    "        'income': income,\n",
    "        'credit_score': credit_score,\n",
    "        'approved': approved\n",
    "    })\n",
    "\n",
    "# Create biased dataset\n",
    "print(\"\\n=== Synthetic Biased Dataset ===\")\n",
    "df = create_synthetic_biased_data(5000)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nApproval rates by gender:\")\n",
    "print(df.groupby('gender')['approved'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fairness Metrics\n",
    "\n",
    "### Key Metrics:\n",
    "1. **Demographic Parity**: Equal positive rates across groups\n",
    "2. **Equalized Odds**: Equal TPR and FPR across groups\n",
    "3. **Calibration**: Predictions mean the same thing across groups\n",
    "4. **Individual Fairness**: Similar individuals treated similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive fairness metrics calculator.\n",
    "    \n",
    "    Based on: IBM AIF360, Microsoft Fairlearn\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                 protected_attribute: np.ndarray,\n",
    "                 privileged_value: Any):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: Ground truth labels\n",
    "            y_pred: Predicted labels (binary)\n",
    "            protected_attribute: Protected attribute values (e.g., gender)\n",
    "            privileged_value: Value of the privileged group (e.g., 'male')\n",
    "        \"\"\"\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.protected = protected_attribute\n",
    "        self.privileged_value = privileged_value\n",
    "        \n",
    "        # Masks for groups\n",
    "        self.privileged_mask = protected_attribute == privileged_value\n",
    "        self.unprivileged_mask = ~self.privileged_mask\n",
    "    \n",
    "    def demographic_parity_difference(self) -> float:\n",
    "        \"\"\"\n",
    "        Demographic Parity: P(Y_hat=1|A=0) = P(Y_hat=1|A=1)\n",
    "        \n",
    "        Measures: Are positive predictions equal across groups?\n",
    "        Ideal: 0 (no difference)\n",
    "        \"\"\"\n",
    "        priv_rate = self.y_pred[self.privileged_mask].mean()\n",
    "        unpriv_rate = self.y_pred[self.unprivileged_mask].mean()\n",
    "        return priv_rate - unpriv_rate\n",
    "    \n",
    "    def disparate_impact_ratio(self) -> float:\n",
    "        \"\"\"\n",
    "        Disparate Impact: P(Y_hat=1|A=0) / P(Y_hat=1|A=1)\n",
    "        \n",
    "        Legal threshold: 0.8 (80% rule)\n",
    "        Ideal: 1.0 (equal rates)\n",
    "        \"\"\"\n",
    "        priv_rate = self.y_pred[self.privileged_mask].mean()\n",
    "        unpriv_rate = self.y_pred[self.unprivileged_mask].mean()\n",
    "        return unpriv_rate / (priv_rate + 1e-10)\n",
    "    \n",
    "    def equalized_odds_difference(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Equalized Odds: Equal TPR and FPR across groups.\n",
    "        \n",
    "        P(Y_hat=1|Y=1,A=0) = P(Y_hat=1|Y=1,A=1)  # TPR\n",
    "        P(Y_hat=1|Y=0,A=0) = P(Y_hat=1|Y=0,A=1)  # FPR\n",
    "        \"\"\"\n",
    "        # TPR for privileged\n",
    "        priv_pos = self.privileged_mask & (self.y_true == 1)\n",
    "        priv_tpr = self.y_pred[priv_pos].mean() if priv_pos.sum() > 0 else 0\n",
    "        \n",
    "        # TPR for unprivileged\n",
    "        unpriv_pos = self.unprivileged_mask & (self.y_true == 1)\n",
    "        unpriv_tpr = self.y_pred[unpriv_pos].mean() if unpriv_pos.sum() > 0 else 0\n",
    "        \n",
    "        # FPR for privileged\n",
    "        priv_neg = self.privileged_mask & (self.y_true == 0)\n",
    "        priv_fpr = self.y_pred[priv_neg].mean() if priv_neg.sum() > 0 else 0\n",
    "        \n",
    "        # FPR for unprivileged\n",
    "        unpriv_neg = self.unprivileged_mask & (self.y_true == 0)\n",
    "        unpriv_fpr = self.y_pred[unpriv_neg].mean() if unpriv_neg.sum() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'tpr_difference': priv_tpr - unpriv_tpr,\n",
    "            'fpr_difference': priv_fpr - unpriv_fpr,\n",
    "            'privileged_tpr': priv_tpr,\n",
    "            'unprivileged_tpr': unpriv_tpr,\n",
    "            'privileged_fpr': priv_fpr,\n",
    "            'unprivileged_fpr': unpriv_fpr\n",
    "        }\n",
    "    \n",
    "    def calibration_difference(self, y_prob: np.ndarray, \n",
    "                               n_bins: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calibration: Predicted probability should match actual rate.\n",
    "        \n",
    "        For each group: P(Y=1|Y_hat=p) = p\n",
    "        \"\"\"\n",
    "        def calibration_error(mask):\n",
    "            bins = np.linspace(0, 1, n_bins + 1)\n",
    "            errors = []\n",
    "            \n",
    "            for i in range(n_bins):\n",
    "                bin_mask = mask & (y_prob >= bins[i]) & (y_prob < bins[i + 1])\n",
    "                if bin_mask.sum() > 0:\n",
    "                    pred_mean = y_prob[bin_mask].mean()\n",
    "                    actual_mean = self.y_true[bin_mask].mean()\n",
    "                    errors.append(abs(pred_mean - actual_mean))\n",
    "            \n",
    "            return np.mean(errors) if errors else 0\n",
    "        \n",
    "        priv_cal = calibration_error(self.privileged_mask)\n",
    "        unpriv_cal = calibration_error(self.unprivileged_mask)\n",
    "        \n",
    "        return {\n",
    "            'privileged_calibration_error': priv_cal,\n",
    "            'unprivileged_calibration_error': unpriv_cal,\n",
    "            'calibration_difference': abs(priv_cal - unpriv_cal)\n",
    "        }\n",
    "    \n",
    "    def get_all_metrics(self, y_prob: np.ndarray = None) -> Dict[str, Any]:\n",
    "        \"\"\"Compute all fairness metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'demographic_parity_diff': self.demographic_parity_difference(),\n",
    "            'disparate_impact_ratio': self.disparate_impact_ratio(),\n",
    "            'equalized_odds': self.equalized_odds_difference()\n",
    "        }\n",
    "        \n",
    "        if y_prob is not None:\n",
    "            metrics['calibration'] = self.calibration_difference(y_prob)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def is_fair(self, threshold: float = 0.1) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check if model passes fairness criteria.\n",
    "        \n",
    "        Common thresholds:\n",
    "        - Demographic parity diff: |diff| < 0.1\n",
    "        - Disparate impact: 0.8 < ratio < 1.25\n",
    "        - Equalized odds: |diff| < 0.1\n",
    "        \"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        dp_diff = abs(self.demographic_parity_difference())\n",
    "        if dp_diff > threshold:\n",
    "            violations.append(f'Demographic parity: {dp_diff:.3f} > {threshold}')\n",
    "        \n",
    "        di_ratio = self.disparate_impact_ratio()\n",
    "        if di_ratio < 0.8 or di_ratio > 1.25:\n",
    "            violations.append(f'Disparate impact: {di_ratio:.3f} (should be 0.8-1.25)')\n",
    "        \n",
    "        eo = self.equalized_odds_difference()\n",
    "        if abs(eo['tpr_difference']) > threshold:\n",
    "            violations.append(f'TPR difference: {eo[\"tpr_difference\"]:.3f} > {threshold}')\n",
    "        if abs(eo['fpr_difference']) > threshold:\n",
    "            violations.append(f'FPR difference: {eo[\"fpr_difference\"]:.3f} > {threshold}')\n",
    "        \n",
    "        return len(violations) == 0, violations\n",
    "\n",
    "# Example: Evaluate fairness on biased data\n",
    "print(\"\\n=== Fairness Metrics Example ===\")\n",
    "\n",
    "# Simulate model predictions (biased model)\n",
    "y_true = df['approved'].values\n",
    "# Biased predictions: higher for males\n",
    "y_prob = np.where(\n",
    "    df['gender'] == 'male',\n",
    "    np.random.beta(3, 2, len(df)),  # Higher for males\n",
    "    np.random.beta(2, 3, len(df))   # Lower for females\n",
    ")\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate fairness metrics\n",
    "fairness = FairnessMetrics(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    protected_attribute=df['gender'].values,\n",
    "    privileged_value='male'\n",
    ")\n",
    "\n",
    "metrics = fairness.get_all_metrics(y_prob)\n",
    "print(f\"\\nDemographic Parity Difference: {metrics['demographic_parity_diff']:.4f}\")\n",
    "print(f\"Disparate Impact Ratio: {metrics['disparate_impact_ratio']:.4f}\")\n",
    "print(f\"TPR Difference: {metrics['equalized_odds']['tpr_difference']:.4f}\")\n",
    "print(f\"FPR Difference: {metrics['equalized_odds']['fpr_difference']:.4f}\")\n",
    "\n",
    "is_fair, violations = fairness.is_fair(threshold=0.1)\n",
    "print(f\"\\nModel is fair: {is_fair}\")\n",
    "if violations:\n",
    "    print(\"Violations:\")\n",
    "    for v in violations:\n",
    "        print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Bias Mitigation Techniques\n",
    "\n",
    "### Mitigation Strategies:\n",
    "1. **Pre-processing**: Fix the data before training\n",
    "2. **In-processing**: Modify the training algorithm\n",
    "3. **Post-processing**: Adjust predictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasMitigation:\n",
    "    \"\"\"Bias mitigation techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def reweighting(y_true: np.ndarray, protected: np.ndarray,\n",
    "                   privileged_value: Any) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pre-processing: Assign weights to balance groups.\n",
    "        \n",
    "        Weight = P(Y,A) / (P(Y) * P(A))\n",
    "        \"\"\"\n",
    "        priv_mask = protected == privileged_value\n",
    "        \n",
    "        # Compute joint and marginal probabilities\n",
    "        n = len(y_true)\n",
    "        weights = np.ones(n)\n",
    "        \n",
    "        for y_val in [0, 1]:\n",
    "            for is_priv in [True, False]:\n",
    "                mask = (y_true == y_val) & (priv_mask == is_priv)\n",
    "                if mask.sum() > 0:\n",
    "                    p_y = (y_true == y_val).mean()\n",
    "                    p_a = (priv_mask == is_priv).mean()\n",
    "                    p_ya = mask.mean()\n",
    "                    \n",
    "                    # Weight to achieve independence\n",
    "                    expected = p_y * p_a\n",
    "                    weights[mask] = expected / (p_ya + 1e-10)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def disparate_impact_remover(X: np.ndarray, protected: np.ndarray,\n",
    "                                 repair_level: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pre-processing: Transform features to remove correlation with protected attribute.\n",
    "        \n",
    "        repair_level: 0.0 (no change) to 1.0 (full repair)\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        unique_groups = np.unique(protected)\n",
    "        \n",
    "        for col in range(X.shape[1]):\n",
    "            # Compute median for each group\n",
    "            medians = {}\n",
    "            for g in unique_groups:\n",
    "                medians[g] = np.median(X[protected == g, col])\n",
    "            \n",
    "            # Overall median\n",
    "            overall_median = np.median(X[:, col])\n",
    "            \n",
    "            # Repair: Move each group's distribution toward overall median\n",
    "            for g in unique_groups:\n",
    "                mask = protected == g\n",
    "                group_vals = X[mask, col]\n",
    "                \n",
    "                # Shift toward overall median\n",
    "                shift = (overall_median - medians[g]) * repair_level\n",
    "                X_transformed[mask, col] = group_vals + shift\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_optimizer(y_prob: np.ndarray, y_true: np.ndarray,\n",
    "                           protected: np.ndarray, privileged_value: Any,\n",
    "                           constraint: str = 'demographic_parity') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Post-processing: Find optimal thresholds per group for fairness.\n",
    "        \n",
    "        Grid search over thresholds to satisfy fairness constraint.\n",
    "        \"\"\"\n",
    "        priv_mask = protected == privileged_value\n",
    "        best_thresholds = {'privileged': 0.5, 'unprivileged': 0.5}\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for priv_thresh in np.arange(0.3, 0.7, 0.05):\n",
    "            for unpriv_thresh in np.arange(0.3, 0.7, 0.05):\n",
    "                # Apply group-specific thresholds\n",
    "                y_pred = np.where(\n",
    "                    priv_mask,\n",
    "                    (y_prob > priv_thresh).astype(int),\n",
    "                    (y_prob > unpriv_thresh).astype(int)\n",
    "                )\n",
    "                \n",
    "                # Check fairness constraint\n",
    "                priv_rate = y_pred[priv_mask].mean()\n",
    "                unpriv_rate = y_pred[~priv_mask].mean()\n",
    "                \n",
    "                if constraint == 'demographic_parity':\n",
    "                    is_fair = abs(priv_rate - unpriv_rate) < 0.05\n",
    "                else:\n",
    "                    is_fair = True\n",
    "                \n",
    "                if is_fair:\n",
    "                    accuracy = (y_pred == y_true).mean()\n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_thresholds = {\n",
    "                            'privileged': priv_thresh,\n",
    "                            'unprivileged': unpriv_thresh\n",
    "                        }\n",
    "        \n",
    "        return best_thresholds\n",
    "\n",
    "# Example: Apply mitigation\n",
    "print(\"\\n=== Bias Mitigation Example ===\")\n",
    "\n",
    "# Reweighting\n",
    "weights = BiasMitigation.reweighting(\n",
    "    y_true=y_true,\n",
    "    protected=df['gender'].values,\n",
    "    privileged_value='male'\n",
    ")\n",
    "print(f\"Sample weights: mean={weights.mean():.3f}, std={weights.std():.3f}\")\n",
    "\n",
    "# Threshold optimization\n",
    "thresholds = BiasMitigation.threshold_optimizer(\n",
    "    y_prob=y_prob,\n",
    "    y_true=y_true,\n",
    "    protected=df['gender'].values,\n",
    "    privileged_value='male'\n",
    ")\n",
    "print(f\"\\nOptimal thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Explainability\n",
    "\n",
    "### Explainability Methods:\n",
    "1. **Global**: Overall model behavior (feature importance)\n",
    "2. **Local**: Individual prediction explanation (SHAP, LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExplainer:\n",
    "    \"\"\"\n",
    "    Model explainability using SHAP-like analysis.\n",
    "    \n",
    "    Implements simplified versions of:\n",
    "    - SHAP (SHapley Additive exPlanations)\n",
    "    - LIME (Local Interpretable Model-agnostic Explanations)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_names: List[str]):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def permutation_importance(self, X: np.ndarray, y: np.ndarray,\n",
    "                              n_repeats: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Global: Feature importance via permutation.\n",
    "        \n",
    "        Shuffle each feature and measure accuracy drop.\n",
    "        \"\"\"\n",
    "        baseline_score = self._score(X, y)\n",
    "        importances = {}\n",
    "        \n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            scores = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_permuted = X.copy()\n",
    "                np.random.shuffle(X_permuted[:, i])\n",
    "                score = self._score(X_permuted, y)\n",
    "                scores.append(baseline_score - score)\n",
    "            \n",
    "            importances[name] = np.mean(scores)\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    def shap_values_approx(self, X: np.ndarray, instance: np.ndarray,\n",
    "                          n_samples: int = 100) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Local: Approximate SHAP values for a single instance.\n",
    "        \n",
    "        Uses sampling-based approximation of Shapley values.\n",
    "        \"\"\"\n",
    "        n_features = len(self.feature_names)\n",
    "        shap_values = np.zeros(n_features)\n",
    "        \n",
    "        # Background data (reference)\n",
    "        background = X[np.random.choice(len(X), min(100, len(X)), replace=False)]\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Random permutation\n",
    "            perm = np.random.permutation(n_features)\n",
    "            \n",
    "            # Background instance\n",
    "            bg_idx = np.random.randint(len(background))\n",
    "            bg = background[bg_idx].copy()\n",
    "            \n",
    "            # Add features one by one according to permutation\n",
    "            current = bg.copy()\n",
    "            prev_pred = self.model(current.reshape(1, -1))[0]\n",
    "            \n",
    "            for i, feat_idx in enumerate(perm):\n",
    "                current[feat_idx] = instance[feat_idx]\n",
    "                curr_pred = self.model(current.reshape(1, -1))[0]\n",
    "                \n",
    "                # Marginal contribution\n",
    "                shap_values[feat_idx] += (curr_pred - prev_pred)\n",
    "                prev_pred = curr_pred\n",
    "        \n",
    "        # Average\n",
    "        shap_values /= n_samples\n",
    "        \n",
    "        return dict(zip(self.feature_names, shap_values))\n",
    "    \n",
    "    def lime_explanation(self, instance: np.ndarray, \n",
    "                        n_samples: int = 500) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Local: LIME explanation for a single instance.\n",
    "        \n",
    "        Fits a local linear model around the instance.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import Ridge\n",
    "        \n",
    "        # Generate perturbed samples around instance\n",
    "        n_features = len(instance)\n",
    "        perturbations = np.random.normal(0, 0.1, (n_samples, n_features))\n",
    "        samples = instance + perturbations\n",
    "        \n",
    "        # Get predictions for perturbed samples\n",
    "        predictions = self.model(samples)\n",
    "        \n",
    "        # Compute weights (closer samples have higher weight)\n",
    "        distances = np.sqrt(np.sum(perturbations ** 2, axis=1))\n",
    "        kernel_width = 0.75 * np.sqrt(n_features)\n",
    "        weights = np.exp(-distances ** 2 / kernel_width ** 2)\n",
    "        \n",
    "        # Fit weighted linear model\n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(samples, predictions, sample_weight=weights)\n",
    "        \n",
    "        return dict(zip(self.feature_names, model.coef_))\n",
    "    \n",
    "    def _score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy score.\"\"\"\n",
    "        predictions = self.model(X)\n",
    "        if predictions.ndim > 1:\n",
    "            predictions = predictions.squeeze()\n",
    "        return (predictions.round() == y).mean()\n",
    "\n",
    "# Example: Create a simple model and explain it\n",
    "print(\"\\n=== Model Explainability Example ===\")\n",
    "\n",
    "# Simple model function\n",
    "def simple_model(X):\n",
    "    \"\"\"Logistic-like model.\"\"\"\n",
    "    weights = np.array([0.001, 0.002, 0.00001, 0.001])  # income, credit, age, gender\n",
    "    logits = X @ weights\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "# Prepare data\n",
    "X = df[['income', 'credit_score', 'age']].values\n",
    "X = np.column_stack([X, (df['gender'] == 'male').astype(float)])\n",
    "feature_names = ['income', 'credit_score', 'age', 'is_male']\n",
    "\n",
    "explainer = ModelExplainer(simple_model, feature_names)\n",
    "\n",
    "# Permutation importance\n",
    "importance = explainer.permutation_importance(X, y_true, n_repeats=5)\n",
    "print(\"\\nPermutation Importance:\")\n",
    "for feat, imp in sorted(importance.items(), key=lambda x: -abs(x[1])):\n",
    "    print(f\"  {feat}: {imp:.4f}\")\n",
    "\n",
    "# SHAP for single instance\n",
    "instance = X[0]\n",
    "shap_vals = explainer.shap_values_approx(X, instance, n_samples=50)\n",
    "print(\"\\nSHAP Values for first instance:\")\n",
    "for feat, val in sorted(shap_vals.items(), key=lambda x: -abs(x[1])):\n",
    "    print(f\"  {feat}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Cards and Documentation\n",
    "\n",
    "### Model Card Template (Google's Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelCard:\n",
    "    \"\"\"\n",
    "    Model Card: Standardized documentation for ML models.\n",
    "    \n",
    "    Based on: Google's Model Cards for Model Reporting\n",
    "    Required for: EU AI Act compliance, responsible AI practices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model details\n",
    "    model_name: str\n",
    "    model_version: str\n",
    "    model_type: str  # e.g., \"binary classification\"\n",
    "    model_description: str\n",
    "    model_owner: str\n",
    "    \n",
    "    # Intended use\n",
    "    primary_intended_uses: List[str]\n",
    "    primary_intended_users: List[str]\n",
    "    out_of_scope_uses: List[str]\n",
    "    \n",
    "    # Training data\n",
    "    training_data_description: str\n",
    "    training_data_size: int\n",
    "    training_data_demographics: Dict[str, Any]\n",
    "    \n",
    "    # Evaluation data\n",
    "    evaluation_data_description: str\n",
    "    evaluation_data_size: int\n",
    "    \n",
    "    # Performance metrics\n",
    "    overall_metrics: Dict[str, float]\n",
    "    disaggregated_metrics: Dict[str, Dict[str, float]]  # By subgroup\n",
    "    \n",
    "    # Fairness analysis\n",
    "    fairness_metrics: Dict[str, float]\n",
    "    fairness_considerations: List[str]\n",
    "    \n",
    "    # Limitations and ethical considerations\n",
    "    limitations: List[str]\n",
    "    ethical_considerations: List[str]\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations: List[str]\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown documentation.\"\"\"\n",
    "        md = f\"\"\"# Model Card: {self.model_name}\n",
    "\n",
    "## Model Details\n",
    "- **Version**: {self.model_version}\n",
    "- **Type**: {self.model_type}\n",
    "- **Owner**: {self.model_owner}\n",
    "- **Description**: {self.model_description}\n",
    "\n",
    "## Intended Use\n",
    "### Primary Uses\n",
    "{\"\".join(f\"- {use}\" + chr(10) for use in self.primary_intended_uses)}\n",
    "\n",
    "### Out of Scope Uses\n",
    "{\"\".join(f\"- {use}\" + chr(10) for use in self.out_of_scope_uses)}\n",
    "\n",
    "## Training Data\n",
    "- **Description**: {self.training_data_description}\n",
    "- **Size**: {self.training_data_size:,} samples\n",
    "\n",
    "## Performance\n",
    "### Overall Metrics\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "{\"\".join(f\"| {k} | {v:.4f} |\" + chr(10) for k, v in self.overall_metrics.items())}\n",
    "\n",
    "## Fairness Analysis\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "{\"\".join(f\"| {k} | {v:.4f} |\" + chr(10) for k, v in self.fairness_metrics.items())}\n",
    "\n",
    "### Considerations\n",
    "{\"\".join(f\"- {c}\" + chr(10) for c in self.fairness_considerations)}\n",
    "\n",
    "## Limitations\n",
    "{\"\".join(f\"- {l}\" + chr(10) for l in self.limitations)}\n",
    "\n",
    "## Ethical Considerations\n",
    "{\"\".join(f\"- {e}\" + chr(10) for e in self.ethical_considerations)}\n",
    "\n",
    "## Recommendations\n",
    "{\"\".join(f\"- {r}\" + chr(10) for r in self.recommendations)}\n",
    "\"\"\"\n",
    "        return md\n",
    "\n",
    "# Example model card\n",
    "print(\"\\n=== Model Card Example ===\")\n",
    "\n",
    "model_card = ModelCard(\n",
    "    model_name=\"Loan Approval Classifier\",\n",
    "    model_version=\"2.1.0\",\n",
    "    model_type=\"Binary Classification\",\n",
    "    model_description=\"Predicts loan approval likelihood based on applicant features\",\n",
    "    model_owner=\"ML Risk Team\",\n",
    "    \n",
    "    primary_intended_uses=[\n",
    "        \"Pre-screening loan applications\",\n",
    "        \"Assisting human reviewers in decision-making\"\n",
    "    ],\n",
    "    primary_intended_users=[\"Loan officers\", \"Risk analysts\"],\n",
    "    out_of_scope_uses=[\n",
    "        \"Fully automated loan decisions without human review\",\n",
    "        \"Credit scoring for non-loan products\"\n",
    "    ],\n",
    "    \n",
    "    training_data_description=\"Historical loan applications from 2020-2023\",\n",
    "    training_data_size=500000,\n",
    "    training_data_demographics={'gender': {'male': 0.55, 'female': 0.45}},\n",
    "    \n",
    "    evaluation_data_description=\"Held-out 2023 Q4 applications\",\n",
    "    evaluation_data_size=50000,\n",
    "    \n",
    "    overall_metrics={\n",
    "        'accuracy': 0.85,\n",
    "        'precision': 0.82,\n",
    "        'recall': 0.78,\n",
    "        'auc_roc': 0.89\n",
    "    },\n",
    "    disaggregated_metrics={\n",
    "        'male': {'accuracy': 0.87, 'precision': 0.85},\n",
    "        'female': {'accuracy': 0.82, 'precision': 0.78}\n",
    "    },\n",
    "    \n",
    "    fairness_metrics={\n",
    "        'demographic_parity_diff': 0.08,\n",
    "        'disparate_impact_ratio': 0.91\n",
    "    },\n",
    "    fairness_considerations=[\n",
    "        \"Model shows slight bias toward male applicants\",\n",
    "        \"Mitigation: Applied threshold optimization per group\"\n",
    "    ],\n",
    "    \n",
    "    limitations=[\n",
    "        \"Performance degrades for applicants under 25\",\n",
    "        \"Limited data for self-employed applicants\"\n",
    "    ],\n",
    "    ethical_considerations=[\n",
    "        \"Model should not be used as sole decision-maker\",\n",
    "        \"Regular fairness audits required\"\n",
    "    ],\n",
    "    \n",
    "    recommendations=[\n",
    "        \"Combine with human review for edge cases\",\n",
    "        \"Monitor for drift monthly\",\n",
    "        \"Retrain quarterly with fresh data\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(model_card.to_markdown()[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Fairness Checklist:\n",
    "1. **Identify protected attributes**: gender, race, age, disability\n",
    "2. **Choose appropriate metrics**: Demographic parity vs equalized odds\n",
    "3. **Measure baseline bias**: Before any mitigation\n",
    "4. **Apply mitigation**: Pre/in/post-processing techniques\n",
    "5. **Document everything**: Model cards, audit trails\n",
    "6. **Monitor in production**: Continuous fairness monitoring\n",
    "\n",
    "### Explainability Checklist:\n",
    "1. **Global explanations**: Feature importance, partial dependence\n",
    "2. **Local explanations**: SHAP, LIME for individual predictions\n",
    "3. **Counterfactuals**: \"What would change the prediction?\"\n",
    "4. **Human-readable**: Explanations non-technical users understand\n",
    "\n",
    "## FAANG Interview Questions\n",
    "\n",
    "**Q1: How do you detect bias in an ML model?**\n",
    "- Compute fairness metrics across protected groups\n",
    "- Check demographic parity, equalized odds, calibration\n",
    "- Use disparate impact ratio (80% rule)\n",
    "- Analyze disaggregated performance by subgroup\n",
    "\n",
    "**Q2: Explain the trade-off between different fairness metrics.**\n",
    "- Demographic parity: Equal positive rates (may sacrifice accuracy)\n",
    "- Equalized odds: Equal TPR/FPR (preserves prediction quality)\n",
    "- Calibration: Same meaning across groups (for probabilistic models)\n",
    "- Impossibility theorem: Can't satisfy all metrics simultaneously\n",
    "\n",
    "**Q3: How do you make model predictions explainable?**\n",
    "- SHAP: Shapley values for feature attribution\n",
    "- LIME: Local linear approximation\n",
    "- Attention weights: For transformer models\n",
    "- Decision rules: For tree-based models\n",
    "\n",
    "**Q4: Design a fairness monitoring system.**\n",
    "- Track fairness metrics over time\n",
    "- Alert on threshold violations\n",
    "- Slice analysis by protected attributes\n",
    "- A/B test fairness interventions\n",
    "- Regular audits and documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
