{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Generative Diffusion Models\n",
    "\n",
    "Diffusion models (like Stable Diffusion and DALL-E) generate images by learning to reverse a gradual noise process. They start with pure noise and slowly refine it into an image.\n",
    "\n",
    "In this notebook, we will implement the core mathematics of **DDPM (Denoising Diffusion Probabilistic Models)**.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Forward Diffusion Process (Adding Noise)\n",
    "- Understand the Reverse Diffusion Process (Denoising)\n",
    "- Implement the Noise Schedule\n",
    "- Build a simplified U-Net for noise prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Forward Process (Adding Noise)\n",
    "\n",
    "We take an image $x_0$ and add Gaussian noise over $T$ steps until it becomes pure noise $x_T$.\n",
    "\n",
    "Formula:\n",
    "$$ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I) $$\n",
    "\n",
    "We can jump directly to any step $t$:\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon $$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Define schedule\n",
    "T = 200\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"Helper to get value at index t and reshape to match x\"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"Takes an image and a timestep t and returns the noisy image at t\"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(torch.sqrt(alphas_cumprod), t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(torch.sqrt(1. - alphas_cumprod), t, x_0.shape)\n",
    "    \n",
    "    # Mean + Variance\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# Visualize\n",
    "image = torch.zeros((1, 3, 64, 64)) # Dummy black image\n",
    "image[:, :, 16:48, 16:48] = 1.0 # White square in middle\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "for idx, t_val in enumerate([0, 50, 100, 199]):\n",
    "    t = torch.tensor([t_val])\n",
    "    noisy_image, _ = forward_diffusion_sample(image, t)\n",
    "    \n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    plt.imshow(noisy_image[0].permute(1, 2, 0).clamp(0, 1))\n",
    "    plt.title(f\"t={t_val}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Reverse Process (The Model)\n",
    "\n",
    "We need a neural network that takes a noisy image $x_t$ and the timestep $t$, and predicts the noise $\\epsilon$ that was added.\n",
    "\n",
    "We typically use a **U-Net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"A very simplified U-Net for demonstration\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Downsample\n",
    "        self.down1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.down2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        # Time embedding (simplified)\n",
    "        self.time_mlp = nn.Linear(1, 128)\n",
    "        \n",
    "        # Upsample\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 3, padding=1)\n",
    "        self.up2 = nn.ConvTranspose2d(64, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embed time\n",
    "        t = t.float().view(-1, 1)\n",
    "        t_emb = self.time_mlp(t).view(-1, 128, 1, 1)\n",
    "        \n",
    "        # Down\n",
    "        x1 = F.relu(self.down1(x))\n",
    "        x2 = F.relu(self.down2(x1))\n",
    "        \n",
    "        # Add time info\n",
    "        x2 = x2 + t_emb\n",
    "        \n",
    "        # Up\n",
    "        x = F.relu(self.up1(x2))\n",
    "        x = self.up2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleUNet()\n",
    "print(\"Model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "\n",
    "1. Sample a random image $x_0$.\n",
    "2. Sample a random timestep $t$.\n",
    "3. Add noise to get $x_t$.\n",
    "4. Model predicts the noise: $\\hat{\\epsilon} = \\text{Model}(x_t, t)$.\n",
    "5. Loss is MSE between real noise $\\epsilon$ and predicted noise $\\hat{\\epsilon}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Training Step\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 1. Get data\n",
    "x_0 = torch.randn(4, 3, 64, 64) # Batch of 4 images\n",
    "t = torch.randint(0, T, (4,))\n",
    "\n",
    "# 2. Forward diffusion\n",
    "x_t, noise = forward_diffusion_sample(x_0, t)\n",
    "\n",
    "# 3. Predict noise\n",
    "noise_pred = model(x_t, t)\n",
    "\n",
    "# 4. Loss\n",
    "loss = F.mse_loss(noise, noise_pred)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling (Generation)\n",
    "\n",
    "To generate an image:\n",
    "1. Start with pure noise $x_T$.\n",
    "2. Loop backwards from $T$ to $0$.\n",
    "3. At each step, remove a bit of noise using the model's prediction.\n",
    "\n",
    "*(Code omitted for brevity, but involves subtracting the predicted noise and adding a small amount of random noise back for stability)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Full DDPM Sampling Algorithm\n\nLet's implement the complete sampling (generation) loop."
  },
  {
   "cell_type": "code",
   "id": "cp38t6uzkfq",
   "source": "@torch.no_grad()\ndef ddpm_sample(model, image_shape, T, betas, alphas_cumprod, device=\"cpu\"):\n    \"\"\"\n    DDPM Sampling: Start from noise, iteratively denoise.\n    \n    Algorithm:\n    1. Sample x_T ~ N(0, I)\n    2. For t = T-1, T-2, ..., 0:\n       a. Predict noise: ε_θ(x_t, t)\n       b. Compute x_{t-1} using the reverse process formula\n    3. Return x_0\n    \"\"\"\n    batch_size = image_shape[0]\n    \n    # Precompute coefficients\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n    alphas = 1 - betas\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n    \n    # Posterior variance: how much noise to add back\n    posterior_variance = betas * (1 - alphas_cumprod[:-1]) / (1 - alphas_cumprod[1:])\n    posterior_variance = torch.cat([posterior_variance[:1], posterior_variance])\n    \n    # Start from pure noise\n    x_t = torch.randn(image_shape, device=device)\n    \n    # Iteratively denoise\n    for t in reversed(range(T)):\n        t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n        \n        # Predict noise\n        predicted_noise = model(x_t, t_batch)\n        \n        # Compute x_{t-1}\n        # Formula: x_{t-1} = 1/sqrt(α_t) * (x_t - β_t/sqrt(1-α̅_t) * ε_θ)\n        alpha_t = alphas[t]\n        alpha_cumprod_t = alphas_cumprod[t]\n        beta_t = betas[t]\n        \n        # Mean of p(x_{t-1} | x_t)\n        model_mean = sqrt_recip_alphas[t] * (\n            x_t - beta_t / sqrt_one_minus_alphas_cumprod[t] * predicted_noise\n        )\n        \n        if t > 0:\n            # Add noise (except for final step)\n            noise = torch.randn_like(x_t)\n            x_t = model_mean + torch.sqrt(posterior_variance[t]) * noise\n        else:\n            x_t = model_mean\n    \n    return x_t\n\n# Demo sampling (with untrained model - will be noise, but shows the algorithm)\nprint(\"Sampling from untrained model (will be random, but shows algorithm)...\")\nsampled = ddpm_sample(model, (4, 3, 64, 64), T, betas, alphas_cumprod)\nprint(f\"Sampled images shape: {sampled.shape}\")\n\n# Visualize\nplt.figure(figsize=(12, 3))\nfor i in range(4):\n    plt.subplot(1, 4, i+1)\n    plt.imshow(sampled[i].permute(1, 2, 0).clamp(0, 1).numpy())\n    plt.axis('off')\n    plt.title(f'Sample {i+1}')\nplt.suptitle('DDPM Samples (untrained model = noise)')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "g83k758dqlv",
   "source": "## 6. DDIM: Faster Sampling\n\nDDPM requires ~1000 steps to generate an image. **DDIM (Denoising Diffusion Implicit Models)** achieves comparable quality in 20-50 steps!\n\nKey insight: DDIM is a **deterministic** sampler (no random noise added during sampling).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f1gsw38bva4",
   "source": "@torch.no_grad()\ndef ddim_sample(model, image_shape, T, alphas_cumprod, num_inference_steps=50, device=\"cpu\"):\n    \"\"\"\n    DDIM Sampling: Deterministic, fewer steps needed.\n    \n    Key difference from DDPM:\n    - Uses a subset of timesteps (e.g., every 20th step)\n    - No stochastic noise during sampling\n    - Same trained model, different inference algorithm\n    \"\"\"\n    batch_size = image_shape[0]\n    \n    # Select a subset of timesteps\n    step_size = T // num_inference_steps\n    timesteps = list(range(0, T, step_size))\n    timesteps = list(reversed(timesteps))\n    \n    # Start from pure noise\n    x_t = torch.randn(image_shape, device=device)\n    \n    for i, t in enumerate(timesteps[:-1]):\n        t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n        \n        # Predict noise\n        predicted_noise = model(x_t, t_batch)\n        \n        # Get alpha values\n        alpha_t = alphas_cumprod[t]\n        alpha_t_prev = alphas_cumprod[timesteps[i + 1]] if i + 1 < len(timesteps) else 1.0\n        \n        # Predict x_0 from x_t\n        pred_x0 = (x_t - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n        \n        # Direction pointing to x_t\n        direction = torch.sqrt(1 - alpha_t_prev) * predicted_noise\n        \n        # DDIM update (deterministic!)\n        x_t = torch.sqrt(alpha_t_prev) * pred_x0 + direction\n    \n    return x_t\n\nprint(\"DDIM uses fewer steps:\")\nprint(f\"  DDPM: {T} steps required\")\nprint(f\"  DDIM: 50 steps (or fewer) for similar quality\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cv47ne6tb3",
   "source": "## 7. Classifier-Free Guidance (CFG)\n\nCFG is the secret sauce behind Stable Diffusion's quality. It allows **trading off diversity for quality**.\n\nThe idea: Amplify the difference between conditional and unconditional predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p1ou54mizn",
   "source": "class ConditionalUNet(nn.Module):\n    \"\"\"U-Net that can accept a text/class condition.\"\"\"\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.down1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.down2 = nn.Conv2d(64, 128, 3, padding=1)\n        \n        # Conditioning: embed class into same dimension\n        self.class_emb = nn.Embedding(num_classes + 1, 128)  # +1 for \"no class\" (unconditional)\n        self.time_mlp = nn.Linear(1, 128)\n        \n        self.up1 = nn.ConvTranspose2d(128, 64, 3, padding=1)\n        self.up2 = nn.ConvTranspose2d(64, 3, 3, padding=1)\n    \n    def forward(self, x, t, class_label=None):\n        # Time embedding\n        t = t.float().view(-1, 1)\n        t_emb = self.time_mlp(t).view(-1, 128, 1, 1)\n        \n        # Class embedding (use num_classes as \"null\" class for unconditional)\n        if class_label is None:\n            class_label = torch.full((x.shape[0],), 10, device=x.device)  # Null class\n        c_emb = self.class_emb(class_label).view(-1, 128, 1, 1)\n        \n        # Forward pass with conditioning\n        x1 = F.relu(self.down1(x))\n        x2 = F.relu(self.down2(x1))\n        x2 = x2 + t_emb + c_emb  # Add both time and class info\n        x = F.relu(self.up1(x2))\n        x = self.up2(x)\n        return x\n\ndef cfg_guided_noise(model, x_t, t, class_label, guidance_scale=7.5):\n    \"\"\"\n    Classifier-Free Guidance: Blend conditional and unconditional predictions.\n    \n    Formula: ε_guided = ε_uncond + w * (ε_cond - ε_uncond)\n    \n    Where w is the guidance scale:\n    - w = 1: No guidance (same as conditional)\n    - w = 7.5: Typical value (strong adherence to prompt)\n    - w > 10: Very strong guidance (may cause artifacts)\n    \"\"\"\n    # Get unconditional prediction (null class)\n    noise_uncond = model(x_t, t, class_label=None)\n    \n    # Get conditional prediction\n    noise_cond = model(x_t, t, class_label=class_label)\n    \n    # CFG formula\n    noise_guided = noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n    \n    return noise_guided\n\n# Demo\ncond_model = ConditionalUNet(num_classes=10)\nx = torch.randn(2, 3, 64, 64)\nt = torch.tensor([50, 100])\nlabels = torch.tensor([3, 7])  # Classes 3 and 7\n\nnoise_no_cfg = cond_model(x, t, labels)\nnoise_with_cfg = cfg_guided_noise(cond_model, x, t, labels, guidance_scale=7.5)\n\nprint(f\"Without CFG (guidance_scale=1): noise range [{noise_no_cfg.min():.2f}, {noise_no_cfg.max():.2f}]\")\nprint(f\"With CFG (guidance_scale=7.5): noise range [{noise_with_cfg.min():.2f}, {noise_with_cfg.max():.2f}]\")\nprint(\"\\nHigher guidance = stronger adherence to condition, but may cause oversaturation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fyfz9080gbh",
   "source": "## 8. Noise Schedules\n\nThe beta schedule significantly affects generation quality. Let's compare different schedules.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "c29a7rl3rwn",
   "source": "def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    return torch.linspace(beta_start, beta_end, timesteps)\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"Cosine schedule from 'Improved DDPM' paper.\"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clamp(betas, 0.0001, 0.9999)\n\ndef quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n\n# Compare schedules\nT = 1000\nschedules = {\n    'Linear': linear_beta_schedule(T),\n    'Cosine': cosine_beta_schedule(T),\n    'Quadratic': quadratic_beta_schedule(T)\n}\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# Plot betas\nfor name, betas in schedules.items():\n    axes[0].plot(betas.numpy(), label=name)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('β_t')\naxes[0].set_title('Beta Schedule')\naxes[0].legend()\n\n# Plot cumulative alpha (signal remaining)\nfor name, betas in schedules.items():\n    alphas_cumprod = torch.cumprod(1 - betas, dim=0)\n    axes[1].plot(alphas_cumprod.numpy(), label=name)\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('α̅_t (signal remaining)')\naxes[1].set_title('Signal Retention Over Time')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Cosine schedule (used by Improved DDPM):\")\nprint(\"  - Preserves signal longer at beginning\")\nprint(\"  - Smoother transition to noise\")\nprint(\"  - Better for high-resolution images\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "34khbojt9ms",
   "source": "## 9. FAANG Interview Questions\n\n### Q1: Explain the forward and reverse diffusion processes. Why does this work for generation?\n\n**Answer**:\n\n**Forward Process (Fixed)**:\n- Gradually adds Gaussian noise over T steps: $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$\n- By t=T, the image is pure noise\n- Key property: We can jump directly to any step: $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$\n\n**Reverse Process (Learned)**:\n- Neural network learns to predict the noise added at each step\n- Start from noise, iteratively remove predicted noise\n- $p_\\theta(x_{t-1}|x_t)$ approximates the true posterior\n\n**Why it works**:\n1. Each denoising step is a small, learnable transformation\n2. The objective (predict noise) is simple and stable\n3. The model learns the data distribution implicitly through noise prediction\n\n---\n\n### Q2: What is Classifier-Free Guidance and why is it important?\n\n**Answer**: CFG combines conditional and unconditional predictions to control generation quality.\n\n**Formula**: $\\epsilon_{guided} = \\epsilon_{uncond} + w \\cdot (\\epsilon_{cond} - \\epsilon_{uncond})$\n\n**Training**:\n- Randomly drop conditioning (e.g., set prompt to empty) with probability ~10%\n- Model learns both conditional and unconditional generation\n\n**Inference**:\n- w=1: Pure conditional (diverse but may not match prompt)\n- w=7.5: Typical (good balance)\n- w>10: Strong adherence (may cause artifacts)\n\n**Why important**: Allows users to control the diversity-quality tradeoff without retraining.\n\n---\n\n### Q3: Compare DDPM vs DDIM sampling. When would you use each?\n\n**Answer**:\n\n| Aspect | DDPM | DDIM |\n|--------|------|------|\n| **Steps** | 1000 (slow) | 20-50 (fast) |\n| **Stochastic** | Yes (adds noise each step) | No (deterministic) |\n| **Reproducible** | No (random each time) | Yes (same latent → same image) |\n| **Quality** | Best | Slightly lower |\n| **Use case** | Final production | Fast iteration, interpolation |\n\nKey insight: DDIM uses the same trained model but a different inference algorithm.\n\n---\n\n### Q4: How does Stable Diffusion achieve high-resolution generation efficiently?\n\n**Answer**: Stable Diffusion operates in **latent space**, not pixel space.\n\n**Architecture**:\n1. **VAE Encoder**: Compress 512x512 image → 64x64 latent (8x compression)\n2. **U-Net**: Denoise in latent space (much cheaper!)\n3. **VAE Decoder**: Decompress 64x64 latent → 512x512 image\n\n**Benefits**:\n- 8x smaller spatial dimensions = 64x fewer computations per attention layer\n- Latent space captures semantic information (easier to learn)\n- Same quality as pixel-space diffusion at fraction of compute\n\n**Cost**: Slight loss in fine details (VAE reconstruction isn't perfect)\n\n---\n\n### Q5: What are the main failure modes of diffusion models and how do you address them?\n\n**Answer**:\n\n| Failure | Cause | Solution |\n|---------|-------|----------|\n| **Mode collapse** | Not enough diversity in training | More data, lower guidance scale |\n| **Artifacts** | High CFG, few steps | Lower guidance (5-8), more steps |\n| **Poor text following** | Weak text encoder | Better CLIP, T5 encoder |\n| **Slow generation** | Many diffusion steps | DDIM, distillation, consistency models |\n| **Out-of-distribution** | Unusual prompts | Fine-tune on domain data |\n| **Composition failures** | \"A red cube on blue sphere\" | Better attention, layout guidance |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9rrtpeb7sy",
   "source": "## Key Takeaways\n\n1. **Diffusion**: A process of slowly destroying data with noise, then learning to reverse it.\n2. **Forward Process**: Fixed mathematical formula (no learning) - adds noise.\n3. **Reverse Process**: Learned by a U-Net - predicts and removes noise.\n4. **Objective**: Predict the noise that was added (simple MSE loss).\n5. **DDIM**: Faster, deterministic sampling using the same trained model.\n6. **CFG**: Trade off diversity for quality by amplifying conditional predictions.\n7. **Noise Schedules**: Cosine schedule preserves signal longer, better for high-res.\n8. **Latent Diffusion**: Operate in compressed latent space for efficiency (Stable Diffusion).",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}