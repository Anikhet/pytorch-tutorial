{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Audio and Speech Processing\n",
    "\n",
    "Audio is a time-series signal that can be processed with neural networks for tasks like speech recognition, music generation, and audio classification. This notebook covers the fundamentals of audio feature extraction and deep learning for audio.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand audio fundamentals (waveforms, sampling, frequency)\n",
    "- Extract spectrograms and mel-spectrograms\n",
    "- Compute MFCCs (Mel-Frequency Cepstral Coefficients)\n",
    "- Build audio classifiers with CNNs\n",
    "- Understand sequence models for speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torchaudio for audio processing\n",
    "try:\n",
    "    import torchaudio\n",
    "    import torchaudio.transforms as T\n",
    "    import torchaudio.functional as AF\n",
    "    TORCHAUDIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"torchaudio not available - install with: pip install torchaudio\")\n",
    "    TORCHAUDIO_AVAILABLE = False\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio Fundamentals\n",
    "\n",
    "Audio is a continuous pressure wave that we digitize by **sampling** at regular intervals.\n",
    "\n",
    "Key concepts:\n",
    "- **Sample rate**: How many samples per second (e.g., 16000 Hz, 44100 Hz)\n",
    "- **Nyquist frequency**: Highest frequency we can capture = sample_rate / 2\n",
    "- **Bit depth**: Precision of each sample (e.g., 16-bit, 32-bit float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a synthetic audio signal\n",
    "sample_rate = 16000  # 16 kHz - common for speech\n",
    "duration = 1.0  # 1 second\n",
    "t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "\n",
    "# Create a signal with multiple frequencies (like speech)\n",
    "# Fundamental frequency + harmonics\n",
    "f0 = 220  # A3 note\n",
    "signal = (\n",
    "    0.5 * np.sin(2 * np.pi * f0 * t) +       # Fundamental\n",
    "    0.25 * np.sin(2 * np.pi * 2 * f0 * t) +  # 2nd harmonic\n",
    "    0.125 * np.sin(2 * np.pi * 3 * f0 * t)   # 3rd harmonic\n",
    ")\n",
    "\n",
    "# Add some amplitude envelope (like speech)\n",
    "envelope = np.exp(-2 * t) * (1 - np.exp(-20 * t))\n",
    "signal = signal * envelope\n",
    "\n",
    "# Convert to tensor\n",
    "waveform = torch.tensor(signal, dtype=torch.float32).unsqueeze(0)  # [1, samples]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Full waveform\n",
    "axes[0].plot(t, signal)\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Audio Waveform (1 second)')\n",
    "\n",
    "# Zoomed view\n",
    "zoom_samples = 500\n",
    "axes[1].plot(t[:zoom_samples], signal[:zoom_samples])\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title(f'Zoomed View (first {zoom_samples} samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {duration} s\")\n",
    "print(f\"Total samples: {len(signal)}\")\n",
    "print(f\"Nyquist frequency: {sample_rate // 2} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectrograms\n",
    "\n",
    "A spectrogram shows frequency content over time. It's created using the **Short-Time Fourier Transform (STFT)**:\n",
    "1. Divide signal into overlapping windows\n",
    "2. Apply FFT to each window\n",
    "3. Stack the results\n",
    "\n",
    "Result: 2D representation [frequency x time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT parameters\n",
    "n_fft = 512       # FFT window size\n",
    "hop_length = 128  # Step between windows\n",
    "win_length = 512  # Window length\n",
    "\n",
    "if TORCHAUDIO_AVAILABLE:\n",
    "    # Using torchaudio\n",
    "    spectrogram_transform = T.Spectrogram(\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        power=2.0,  # Power spectrogram\n",
    "    )\n",
    "    spectrogram = spectrogram_transform(waveform)\n",
    "else:\n",
    "    # Manual STFT using torch\n",
    "    window = torch.hann_window(n_fft)\n",
    "    stft = torch.stft(waveform, n_fft=n_fft, hop_length=hop_length, \n",
    "                      win_length=win_length, window=window, return_complex=True)\n",
    "    spectrogram = torch.abs(stft) ** 2\n",
    "\n",
    "print(f\"Spectrogram shape: {spectrogram.shape}\")\n",
    "print(f\"  - Channels: {spectrogram.shape[0]}\")\n",
    "print(f\"  - Frequency bins: {spectrogram.shape[1]}\")\n",
    "print(f\"  - Time frames: {spectrogram.shape[2]}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(\n",
    "    10 * torch.log10(spectrogram[0] + 1e-10).numpy(),\n",
    "    aspect='auto',\n",
    "    origin='lower',\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(label='Power (dB)')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('Frequency Bin')\n",
    "plt.title('Spectrogram (Power)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mel-Spectrograms\n",
    "\n",
    "The **mel scale** approximates human hearing perception - we're more sensitive to differences in lower frequencies.\n",
    "\n",
    "Mel-spectrograms:\n",
    "1. Compute power spectrogram\n",
    "2. Apply mel filterbank (triangular filters spaced on mel scale)\n",
    "3. Take log for better dynamic range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel scale conversion\n",
    "def hz_to_mel(hz):\n",
    "    return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "# Visualize mel scale\n",
    "hz_range = np.linspace(0, 8000, 100)\n",
    "mel_range = hz_to_mel(hz_range)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(hz_range, mel_range)\n",
    "axes[0].set_xlabel('Frequency (Hz)')\n",
    "axes[0].set_ylabel('Mel')\n",
    "axes[0].set_title('Hz to Mel Scale')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Show mel filterbank spacing\n",
    "n_mels = 40\n",
    "mel_points = np.linspace(hz_to_mel(0), hz_to_mel(8000), n_mels + 2)\n",
    "hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "axes[1].bar(range(len(hz_points)), hz_points, width=0.8)\n",
    "axes[1].set_xlabel('Mel Filter Index')\n",
    "axes[1].set_ylabel('Center Frequency (Hz)')\n",
    "axes[1].set_title('Mel Filterbank Center Frequencies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how mel filters are:\")\n",
    "print(\"- Closely spaced at low frequencies (where we're sensitive)\")\n",
    "print(\"- Widely spaced at high frequencies (where we're less sensitive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mel-spectrogram\n",
    "n_mels = 64\n",
    "\n",
    "if TORCHAUDIO_AVAILABLE:\n",
    "    mel_spectrogram_transform = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=sample_rate // 2,\n",
    "    )\n",
    "    mel_spec = mel_spectrogram_transform(waveform)\n",
    "else:\n",
    "    # Manual mel filterbank (simplified)\n",
    "    def create_mel_filterbank(n_fft, n_mels, sample_rate):\n",
    "        n_freqs = n_fft // 2 + 1\n",
    "        mel_min = hz_to_mel(0)\n",
    "        mel_max = hz_to_mel(sample_rate / 2)\n",
    "        mel_points = np.linspace(mel_min, mel_max, n_mels + 2)\n",
    "        hz_points = mel_to_hz(mel_points)\n",
    "        bin_points = np.floor((n_fft + 1) * hz_points / sample_rate).astype(int)\n",
    "        \n",
    "        filterbank = np.zeros((n_mels, n_freqs))\n",
    "        for i in range(n_mels):\n",
    "            for j in range(bin_points[i], bin_points[i+1]):\n",
    "                filterbank[i, j] = (j - bin_points[i]) / (bin_points[i+1] - bin_points[i])\n",
    "            for j in range(bin_points[i+1], bin_points[i+2]):\n",
    "                filterbank[i, j] = (bin_points[i+2] - j) / (bin_points[i+2] - bin_points[i+1])\n",
    "        return torch.tensor(filterbank, dtype=torch.float32)\n",
    "    \n",
    "    filterbank = create_mel_filterbank(n_fft, n_mels, sample_rate)\n",
    "    mel_spec = torch.matmul(filterbank, spectrogram[0])\n",
    "    mel_spec = mel_spec.unsqueeze(0)\n",
    "\n",
    "# Convert to log scale (dB)\n",
    "log_mel_spec = 10 * torch.log10(mel_spec + 1e-10)\n",
    "\n",
    "print(f\"Mel-spectrogram shape: {mel_spec.shape}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(\n",
    "    log_mel_spec[0].numpy(),\n",
    "    aspect='auto',\n",
    "    origin='lower',\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(label='Log Power (dB)')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('Mel Frequency Bin')\n",
    "plt.title('Log Mel-Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MFCCs (Mel-Frequency Cepstral Coefficients)\n",
    "\n",
    "MFCCs are a compact representation of the spectral envelope:\n",
    "1. Compute log mel-spectrogram\n",
    "2. Apply Discrete Cosine Transform (DCT)\n",
    "3. Keep first N coefficients (typically 13-40)\n",
    "\n",
    "MFCCs capture the \"shape\" of the spectrum, useful for speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 13\n",
    "\n",
    "if TORCHAUDIO_AVAILABLE:\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\n",
    "            'n_fft': n_fft,\n",
    "            'hop_length': hop_length,\n",
    "            'n_mels': n_mels,\n",
    "        }\n",
    "    )\n",
    "    mfccs = mfcc_transform(waveform)\n",
    "else:\n",
    "    # Manual DCT (Type-II)\n",
    "    def dct_matrix(n_mfcc, n_mels):\n",
    "        basis = np.zeros((n_mfcc, n_mels))\n",
    "        for k in range(n_mfcc):\n",
    "            for n in range(n_mels):\n",
    "                basis[k, n] = np.cos(np.pi * k * (2*n + 1) / (2 * n_mels))\n",
    "        basis[0, :] *= np.sqrt(1 / n_mels)\n",
    "        basis[1:, :] *= np.sqrt(2 / n_mels)\n",
    "        return torch.tensor(basis, dtype=torch.float32)\n",
    "    \n",
    "    dct = dct_matrix(n_mfcc, n_mels)\n",
    "    mfccs = torch.matmul(dct, log_mel_spec[0])\n",
    "    mfccs = mfccs.unsqueeze(0)\n",
    "\n",
    "print(f\"MFCC shape: {mfccs.shape}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(\n",
    "    mfccs[0].numpy(),\n",
    "    aspect='auto',\n",
    "    origin='lower',\n",
    "    cmap='coolwarm'\n",
    ")\n",
    "plt.colorbar(label='MFCC Value')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('MFCC Coefficient')\n",
    "plt.title('MFCCs')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMFCC coefficients:\")\n",
    "print(\"- MFCC 0: Overall energy\")\n",
    "print(\"- MFCC 1-12: Spectral envelope shape\")\n",
    "print(\"- Higher coefficients: Finer spectral details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Delta and Delta-Delta Features\n",
    "\n",
    "Adding temporal derivatives helps capture dynamics:\n",
    "- **Delta**: First derivative (velocity of change)\n",
    "- **Delta-delta**: Second derivative (acceleration of change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(features, width=2):\n",
    "    \"\"\"\n",
    "    Compute delta features using finite differences.\n",
    "    features: [channels, n_features, time]\n",
    "    \"\"\"\n",
    "    # Pad features\n",
    "    padded = F.pad(features, (width, width), mode='replicate')\n",
    "    \n",
    "    # Compute weighted difference\n",
    "    denominator = 2 * sum(i**2 for i in range(1, width + 1))\n",
    "    delta = torch.zeros_like(features)\n",
    "    \n",
    "    for i in range(1, width + 1):\n",
    "        delta += i * (padded[:, :, width + i:width + i + features.shape[2]] - \n",
    "                     padded[:, :, width - i:width - i + features.shape[2]])\n",
    "    \n",
    "    return delta / denominator\n",
    "\n",
    "# Compute deltas\n",
    "delta = compute_deltas(mfccs)\n",
    "delta_delta = compute_deltas(delta)\n",
    "\n",
    "# Concatenate all features\n",
    "full_features = torch.cat([mfccs, delta, delta_delta], dim=1)\n",
    "print(f\"Full feature shape (MFCC + delta + delta-delta): {full_features.shape}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "for i, (feat, name) in enumerate([(mfccs, 'MFCC'), (delta, 'Delta'), (delta_delta, 'Delta-Delta')]):\n",
    "    im = axes[i].imshow(feat[0].numpy(), aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    axes[i].set_ylabel(f'{name} Coef')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "axes[-1].set_xlabel('Time Frame')\n",
    "plt.suptitle('MFCC with Delta Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Audio Classification with CNNs\n",
    "\n",
    "We can treat spectrograms as images and use CNNs for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for audio classification using mel-spectrograms.\n",
    "    Input: [batch, 1, n_mels, time]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels: int = 64, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),  # Fixed output size\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, n_mels, time] or [batch, 1, n_mels, time]\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "model = AudioCNN(n_mels=64, num_classes=10)\n",
    "test_input = torch.randn(4, 1, 64, 100)  # [batch, channel, n_mels, time]\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequence Models for Speech\n",
    "\n",
    "For tasks like speech recognition, we need to model temporal dependencies. RNNs and Transformers work well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for sequence-to-sequence audio tasks.\n",
    "    Input: [batch, time, features]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 256, num_layers: int = 2, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        # Output layer (for frame-level predictions)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, time, features]\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, time, hidden*2]\n",
    "        output = self.fc(lstm_out)   # [batch, time, num_classes]\n",
    "        return output\n",
    "\n",
    "# Test\n",
    "rnn_model = AudioRNN(input_dim=64, hidden_dim=256, num_classes=29)  # 26 letters + space + blank + unknown\n",
    "test_input = torch.randn(4, 100, 64)  # [batch, time, features]\n",
    "output = rnn_model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder for audio classification.\n",
    "    Input: [batch, time, features]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, d_model: int = 256, nhead: int = 8, \n",
    "                 num_layers: int = 4, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1000, d_model) * 0.1)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, time, features]\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global average pooling for classification\n",
    "        x = x.mean(dim=1)  # [batch, d_model]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "transformer_model = AudioTransformer(input_dim=64, d_model=256, num_classes=10)\n",
    "test_input = torch.randn(4, 100, 64)\n",
    "output = transformer_model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Audio Data Augmentation\n",
    "\n",
    "Augmentation improves model robustness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentation:\n",
    "    \"\"\"Common audio augmentation techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(waveform: torch.Tensor, noise_level: float = 0.005) -> torch.Tensor:\n",
    "        \"\"\"Add Gaussian noise.\"\"\"\n",
    "        noise = torch.randn_like(waveform) * noise_level\n",
    "        return waveform + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(waveform: torch.Tensor, shift_ratio: float = 0.1) -> torch.Tensor:\n",
    "        \"\"\"Shift audio in time.\"\"\"\n",
    "        shift = int(waveform.shape[-1] * shift_ratio * (2 * torch.rand(1).item() - 1))\n",
    "        return torch.roll(waveform, shifts=shift, dims=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_stretch(waveform: torch.Tensor, rate: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"Time stretch (simplified - just resampling).\"\"\"\n",
    "        new_length = int(waveform.shape[-1] / rate)\n",
    "        return F.interpolate(waveform.unsqueeze(0), size=new_length, mode='linear').squeeze(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spec_augment(spec: torch.Tensor, freq_mask_param: int = 10, time_mask_param: int = 20) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SpecAugment: Mask random frequency and time bands.\n",
    "        spec: [channels, freq, time]\n",
    "        \"\"\"\n",
    "        spec = spec.clone()\n",
    "        _, n_freq, n_time = spec.shape\n",
    "        \n",
    "        # Frequency masking\n",
    "        f = torch.randint(0, freq_mask_param, (1,)).item()\n",
    "        f0 = torch.randint(0, max(1, n_freq - f), (1,)).item()\n",
    "        spec[:, f0:f0+f, :] = 0\n",
    "        \n",
    "        # Time masking\n",
    "        t = torch.randint(0, time_mask_param, (1,)).item()\n",
    "        t0 = torch.randint(0, max(1, n_time - t), (1,)).item()\n",
    "        spec[:, :, t0:t0+t] = 0\n",
    "        \n",
    "        return spec\n",
    "\n",
    "# Demonstrate augmentations\n",
    "aug = AudioAugmentation()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].plot(waveform[0].numpy())\n",
    "axes[0, 0].set_title('Original')\n",
    "\n",
    "# With noise\n",
    "noisy = aug.add_noise(waveform, noise_level=0.02)\n",
    "axes[0, 1].plot(noisy[0].numpy())\n",
    "axes[0, 1].set_title('With Noise')\n",
    "\n",
    "# Time shifted\n",
    "shifted = aug.time_shift(waveform, shift_ratio=0.2)\n",
    "axes[1, 0].plot(shifted[0].numpy())\n",
    "axes[1, 0].set_title('Time Shifted')\n",
    "\n",
    "# SpecAugment on mel-spectrogram\n",
    "augmented_spec = aug.spec_augment(log_mel_spec, freq_mask_param=15, time_mask_param=30)\n",
    "axes[1, 1].imshow(augmented_spec[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 1].set_title('SpecAugment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. FAANG Interview Questions\n",
    "\n",
    "### Q1: What is a mel-spectrogram and why is it used for audio?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A mel-spectrogram is a spectrogram where frequencies are mapped to the **mel scale**, which approximates human hearing perception.\n",
    "\n",
    "**Construction**:\n",
    "1. Compute STFT (Short-Time Fourier Transform)\n",
    "2. Apply mel filterbank (triangular filters)\n",
    "3. Take log of power for better dynamic range\n",
    "\n",
    "**Why mel scale**:\n",
    "- Humans perceive pitch logarithmically\n",
    "- More sensitive to low frequency differences\n",
    "- Matches how cochlea processes sound\n",
    "\n",
    "**Advantages**:\n",
    "- Compact representation (fewer bins than full spectrogram)\n",
    "- Perceptually meaningful\n",
    "- Works well with CNNs (treat as image)\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: What are MFCCs and why are they useful?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**MFCCs** (Mel-Frequency Cepstral Coefficients) capture the spectral envelope of audio.\n",
    "\n",
    "**Computation**:\n",
    "1. Compute log mel-spectrogram\n",
    "2. Apply DCT (Discrete Cosine Transform)\n",
    "3. Keep first N coefficients (typically 13-40)\n",
    "\n",
    "**What they represent**:\n",
    "- MFCC 0: Overall energy\n",
    "- Lower coefficients: Coarse spectral shape (formants)\n",
    "- Higher coefficients: Fine spectral details\n",
    "\n",
    "**Why useful**:\n",
    "- Compact representation (~13 features per frame)\n",
    "- Decorrelated features (DCT removes correlation)\n",
    "- Capture vocal tract shape (important for speech)\n",
    "- Robust to pitch variations\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: Explain the Nyquist theorem and aliasing.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Nyquist theorem**: To accurately capture a signal, sample rate must be at least 2x the highest frequency component.\n",
    "\n",
    "**Nyquist frequency** = sample_rate / 2\n",
    "\n",
    "**Aliasing**: When frequencies above Nyquist are present, they \"fold back\" and appear as lower frequencies (distortion).\n",
    "\n",
    "**Example**:\n",
    "- Sample rate: 16 kHz\n",
    "- Nyquist: 8 kHz\n",
    "- A 10 kHz tone would alias to 6 kHz\n",
    "\n",
    "**Prevention**: Apply low-pass (anti-aliasing) filter before sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: What is SpecAugment and why does it help?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**SpecAugment** is a data augmentation technique that masks random regions of spectrograms:\n",
    "\n",
    "1. **Time masking**: Zero out random time bands\n",
    "2. **Frequency masking**: Zero out random frequency bands\n",
    "3. (Optional) **Time warping**: Warp spectrogram along time axis\n",
    "\n",
    "**Why it helps**:\n",
    "- Forces model to use all parts of spectrogram\n",
    "- Prevents overfitting to specific patterns\n",
    "- Simulates real-world conditions (noise, occlusion)\n",
    "- Simple yet very effective\n",
    "\n",
    "**Results**: Significant improvements in ASR (automatic speech recognition) without additional data.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: Compare CNN vs RNN vs Transformer for audio.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "| Aspect | CNN | RNN/LSTM | Transformer |\n",
    "|--------|-----|----------|-------------|\n",
    "| **Input** | 2D spectrogram | 1D sequence | 1D sequence |\n",
    "| **Local patterns** | Excellent | Good | Via attention |\n",
    "| **Long-range** | Limited | Good (with LSTM) | Excellent |\n",
    "| **Parallelization** | Excellent | Poor | Excellent |\n",
    "| **Memory** | Low | High | High |\n",
    "| **Best for** | Classification | Seq2seq (ASR) | Everything (modern) |\n",
    "\n",
    "**Modern approach**: Hybrid architectures\n",
    "- CNN for local feature extraction\n",
    "- Transformer for sequence modeling\n",
    "- Examples: Wav2Vec, Whisper, Conformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "1. **Audio is a time-series** that we digitize by sampling at regular intervals\n",
    "2. **Spectrograms** show frequency content over time (STFT)\n",
    "3. **Mel scale** approximates human hearing - more resolution at low frequencies\n",
    "4. **MFCCs** capture spectral envelope, useful for speech recognition\n",
    "5. **Delta features** add temporal dynamics (velocity, acceleration)\n",
    "6. **CNNs** work well on spectrograms (treat as images)\n",
    "7. **RNNs/Transformers** model temporal sequences for ASR\n",
    "8. **SpecAugment** is a simple but effective augmentation technique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
