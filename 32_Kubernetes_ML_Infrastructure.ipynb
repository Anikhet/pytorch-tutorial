{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes ML Infrastructure\n",
    "\n",
    "This notebook covers production ML infrastructure on Kubernetes - essential for FAANG-level ML engineering.\n",
    "\n",
    "## Topics Covered\n",
    "1. **KServe** - Serverless model inference on Kubernetes\n",
    "2. **Ray Serve** - Distributed model serving\n",
    "3. **Kubeflow Pipelines** - ML workflow orchestration\n",
    "4. **GPU Scheduling** - Resource management for ML workloads\n",
    "5. **Auto-scaling** - Scaling patterns for ML services\n",
    "6. **Helm Charts** - Packaging ML deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KServe - Serverless Model Inference\n",
    "\n",
    "KServe provides serverless inference on Kubernetes with auto-scaling, canary deployments, and multi-framework support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KServeInferenceService:\n",
    "    \"\"\"\n",
    "    Represents a KServe InferenceService configuration.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    namespace: str\n",
    "    framework: str  # pytorch, tensorflow, sklearn, xgboost, custom\n",
    "    storage_uri: str\n",
    "    runtime_version: str = \"latest\"\n",
    "    \n",
    "    # Resource configuration\n",
    "    min_replicas: int = 1\n",
    "    max_replicas: int = 10\n",
    "    target_utilization: int = 70\n",
    "    \n",
    "    # Container resources\n",
    "    cpu_request: str = \"1\"\n",
    "    cpu_limit: str = \"2\"\n",
    "    memory_request: str = \"2Gi\"\n",
    "    memory_limit: str = \"4Gi\"\n",
    "    gpu_count: int = 0\n",
    "    \n",
    "    # Canary configuration\n",
    "    canary_traffic_percent: int = 0\n",
    "    \n",
    "    def to_yaml(self) -> str:\n",
    "        \"\"\"Generate KServe InferenceService YAML\"\"\"\n",
    "        spec = {\n",
    "            \"apiVersion\": \"serving.kserve.io/v1beta1\",\n",
    "            \"kind\": \"InferenceService\",\n",
    "            \"metadata\": {\n",
    "                \"name\": self.name,\n",
    "                \"namespace\": self.namespace,\n",
    "                \"annotations\": {\n",
    "                    \"autoscaling.knative.dev/minScale\": str(self.min_replicas),\n",
    "                    \"autoscaling.knative.dev/maxScale\": str(self.max_replicas),\n",
    "                    \"autoscaling.knative.dev/target\": str(self.target_utilization)\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"predictor\": {\n",
    "                    self.framework: {\n",
    "                        \"storageUri\": self.storage_uri,\n",
    "                        \"runtimeVersion\": self.runtime_version,\n",
    "                        \"resources\": {\n",
    "                            \"requests\": {\n",
    "                                \"cpu\": self.cpu_request,\n",
    "                                \"memory\": self.memory_request\n",
    "                            },\n",
    "                            \"limits\": {\n",
    "                                \"cpu\": self.cpu_limit,\n",
    "                                \"memory\": self.memory_limit\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add GPU if requested\n",
    "        if self.gpu_count > 0:\n",
    "            spec[\"spec\"][\"predictor\"][self.framework][\"resources\"][\"limits\"][\"nvidia.com/gpu\"] = str(self.gpu_count)\n",
    "        \n",
    "        return yaml.dump(spec, default_flow_style=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KServeTransformer:\n",
    "    \"\"\"\n",
    "    Pre/post processing transformer for KServe.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    image: str\n",
    "    \n",
    "    def to_spec(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"transformer\": {\n",
    "                \"containers\": [{\n",
    "                    \"name\": self.name,\n",
    "                    \"image\": self.image,\n",
    "                    \"resources\": {\n",
    "                        \"requests\": {\"cpu\": \"100m\", \"memory\": \"256Mi\"},\n",
    "                        \"limits\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"}\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Create KServe InferenceService\n",
    "pytorch_service = KServeInferenceService(\n",
    "    name=\"bert-classifier\",\n",
    "    namespace=\"ml-production\",\n",
    "    framework=\"pytorch\",\n",
    "    storage_uri=\"s3://models/bert-classifier/v1\",\n",
    "    min_replicas=2,\n",
    "    max_replicas=20,\n",
    "    gpu_count=1,\n",
    "    memory_request=\"4Gi\",\n",
    "    memory_limit=\"8Gi\"\n",
    ")\n",
    "\n",
    "print(\"KServe InferenceService YAML:\")\n",
    "print(pytorch_service.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KServeCanaryDeployment:\n",
    "    \"\"\"\n",
    "    Manages canary deployments for KServe InferenceServices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        service_name: str,\n",
    "        namespace: str = \"default\"\n",
    "    ):\n",
    "        self.service_name = service_name\n",
    "        self.namespace = namespace\n",
    "        self.deployment_history: List[Dict] = []\n",
    "    \n",
    "    def create_canary_spec(\n",
    "        self,\n",
    "        default_storage_uri: str,\n",
    "        canary_storage_uri: str,\n",
    "        canary_traffic_percent: int = 10,\n",
    "        framework: str = \"pytorch\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create a canary deployment specification\"\"\"\n",
    "        return {\n",
    "            \"apiVersion\": \"serving.kserve.io/v1beta1\",\n",
    "            \"kind\": \"InferenceService\",\n",
    "            \"metadata\": {\n",
    "                \"name\": self.service_name,\n",
    "                \"namespace\": self.namespace\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"predictor\": {\n",
    "                    framework: {\n",
    "                        \"storageUri\": default_storage_uri\n",
    "                    }\n",
    "                },\n",
    "                \"canaryTrafficPercent\": canary_traffic_percent,\n",
    "                \"canaryPredictor\": {\n",
    "                    framework: {\n",
    "                        \"storageUri\": canary_storage_uri\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def progressive_rollout(\n",
    "        self,\n",
    "        stages: List[int] = [10, 25, 50, 75, 100]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generate progressive rollout stages.\n",
    "        Each stage increases canary traffic.\n",
    "        \"\"\"\n",
    "        rollout_plan = []\n",
    "        \n",
    "        for percentage in stages:\n",
    "            stage = {\n",
    "                \"traffic_percent\": percentage,\n",
    "                \"duration_minutes\": 15 if percentage < 100 else 0,\n",
    "                \"success_criteria\": {\n",
    "                    \"error_rate_threshold\": 0.01,\n",
    "                    \"latency_p99_threshold_ms\": 500\n",
    "                },\n",
    "                \"rollback_on_failure\": True\n",
    "            }\n",
    "            rollout_plan.append(stage)\n",
    "        \n",
    "        return rollout_plan\n",
    "\n",
    "\n",
    "# Example canary deployment\n",
    "canary = KServeCanaryDeployment(\"bert-classifier\", \"ml-production\")\n",
    "\n",
    "canary_spec = canary.create_canary_spec(\n",
    "    default_storage_uri=\"s3://models/bert-classifier/v1\",\n",
    "    canary_storage_uri=\"s3://models/bert-classifier/v2\",\n",
    "    canary_traffic_percent=10\n",
    ")\n",
    "\n",
    "print(\"Canary Deployment Spec:\")\n",
    "print(yaml.dump(canary_spec, default_flow_style=False))\n",
    "\n",
    "print(\"\\nProgressive Rollout Plan:\")\n",
    "for stage in canary.progressive_rollout():\n",
    "    print(f\"  - {stage['traffic_percent']}% traffic for {stage['duration_minutes']} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ray Serve - Distributed Model Serving\n",
    "\n",
    "Ray Serve enables distributed, scalable model serving with Python-native APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayServeDeployment:\n",
    "    \"\"\"\n",
    "    Simulates Ray Serve deployment configuration.\n",
    "    \n",
    "    In production, this would use:\n",
    "    from ray import serve\n",
    "    @serve.deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        num_replicas: int = 1,\n",
    "        max_concurrent_queries: int = 100,\n",
    "        ray_actor_options: Dict[str, Any] = None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.num_replicas = num_replicas\n",
    "        self.max_concurrent_queries = max_concurrent_queries\n",
    "        self.ray_actor_options = ray_actor_options or {}\n",
    "    \n",
    "    def to_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate Ray Serve deployment config\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"num_replicas\": self.num_replicas,\n",
    "            \"max_concurrent_queries\": self.max_concurrent_queries,\n",
    "            \"ray_actor_options\": self.ray_actor_options,\n",
    "            \"autoscaling_config\": {\n",
    "                \"min_replicas\": 1,\n",
    "                \"max_replicas\": 10,\n",
    "                \"target_num_ongoing_requests_per_replica\": 10\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class RayServeModelComposition:\n",
    "    \"\"\"\n",
    "    Demonstrates Ray Serve model composition patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.deployments: Dict[str, RayServeDeployment] = {}\n",
    "        self.dag_edges: List[Tuple[str, str]] = []\n",
    "    \n",
    "    def add_deployment(self, deployment: RayServeDeployment) -> None:\n",
    "        \"\"\"Add a deployment to the composition\"\"\"\n",
    "        self.deployments[deployment.name] = deployment\n",
    "    \n",
    "    def add_edge(self, from_deployment: str, to_deployment: str) -> None:\n",
    "        \"\"\"Add a DAG edge between deployments\"\"\"\n",
    "        self.dag_edges.append((from_deployment, to_deployment))\n",
    "    \n",
    "    def to_serve_dag(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate Ray Serve DAG configuration\"\"\"\n",
    "        return {\n",
    "            \"deployments\": {\n",
    "                name: dep.to_config() \n",
    "                for name, dep in self.deployments.items()\n",
    "            },\n",
    "            \"dag\": {\n",
    "                \"edges\": self.dag_edges\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Ray Serve ML Pipeline\n",
    "composition = RayServeModelComposition()\n",
    "\n",
    "# Add deployments\n",
    "composition.add_deployment(RayServeDeployment(\n",
    "    name=\"preprocessor\",\n",
    "    num_replicas=2,\n",
    "    ray_actor_options={\"num_cpus\": 1}\n",
    "))\n",
    "\n",
    "composition.add_deployment(RayServeDeployment(\n",
    "    name=\"encoder\",\n",
    "    num_replicas=4,\n",
    "    ray_actor_options={\"num_gpus\": 1}\n",
    "))\n",
    "\n",
    "composition.add_deployment(RayServeDeployment(\n",
    "    name=\"classifier\",\n",
    "    num_replicas=2,\n",
    "    ray_actor_options={\"num_gpus\": 0.5}\n",
    "))\n",
    "\n",
    "# Define DAG\n",
    "composition.add_edge(\"preprocessor\", \"encoder\")\n",
    "composition.add_edge(\"encoder\", \"classifier\")\n",
    "\n",
    "print(\"Ray Serve DAG Configuration:\")\n",
    "print(json.dumps(composition.to_serve_dag(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Ray Serve deployment class (production would use @serve.deployment)\n",
    "class SimulatedRayServeModel:\n",
    "    \"\"\"\n",
    "    Simulates a Ray Serve model deployment.\n",
    "    \n",
    "    Production code would look like:\n",
    "    \n",
    "    @serve.deployment(\n",
    "        num_replicas=2,\n",
    "        ray_actor_options={\"num_gpus\": 1}\n",
    "    )\n",
    "    class BertClassifier:\n",
    "        def __init__(self):\n",
    "            self.model = load_model()\n",
    "        \n",
    "        async def __call__(self, request):\n",
    "            return self.model(request.json())\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = nn.Linear(768, 2)  # Simulated model\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def __call__(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle inference request\"\"\"\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Simulate model inference\n",
    "        batch_size = inputs.get(\"batch_size\", 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(batch_size, 768)\n",
    "            output = self.model(dummy_input)\n",
    "            predictions = torch.softmax(output, dim=-1)\n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions.tolist(),\n",
    "            \"model\": self.model_name,\n",
    "            \"request_id\": self.request_count\n",
    "        }\n",
    "\n",
    "\n",
    "# Test simulated deployment\n",
    "model = SimulatedRayServeModel(\"bert-classifier\")\n",
    "result = model({\"batch_size\": 4})\n",
    "print(f\"Inference result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kubeflow Pipelines\n",
    "\n",
    "Kubeflow Pipelines enables ML workflow orchestration on Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineComponent:\n",
    "    \"\"\"\n",
    "    Represents a Kubeflow Pipeline component.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    image: str\n",
    "    command: List[str]\n",
    "    arguments: List[str] = field(default_factory=list)\n",
    "    \n",
    "    # Resource requirements\n",
    "    cpu_request: str = \"1\"\n",
    "    memory_request: str = \"2Gi\"\n",
    "    gpu_limit: int = 0\n",
    "    \n",
    "    # Inputs/Outputs\n",
    "    inputs: Dict[str, str] = field(default_factory=dict)\n",
    "    outputs: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    def to_component_spec(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate component specification\"\"\"\n",
    "        spec = {\n",
    "            \"name\": self.name,\n",
    "            \"implementation\": {\n",
    "                \"container\": {\n",
    "                    \"image\": self.image,\n",
    "                    \"command\": self.command,\n",
    "                    \"args\": self.arguments\n",
    "                }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "                {\"name\": k, \"type\": v} for k, v in self.inputs.items()\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\"name\": k, \"type\": v} for k, v in self.outputs.items()\n",
    "            ]\n",
    "        }\n",
    "        return spec\n",
    "\n",
    "\n",
    "class KubeflowPipeline:\n",
    "    \"\"\"\n",
    "    Builds Kubeflow ML pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.components: List[PipelineComponent] = []\n",
    "        self.dependencies: Dict[str, List[str]] = {}\n",
    "    \n",
    "    def add_component(\n",
    "        self,\n",
    "        component: PipelineComponent,\n",
    "        depends_on: List[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add component to pipeline\"\"\"\n",
    "        self.components.append(component)\n",
    "        self.dependencies[component.name] = depends_on or []\n",
    "    \n",
    "    def to_pipeline_spec(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate pipeline specification\"\"\"\n",
    "        return {\n",
    "            \"apiVersion\": \"argoproj.io/v1alpha1\",\n",
    "            \"kind\": \"Workflow\",\n",
    "            \"metadata\": {\n",
    "                \"name\": self.name,\n",
    "                \"annotations\": {\n",
    "                    \"pipelines.kubeflow.org/pipeline_name\": self.name\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"entrypoint\": \"main\",\n",
    "                \"templates\": [\n",
    "                    self._create_dag_template(),\n",
    "                    *[self._create_component_template(c) for c in self.components]\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_dag_template(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create DAG template for pipeline\"\"\"\n",
    "        tasks = []\n",
    "        for component in self.components:\n",
    "            task = {\n",
    "                \"name\": component.name,\n",
    "                \"template\": component.name\n",
    "            }\n",
    "            if self.dependencies[component.name]:\n",
    "                task[\"dependencies\"] = self.dependencies[component.name]\n",
    "            tasks.append(task)\n",
    "        \n",
    "        return {\n",
    "            \"name\": \"main\",\n",
    "            \"dag\": {\"tasks\": tasks}\n",
    "        }\n",
    "    \n",
    "    def _create_component_template(self, component: PipelineComponent) -> Dict[str, Any]:\n",
    "        \"\"\"Create component template\"\"\"\n",
    "        template = {\n",
    "            \"name\": component.name,\n",
    "            \"container\": {\n",
    "                \"image\": component.image,\n",
    "                \"command\": component.command,\n",
    "                \"args\": component.arguments,\n",
    "                \"resources\": {\n",
    "                    \"requests\": {\n",
    "                        \"cpu\": component.cpu_request,\n",
    "                        \"memory\": component.memory_request\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if component.gpu_limit > 0:\n",
    "            template[\"container\"][\"resources\"][\"limits\"] = {\n",
    "                \"nvidia.com/gpu\": str(component.gpu_limit)\n",
    "            }\n",
    "        \n",
    "        return template\n",
    "\n",
    "\n",
    "# Example: ML Training Pipeline\n",
    "pipeline = KubeflowPipeline(\n",
    "    name=\"bert-training-pipeline\",\n",
    "    description=\"End-to-end BERT training pipeline\"\n",
    ")\n",
    "\n",
    "# Data preprocessing\n",
    "pipeline.add_component(PipelineComponent(\n",
    "    name=\"data-preprocessing\",\n",
    "    image=\"ml-images/preprocessor:latest\",\n",
    "    command=[\"python\", \"preprocess.py\"],\n",
    "    arguments=[\"--input\", \"/data/raw\", \"--output\", \"/data/processed\"],\n",
    "    cpu_request=\"2\",\n",
    "    memory_request=\"8Gi\"\n",
    "))\n",
    "\n",
    "# Model training\n",
    "pipeline.add_component(\n",
    "    PipelineComponent(\n",
    "        name=\"model-training\",\n",
    "        image=\"ml-images/trainer:latest\",\n",
    "        command=[\"python\", \"train.py\"],\n",
    "        arguments=[\"--data\", \"/data/processed\", \"--epochs\", \"10\"],\n",
    "        cpu_request=\"4\",\n",
    "        memory_request=\"16Gi\",\n",
    "        gpu_limit=4\n",
    "    ),\n",
    "    depends_on=[\"data-preprocessing\"]\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "pipeline.add_component(\n",
    "    PipelineComponent(\n",
    "        name=\"model-evaluation\",\n",
    "        image=\"ml-images/evaluator:latest\",\n",
    "        command=[\"python\", \"evaluate.py\"],\n",
    "        cpu_request=\"2\",\n",
    "        memory_request=\"4Gi\",\n",
    "        gpu_limit=1\n",
    "    ),\n",
    "    depends_on=[\"model-training\"]\n",
    ")\n",
    "\n",
    "# Model deployment\n",
    "pipeline.add_component(\n",
    "    PipelineComponent(\n",
    "        name=\"model-deployment\",\n",
    "        image=\"ml-images/deployer:latest\",\n",
    "        command=[\"python\", \"deploy.py\"],\n",
    "        cpu_request=\"1\",\n",
    "        memory_request=\"2Gi\"\n",
    "    ),\n",
    "    depends_on=[\"model-evaluation\"]\n",
    ")\n",
    "\n",
    "print(\"Kubeflow Pipeline Spec:\")\n",
    "print(yaml.dump(pipeline.to_pipeline_spec(), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU Scheduling & Resource Management\n",
    "\n",
    "Efficient GPU scheduling is critical for ML workloads on Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPUNode:\n",
    "    \"\"\"Represents a GPU node in the cluster\"\"\"\n",
    "    name: str\n",
    "    gpu_type: str  # nvidia-a100, nvidia-v100, nvidia-t4\n",
    "    gpu_count: int\n",
    "    gpu_memory_gb: int\n",
    "    available_gpus: int\n",
    "    labels: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPURequest:\n",
    "    \"\"\"Represents a GPU resource request\"\"\"\n",
    "    job_name: str\n",
    "    gpu_count: int\n",
    "    gpu_memory_gb: int\n",
    "    preferred_gpu_type: str = None\n",
    "    priority: int = 0  # Higher = more important\n",
    "\n",
    "\n",
    "class GPUScheduler:\n",
    "    \"\"\"\n",
    "    Simulates GPU scheduling for ML workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes: List[GPUNode] = []\n",
    "        self.pending_requests: List[GPURequest] = []\n",
    "        self.scheduled_jobs: Dict[str, str] = {}  # job -> node mapping\n",
    "    \n",
    "    def add_node(self, node: GPUNode) -> None:\n",
    "        \"\"\"Add a GPU node to the cluster\"\"\"\n",
    "        self.nodes.append(node)\n",
    "    \n",
    "    def submit_request(self, request: GPURequest) -> None:\n",
    "        \"\"\"Submit a GPU request\"\"\"\n",
    "        self.pending_requests.append(request)\n",
    "        self.pending_requests.sort(key=lambda x: x.priority, reverse=True)\n",
    "    \n",
    "    def schedule(self) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Schedule pending requests to available nodes.\n",
    "        Returns list of (job_name, node_name) tuples.\n",
    "        \"\"\"\n",
    "        scheduled = []\n",
    "        remaining_requests = []\n",
    "        \n",
    "        for request in self.pending_requests:\n",
    "            best_node = self._find_best_node(request)\n",
    "            \n",
    "            if best_node:\n",
    "                # Allocate GPUs\n",
    "                best_node.available_gpus -= request.gpu_count\n",
    "                self.scheduled_jobs[request.job_name] = best_node.name\n",
    "                scheduled.append((request.job_name, best_node.name))\n",
    "            else:\n",
    "                remaining_requests.append(request)\n",
    "        \n",
    "        self.pending_requests = remaining_requests\n",
    "        return scheduled\n",
    "    \n",
    "    def _find_best_node(self, request: GPURequest) -> Optional[GPUNode]:\n",
    "        \"\"\"Find best node for request using bin-packing\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            # Check availability\n",
    "            if node.available_gpus < request.gpu_count:\n",
    "                continue\n",
    "            \n",
    "            # Check memory requirement\n",
    "            if node.gpu_memory_gb < request.gpu_memory_gb:\n",
    "                continue\n",
    "            \n",
    "            # Calculate score (prefer nodes with matching GPU type and less fragmentation)\n",
    "            score = 0\n",
    "            \n",
    "            if request.preferred_gpu_type and node.gpu_type == request.preferred_gpu_type:\n",
    "                score += 100\n",
    "            \n",
    "            # Best-fit: prefer nodes with just enough resources\n",
    "            score -= (node.available_gpus - request.gpu_count) * 10\n",
    "            \n",
    "            candidates.append((score, node))\n",
    "        \n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            return candidates[0][1]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def release(self, job_name: str) -> None:\n",
    "        \"\"\"Release GPUs when job completes\"\"\"\n",
    "        if job_name in self.scheduled_jobs:\n",
    "            node_name = self.scheduled_jobs[job_name]\n",
    "            for node in self.nodes:\n",
    "                if node.name == node_name:\n",
    "                    # Find original request to get GPU count\n",
    "                    # (simplified: assume 1 GPU)\n",
    "                    node.available_gpus += 1\n",
    "            del self.scheduled_jobs[job_name]\n",
    "    \n",
    "    def get_cluster_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cluster GPU status\"\"\"\n",
    "        total_gpus = sum(n.gpu_count for n in self.nodes)\n",
    "        available_gpus = sum(n.available_gpus for n in self.nodes)\n",
    "        \n",
    "        return {\n",
    "            \"total_gpus\": total_gpus,\n",
    "            \"available_gpus\": available_gpus,\n",
    "            \"utilization\": (total_gpus - available_gpus) / total_gpus if total_gpus > 0 else 0,\n",
    "            \"pending_jobs\": len(self.pending_requests),\n",
    "            \"running_jobs\": len(self.scheduled_jobs),\n",
    "            \"nodes\": [\n",
    "                {\n",
    "                    \"name\": n.name,\n",
    "                    \"gpu_type\": n.gpu_type,\n",
    "                    \"available\": f\"{n.available_gpus}/{n.gpu_count}\"\n",
    "                }\n",
    "                for n in self.nodes\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: GPU Scheduling\n",
    "scheduler = GPUScheduler()\n",
    "\n",
    "# Add nodes\n",
    "scheduler.add_node(GPUNode(\"node-1\", \"nvidia-a100\", 8, 80, 8))\n",
    "scheduler.add_node(GPUNode(\"node-2\", \"nvidia-a100\", 8, 80, 8))\n",
    "scheduler.add_node(GPUNode(\"node-3\", \"nvidia-v100\", 4, 32, 4))\n",
    "\n",
    "# Submit requests\n",
    "scheduler.submit_request(GPURequest(\"llm-training\", 4, 80, \"nvidia-a100\", priority=10))\n",
    "scheduler.submit_request(GPURequest(\"inference-1\", 1, 16, priority=5))\n",
    "scheduler.submit_request(GPURequest(\"inference-2\", 1, 16, priority=5))\n",
    "scheduler.submit_request(GPURequest(\"batch-job\", 2, 32, \"nvidia-v100\", priority=1))\n",
    "\n",
    "# Schedule\n",
    "scheduled = scheduler.schedule()\n",
    "print(\"Scheduled jobs:\")\n",
    "for job, node in scheduled:\n",
    "    print(f\"  {job} -> {node}\")\n",
    "\n",
    "print(f\"\\nCluster status: {json.dumps(scheduler.get_cluster_status(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUFractionalScheduling:\n",
    "    \"\"\"\n",
    "    Fractional GPU scheduling using MIG (Multi-Instance GPU) or time-slicing.\n",
    "    Enables multiple workloads to share a single GPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_slices: Dict[str, List[Dict]] = {}  # gpu_id -> list of slices\n",
    "    \n",
    "    def create_mig_profiles(self, gpu_id: str, gpu_memory_gb: int = 80) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create MIG profiles for A100 GPU.\n",
    "        A100 80GB supports various MIG configurations.\n",
    "        \"\"\"\n",
    "        # Example A100 MIG profiles\n",
    "        profiles = [\n",
    "            {\"profile\": \"1g.10gb\", \"memory_gb\": 10, \"compute_fraction\": 1/7},\n",
    "            {\"profile\": \"2g.20gb\", \"memory_gb\": 20, \"compute_fraction\": 2/7},\n",
    "            {\"profile\": \"3g.40gb\", \"memory_gb\": 40, \"compute_fraction\": 3/7},\n",
    "            {\"profile\": \"4g.40gb\", \"memory_gb\": 40, \"compute_fraction\": 4/7},\n",
    "            {\"profile\": \"7g.80gb\", \"memory_gb\": 80, \"compute_fraction\": 1.0},\n",
    "        ]\n",
    "        \n",
    "        self.gpu_slices[gpu_id] = profiles\n",
    "        return profiles\n",
    "    \n",
    "    def allocate_slice(\n",
    "        self,\n",
    "        gpu_id: str,\n",
    "        required_memory_gb: int\n",
    "    ) -> Optional[Dict]:\n",
    "        \"\"\"Allocate a GPU slice for a workload\"\"\"\n",
    "        if gpu_id not in self.gpu_slices:\n",
    "            return None\n",
    "        \n",
    "        for profile in self.gpu_slices[gpu_id]:\n",
    "            if profile[\"memory_gb\"] >= required_memory_gb:\n",
    "                return profile\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_pod_spec_with_mig(\n",
    "        self,\n",
    "        name: str,\n",
    "        mig_profile: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate pod spec with MIG resource request\"\"\"\n",
    "        return {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"Pod\",\n",
    "            \"metadata\": {\"name\": name},\n",
    "            \"spec\": {\n",
    "                \"containers\": [{\n",
    "                    \"name\": \"ml-container\",\n",
    "                    \"resources\": {\n",
    "                        \"limits\": {\n",
    "                            f\"nvidia.com/mig-{mig_profile}\": \"1\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: MIG scheduling\n",
    "mig_scheduler = GPUFractionalScheduling()\n",
    "profiles = mig_scheduler.create_mig_profiles(\"gpu-0\")\n",
    "\n",
    "print(\"Available MIG profiles:\")\n",
    "for p in profiles:\n",
    "    print(f\"  {p['profile']}: {p['memory_gb']}GB, {p['compute_fraction']:.1%} compute\")\n",
    "\n",
    "# Allocate for inference workload needing 15GB\n",
    "allocation = mig_scheduler.allocate_slice(\"gpu-0\", 15)\n",
    "if allocation:\n",
    "    print(f\"\\nAllocated profile: {allocation['profile']}\")\n",
    "    pod_spec = mig_scheduler.generate_pod_spec_with_mig(\"inference-pod\", allocation['profile'])\n",
    "    print(f\"Pod spec: {yaml.dump(pod_spec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto-scaling Patterns\n",
    "\n",
    "Scaling ML services based on various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalPodAutoscaler:\n",
    "    \"\"\"\n",
    "    Simulates Kubernetes HPA for ML workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        deployment_name: str,\n",
    "        min_replicas: int = 1,\n",
    "        max_replicas: int = 10,\n",
    "        target_cpu_utilization: int = 70,\n",
    "        target_memory_utilization: int = 80,\n",
    "        target_requests_per_second: int = None,\n",
    "        scale_up_stabilization_seconds: int = 0,\n",
    "        scale_down_stabilization_seconds: int = 300\n",
    "    ):\n",
    "        self.deployment_name = deployment_name\n",
    "        self.min_replicas = min_replicas\n",
    "        self.max_replicas = max_replicas\n",
    "        self.target_cpu = target_cpu_utilization\n",
    "        self.target_memory = target_memory_utilization\n",
    "        self.target_rps = target_requests_per_second\n",
    "        self.scale_up_stabilization = scale_up_stabilization_seconds\n",
    "        self.scale_down_stabilization = scale_down_stabilization_seconds\n",
    "        \n",
    "        self.current_replicas = min_replicas\n",
    "        self.scaling_history: List[Dict] = []\n",
    "    \n",
    "    def calculate_desired_replicas(\n",
    "        self,\n",
    "        current_cpu: float,\n",
    "        current_memory: float = None,\n",
    "        current_rps: float = None\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Calculate desired replica count based on metrics.\n",
    "        Uses the formula: desired = ceil(current_replicas * (current / target))\n",
    "        \"\"\"\n",
    "        desired_counts = []\n",
    "        \n",
    "        # CPU-based scaling\n",
    "        cpu_desired = int(np.ceil(\n",
    "            self.current_replicas * (current_cpu / self.target_cpu)\n",
    "        ))\n",
    "        desired_counts.append(cpu_desired)\n",
    "        \n",
    "        # Memory-based scaling\n",
    "        if current_memory is not None:\n",
    "            memory_desired = int(np.ceil(\n",
    "                self.current_replicas * (current_memory / self.target_memory)\n",
    "            ))\n",
    "            desired_counts.append(memory_desired)\n",
    "        \n",
    "        # RPS-based scaling (custom metric)\n",
    "        if current_rps is not None and self.target_rps:\n",
    "            rps_desired = int(np.ceil(\n",
    "                self.current_replicas * (current_rps / self.target_rps)\n",
    "            ))\n",
    "            desired_counts.append(rps_desired)\n",
    "        \n",
    "        # Take maximum (most aggressive scaling)\n",
    "        desired = max(desired_counts)\n",
    "        \n",
    "        # Apply bounds\n",
    "        desired = max(self.min_replicas, min(self.max_replicas, desired))\n",
    "        \n",
    "        return desired\n",
    "    \n",
    "    def scale(self, desired_replicas: int) -> Dict[str, Any]:\n",
    "        \"\"\"Execute scaling action\"\"\"\n",
    "        previous = self.current_replicas\n",
    "        self.current_replicas = desired_replicas\n",
    "        \n",
    "        action = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"previous_replicas\": previous,\n",
    "            \"new_replicas\": desired_replicas,\n",
    "            \"action\": \"scale_up\" if desired_replicas > previous else \"scale_down\"\n",
    "        }\n",
    "        \n",
    "        self.scaling_history.append(action)\n",
    "        return action\n",
    "    \n",
    "    def to_hpa_spec(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate HPA YAML specification\"\"\"\n",
    "        spec = {\n",
    "            \"apiVersion\": \"autoscaling/v2\",\n",
    "            \"kind\": \"HorizontalPodAutoscaler\",\n",
    "            \"metadata\": {\"name\": f\"{self.deployment_name}-hpa\"},\n",
    "            \"spec\": {\n",
    "                \"scaleTargetRef\": {\n",
    "                    \"apiVersion\": \"apps/v1\",\n",
    "                    \"kind\": \"Deployment\",\n",
    "                    \"name\": self.deployment_name\n",
    "                },\n",
    "                \"minReplicas\": self.min_replicas,\n",
    "                \"maxReplicas\": self.max_replicas,\n",
    "                \"metrics\": [\n",
    "                    {\n",
    "                        \"type\": \"Resource\",\n",
    "                        \"resource\": {\n",
    "                            \"name\": \"cpu\",\n",
    "                            \"target\": {\n",
    "                                \"type\": \"Utilization\",\n",
    "                                \"averageUtilization\": self.target_cpu\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"behavior\": {\n",
    "                    \"scaleUp\": {\n",
    "                        \"stabilizationWindowSeconds\": self.scale_up_stabilization,\n",
    "                        \"policies\": [{\n",
    "                            \"type\": \"Percent\",\n",
    "                            \"value\": 100,\n",
    "                            \"periodSeconds\": 15\n",
    "                        }]\n",
    "                    },\n",
    "                    \"scaleDown\": {\n",
    "                        \"stabilizationWindowSeconds\": self.scale_down_stabilization,\n",
    "                        \"policies\": [{\n",
    "                            \"type\": \"Percent\",\n",
    "                            \"value\": 10,\n",
    "                            \"periodSeconds\": 60\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add custom RPS metric if specified\n",
    "        if self.target_rps:\n",
    "            spec[\"spec\"][\"metrics\"].append({\n",
    "                \"type\": \"Pods\",\n",
    "                \"pods\": {\n",
    "                    \"metric\": {\"name\": \"requests_per_second\"},\n",
    "                    \"target\": {\n",
    "                        \"type\": \"AverageValue\",\n",
    "                        \"averageValue\": str(self.target_rps)\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return spec\n",
    "\n",
    "\n",
    "# Example: HPA for ML inference service\n",
    "hpa = HorizontalPodAutoscaler(\n",
    "    deployment_name=\"bert-inference\",\n",
    "    min_replicas=2,\n",
    "    max_replicas=20,\n",
    "    target_cpu_utilization=60,\n",
    "    target_requests_per_second=100\n",
    ")\n",
    "\n",
    "# Simulate scaling decisions\n",
    "print(\"Scaling simulation:\")\n",
    "for cpu_util in [30, 50, 80, 120, 90, 60, 40]:\n",
    "    desired = hpa.calculate_desired_replicas(current_cpu=cpu_util)\n",
    "    if desired != hpa.current_replicas:\n",
    "        action = hpa.scale(desired)\n",
    "        print(f\"  CPU {cpu_util}% -> {action['action']}: {action['previous_replicas']} -> {action['new_replicas']} replicas\")\n",
    "    else:\n",
    "        print(f\"  CPU {cpu_util}% -> no change ({hpa.current_replicas} replicas)\")\n",
    "\n",
    "print(f\"\\nHPA Spec:\")\n",
    "print(yaml.dump(hpa.to_hpa_spec(), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalPodAutoscaler:\n",
    "    \"\"\"\n",
    "    Simulates Kubernetes VPA for right-sizing ML workloads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        deployment_name: str,\n",
    "        update_mode: str = \"Auto\",  # Off, Initial, Recreate, Auto\n",
    "        min_cpu: str = \"100m\",\n",
    "        max_cpu: str = \"8\",\n",
    "        min_memory: str = \"256Mi\",\n",
    "        max_memory: str = \"32Gi\"\n",
    "    ):\n",
    "        self.deployment_name = deployment_name\n",
    "        self.update_mode = update_mode\n",
    "        self.min_cpu = min_cpu\n",
    "        self.max_cpu = max_cpu\n",
    "        self.min_memory = min_memory\n",
    "        self.max_memory = max_memory\n",
    "        \n",
    "        self.recommendations: List[Dict] = []\n",
    "    \n",
    "    def analyze_usage(\n",
    "        self,\n",
    "        cpu_usage_samples: List[float],\n",
    "        memory_usage_samples: List[float]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze historical usage to generate recommendations.\n",
    "        \"\"\"\n",
    "        cpu_array = np.array(cpu_usage_samples)\n",
    "        memory_array = np.array(memory_usage_samples)\n",
    "        \n",
    "        recommendation = {\n",
    "            \"target\": {\n",
    "                \"cpu\": f\"{int(np.percentile(cpu_array, 90))}m\",\n",
    "                \"memory\": f\"{int(np.percentile(memory_array, 90))}Mi\"\n",
    "            },\n",
    "            \"lower_bound\": {\n",
    "                \"cpu\": f\"{int(np.percentile(cpu_array, 50))}m\",\n",
    "                \"memory\": f\"{int(np.percentile(memory_array, 50))}Mi\"\n",
    "            },\n",
    "            \"upper_bound\": {\n",
    "                \"cpu\": f\"{int(np.percentile(cpu_array, 99))}m\",\n",
    "                \"memory\": f\"{int(np.percentile(memory_array, 99))}Mi\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.recommendations.append(recommendation)\n",
    "        return recommendation\n",
    "    \n",
    "    def to_vpa_spec(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate VPA YAML specification\"\"\"\n",
    "        return {\n",
    "            \"apiVersion\": \"autoscaling.k8s.io/v1\",\n",
    "            \"kind\": \"VerticalPodAutoscaler\",\n",
    "            \"metadata\": {\"name\": f\"{self.deployment_name}-vpa\"},\n",
    "            \"spec\": {\n",
    "                \"targetRef\": {\n",
    "                    \"apiVersion\": \"apps/v1\",\n",
    "                    \"kind\": \"Deployment\",\n",
    "                    \"name\": self.deployment_name\n",
    "                },\n",
    "                \"updatePolicy\": {\n",
    "                    \"updateMode\": self.update_mode\n",
    "                },\n",
    "                \"resourcePolicy\": {\n",
    "                    \"containerPolicies\": [{\n",
    "                        \"containerName\": \"*\",\n",
    "                        \"minAllowed\": {\n",
    "                            \"cpu\": self.min_cpu,\n",
    "                            \"memory\": self.min_memory\n",
    "                        },\n",
    "                        \"maxAllowed\": {\n",
    "                            \"cpu\": self.max_cpu,\n",
    "                            \"memory\": self.max_memory\n",
    "                        }\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: VPA for ML training job\n",
    "vpa = VerticalPodAutoscaler(\n",
    "    deployment_name=\"ml-trainer\",\n",
    "    update_mode=\"Auto\",\n",
    "    min_cpu=\"500m\",\n",
    "    max_cpu=\"16\",\n",
    "    min_memory=\"1Gi\",\n",
    "    max_memory=\"64Gi\"\n",
    ")\n",
    "\n",
    "# Simulate usage analysis\n",
    "cpu_samples = np.random.normal(2000, 500, 100).tolist()  # millicores\n",
    "memory_samples = np.random.normal(4000, 1000, 100).tolist()  # MiB\n",
    "\n",
    "recommendation = vpa.analyze_usage(cpu_samples, memory_samples)\n",
    "print(\"VPA Recommendation:\")\n",
    "print(json.dumps(recommendation, indent=2))\n",
    "\n",
    "print(f\"\\nVPA Spec:\")\n",
    "print(yaml.dump(vpa.to_vpa_spec(), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Helm Charts for ML Deployments\n",
    "\n",
    "Packaging ML deployments with Helm for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLHelmChart:\n",
    "    \"\"\"\n",
    "    Generates Helm chart structure for ML deployments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        version: str = \"1.0.0\",\n",
    "        app_version: str = \"1.0.0\"\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.version = version\n",
    "        self.app_version = app_version\n",
    "    \n",
    "    def generate_chart_yaml(self) -> str:\n",
    "        \"\"\"Generate Chart.yaml\"\"\"\n",
    "        return yaml.dump({\n",
    "            \"apiVersion\": \"v2\",\n",
    "            \"name\": self.name,\n",
    "            \"description\": f\"Helm chart for {self.name} ML deployment\",\n",
    "            \"type\": \"application\",\n",
    "            \"version\": self.version,\n",
    "            \"appVersion\": self.app_version,\n",
    "            \"dependencies\": [\n",
    "                {\n",
    "                    \"name\": \"redis\",\n",
    "                    \"version\": \"17.0.0\",\n",
    "                    \"repository\": \"https://charts.bitnami.com/bitnami\",\n",
    "                    \"condition\": \"redis.enabled\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    def generate_values_yaml(self) -> str:\n",
    "        \"\"\"Generate default values.yaml\"\"\"\n",
    "        return yaml.dump({\n",
    "            \"replicaCount\": 2,\n",
    "            \"image\": {\n",
    "                \"repository\": f\"ml-images/{self.name}\",\n",
    "                \"tag\": \"latest\",\n",
    "                \"pullPolicy\": \"IfNotPresent\"\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"name\": \"bert-base\",\n",
    "                \"version\": \"v1\",\n",
    "                \"storageUri\": \"s3://models/bert-base/v1\"\n",
    "            },\n",
    "            \"resources\": {\n",
    "                \"requests\": {\n",
    "                    \"cpu\": \"1\",\n",
    "                    \"memory\": \"4Gi\"\n",
    "                },\n",
    "                \"limits\": {\n",
    "                    \"cpu\": \"4\",\n",
    "                    \"memory\": \"8Gi\",\n",
    "                    \"nvidia.com/gpu\": \"1\"\n",
    "                }\n",
    "            },\n",
    "            \"autoscaling\": {\n",
    "                \"enabled\": True,\n",
    "                \"minReplicas\": 2,\n",
    "                \"maxReplicas\": 20,\n",
    "                \"targetCPUUtilizationPercentage\": 70\n",
    "            },\n",
    "            \"service\": {\n",
    "                \"type\": \"ClusterIP\",\n",
    "                \"port\": 8080\n",
    "            },\n",
    "            \"ingress\": {\n",
    "                \"enabled\": True,\n",
    "                \"className\": \"nginx\",\n",
    "                \"hosts\": [\n",
    "                    {\"host\": f\"{self.name}.ml.example.com\", \"paths\": [\"/\"]}\n",
    "                ]\n",
    "            },\n",
    "            \"monitoring\": {\n",
    "                \"enabled\": True,\n",
    "                \"prometheus\": {\n",
    "                    \"scrape\": True,\n",
    "                    \"port\": 9090\n",
    "                }\n",
    "            },\n",
    "            \"redis\": {\n",
    "                \"enabled\": True,\n",
    "                \"auth\": {\"enabled\": False}\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    def generate_deployment_template(self) -> str:\n",
    "        \"\"\"Generate deployment.yaml template\"\"\"\n",
    "        template = {\n",
    "            \"apiVersion\": \"apps/v1\",\n",
    "            \"kind\": \"Deployment\",\n",
    "            \"metadata\": {\n",
    "                \"name\": \"{{ include \\\"chart.fullname\\\" . }}\",\n",
    "                \"labels\": \"{{- include \\\"chart.labels\\\" . | nindent 4 }}\"\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"replicas\": \"{{ .Values.replicaCount }}\",\n",
    "                \"selector\": {\n",
    "                    \"matchLabels\": \"{{- include \\\"chart.selectorLabels\\\" . | nindent 6 }}\"\n",
    "                },\n",
    "                \"template\": {\n",
    "                    \"metadata\": {\n",
    "                        \"labels\": \"{{- include \\\"chart.selectorLabels\\\" . | nindent 8 }}\"\n",
    "                    },\n",
    "                    \"spec\": {\n",
    "                        \"containers\": [{\n",
    "                            \"name\": \"{{ .Chart.Name }}\",\n",
    "                            \"image\": \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\",\n",
    "                            \"ports\": [{\"containerPort\": \"{{ .Values.service.port }}\"}],\n",
    "                            \"env\": [\n",
    "                                {\"name\": \"MODEL_NAME\", \"value\": \"{{ .Values.model.name }}\"},\n",
    "                                {\"name\": \"MODEL_VERSION\", \"value\": \"{{ .Values.model.version }}\"},\n",
    "                                {\"name\": \"MODEL_STORAGE_URI\", \"value\": \"{{ .Values.model.storageUri }}\"}\n",
    "                            ],\n",
    "                            \"resources\": \"{{ toYaml .Values.resources | nindent 12 }}\"\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return yaml.dump(template, default_flow_style=False)\n",
    "    \n",
    "    def get_chart_structure(self) -> Dict[str, str]:\n",
    "        \"\"\"Get complete chart file structure\"\"\"\n",
    "        return {\n",
    "            \"Chart.yaml\": self.generate_chart_yaml(),\n",
    "            \"values.yaml\": self.generate_values_yaml(),\n",
    "            \"templates/deployment.yaml\": self.generate_deployment_template()\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Generate Helm chart\n",
    "chart = MLHelmChart(\n",
    "    name=\"bert-inference\",\n",
    "    version=\"1.2.0\",\n",
    "    app_version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Chart.yaml:\")\n",
    "print(\"=\" * 50)\n",
    "print(chart.generate_chart_yaml())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"values.yaml:\")\n",
    "print(\"=\" * 50)\n",
    "print(chart.generate_values_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAANG Interview Questions\n",
    "\n",
    "### Q1: How would you design a multi-region ML inference system on Kubernetes?\n",
    "\n",
    "**Answer:**\n",
    "I would design a federated Kubernetes architecture:\n",
    "\n",
    "1. **Multi-Cluster Setup**: Deploy identical inference services in each region using GitOps (ArgoCD/Flux)\n",
    "2. **Global Load Balancer**: Use cloud provider's global LB (GCP GLB, AWS Global Accelerator) for geo-routing\n",
    "3. **Model Synchronization**: Use a model registry (MLflow) with replication to regional object stores\n",
    "4. **Caching Layer**: Regional Redis/Memcached clusters for embedding/feature caching\n",
    "5. **Observability**: Unified monitoring with Prometheus federation and Grafana\n",
    "6. **Failover**: Automatic traffic shifting when regional health checks fail\n",
    "\n",
    "### Q2: How do you handle GPU resource fragmentation in a shared Kubernetes cluster?\n",
    "\n",
    "**Answer:**\n",
    "Multiple strategies:\n",
    "\n",
    "1. **MIG (Multi-Instance GPU)**: Use NVIDIA MIG on A100s to partition GPUs into smaller slices\n",
    "2. **Time-Slicing**: Configure NVIDIA device plugin for time-sharing on older GPUs\n",
    "3. **Bin-Packing Scheduler**: Custom scheduler that optimizes GPU utilization\n",
    "4. **Priority Classes**: Use PriorityClass to preempt low-priority jobs when needed\n",
    "5. **Job Queuing**: Use Kueue or Volcano for batch job scheduling with fair-share\n",
    "6. **Right-Sizing**: Use VPA recommendations to match workloads to appropriate GPU types\n",
    "\n",
    "### Q3: What's your strategy for zero-downtime ML model updates?\n",
    "\n",
    "**Answer:**\n",
    "Progressive rollout strategy:\n",
    "\n",
    "1. **Canary Deployment**: Start with 5% traffic to new model version\n",
    "2. **Automated Metrics Check**: Monitor latency, error rate, prediction distribution\n",
    "3. **Shadow Mode**: Optionally run new model in shadow without serving traffic\n",
    "4. **Progressive Traffic Shift**: 5% -> 25% -> 50% -> 75% -> 100% with stabilization periods\n",
    "5. **Automatic Rollback**: Revert if SLO violations detected\n",
    "6. **Blue-Green for Critical Models**: Maintain warm standby for instant rollback\n",
    "\n",
    "Tools: KServe with canary, Argo Rollouts, or Istio traffic management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **KServe**: Serverless model inference with auto-scaling and canary deployments\n",
    "2. **Ray Serve**: Distributed model serving with DAG composition\n",
    "3. **Kubeflow Pipelines**: ML workflow orchestration with component DAGs\n",
    "4. **GPU Scheduling**: Resource management including MIG fractional scheduling\n",
    "5. **Auto-scaling**: HPA and VPA for ML workloads with custom metrics\n",
    "6. **Helm Charts**: Packaging ML deployments for reproducibility\n",
    "\n",
    "### Key Takeaways for FAANG Interviews:\n",
    "- Kubernetes is the de facto platform for production ML\n",
    "- GPU scheduling requires careful bin-packing and fragmentation management\n",
    "- Canary deployments are essential for safe model updates\n",
    "- Auto-scaling should consider ML-specific metrics (latency, throughput)\n",
    "- Infrastructure-as-Code (Helm, GitOps) enables reproducible deployments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
