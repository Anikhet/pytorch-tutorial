{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Reinforcement Learning (RL)\n",
    "\n",
    "Reinforcement Learning is a paradigm where an **agent** learns to make decisions by interacting with an **environment** to maximize a **reward**. Unlike supervised learning (where you have labels), in RL, the feedback is delayed and sparse.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Vocabulary of RL (Agent, Environment, State, Action, Reward)\n",
    "- Implement a Q-Learning Agent (Tabular)\n",
    "- Implement a Policy Gradient Agent (REINFORCE) using PyTorch\n",
    "- Solve the `CartPole-v1` environment from Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "Before coding, let's define the key terms:\n",
    "\n",
    "- **Agent**: The learner (the model).\n",
    "- **Environment**: The world the agent interacts with (e.g., a game, a robot simulation).\n",
    "- **State ($s$)**: The current situation of the agent (e.g., position, velocity).\n",
    "- **Action ($a$)**: What the agent does (e.g., move left, jump).\n",
    "- **Reward ($r$)**: Feedback from the environment (e.g., +1 for surviving, -10 for crashing).\n",
    "- **Policy ($\\pi$)**: The strategy the agent uses to decide an action given a state ($s \\to a$).\n",
    "- **Episode**: One full run of the game/task from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym # Standard RL library (formerly OpenAI Gym)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready for RL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Environment: CartPole\n",
    "\n",
    "We will use `CartPole-v1`. The goal is to balance a pole on a cart.\n",
    "- **State**: [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\n",
    "- **Actions**: 0 (Push Left), 1 (Push Right)\n",
    "- **Reward**: +1 for every step the pole stays upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state, info = env.reset()\n",
    "print(f\"Initial State: {state}\")\n",
    "print(f\"Action Space: {env.action_space}\") # Discrete(2) -> 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient (REINFORCE)\n",
    "\n",
    "In Deep RL, we use a Neural Network to approximate the policy $\\pi(a|s)$.\n",
    "\n",
    "### The Network\n",
    "Input: State (4 values) -> Hidden Layer -> Output: Probability of each Action (2 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize\n",
    "policy = PolicyNetwork(s_size=4, a_size=2, h_size=16)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop (REINFORCE Algorithm)\n",
    "\n",
    "1. **Collect Trajectory**: Play an entire episode using the current policy.\n",
    "2. **Calculate Returns**: Compute the total discounted reward for each step.\n",
    "3. **Update Policy**: Increase probability of actions that led to high rewards.\n",
    "\n",
    "$$ Loss = - \\sum \\log \\pi(a_t|s_t) \\times R_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy, optimizer, n_episodes=500):\n",
    "    gamma = 0.99 # Discount factor\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        # 1. Collect Trajectory\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            probs = policy(state_t)\n",
    "            \n",
    "            # Sample action from probability distribution\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "            \n",
    "            log_probs.append(m.log_prob(action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        # 2. Calculate Returns (Discounted Reward)\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Normalize\n",
    "        \n",
    "        # 3. Update Policy\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
    "\n",
    "# Run training\n",
    "# reinforce(env, policy, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Value Functions and Advantage\n\nBefore Actor-Critic, we need to understand **value functions**."
  },
  {
   "cell_type": "code",
   "id": "60uhbaj232",
   "source": "# Value Functions\nprint(\"\"\"\nVALUE FUNCTIONS IN RL\n=====================\n\n1. State Value Function V(s):\n   \"Expected total reward starting from state s\"\n   V(s) = E[R_t + γR_{t+1} + γ²R_{t+2} + ... | s_t = s]\n\n2. Action Value Function Q(s, a):\n   \"Expected total reward taking action a in state s\"\n   Q(s, a) = E[R_t + γR_{t+1} + ... | s_t = s, a_t = a]\n\n3. Advantage Function A(s, a):\n   \"How much better is action a compared to average?\"\n   A(s, a) = Q(s, a) - V(s)\n   \n   - A > 0: Better than average action\n   - A < 0: Worse than average action\n   - A = 0: Average action\n\nWhy Advantage is useful:\n- Reduces variance in policy gradient\n- Makes credit assignment clearer\n- The \"advantage\" tells us exactly how good an action was\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ny1pblrcno",
   "source": "## 5. Actor-Critic Architecture\n\n**Actor-Critic** combines policy gradient (Actor) with value estimation (Critic).\n\n- **Actor**: Policy network π(a|s) - decides actions\n- **Critic**: Value network V(s) - evaluates states",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "08h3b2ij5uox",
   "source": "class ActorCritic(nn.Module):\n    \"\"\"\n    Actor-Critic Network with shared backbone.\n    \n    Architecture:\n    State -> Shared Layers -> Actor Head (actions)\n                           -> Critic Head (value)\n    \"\"\"\n    def __init__(self, state_dim, action_dim, hidden_dim=64):\n        super().__init__()\n        \n        # Shared layers (feature extraction)\n        self.shared = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        \n        # Actor head: outputs action probabilities\n        self.actor = nn.Sequential(\n            nn.Linear(hidden_dim, action_dim),\n            nn.Softmax(dim=-1)\n        )\n        \n        # Critic head: outputs state value\n        self.critic = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, state):\n        features = self.shared(state)\n        action_probs = self.actor(features)\n        state_value = self.critic(features)\n        return action_probs, state_value\n    \n    def act(self, state):\n        action_probs, value = self.forward(state)\n        dist = torch.distributions.Categorical(action_probs)\n        action = dist.sample()\n        return action, dist.log_prob(action), value\n\n# Create network\nac_network = ActorCritic(state_dim=4, action_dim=2)\n\n# Forward pass\nstate = torch.randn(1, 4)\naction_probs, value = ac_network(state)\nprint(f\"Action probabilities: {action_probs.detach().numpy()}\")\nprint(f\"State value: {value.item():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gx32bown9zv",
   "source": "def train_actor_critic(env, model, optimizer, n_episodes=500, gamma=0.99):\n    \"\"\"\n    Advantage Actor-Critic (A2C) Training Loop.\n    \n    Loss = Policy Loss + Value Loss\n    \n    Policy Loss: -log(π(a|s)) * Advantage\n    Value Loss:  (V(s) - Return)²\n    \"\"\"\n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        log_probs = []\n        values = []\n        rewards = []\n        \n        done = False\n        while not done:\n            state_t = torch.FloatTensor(state).unsqueeze(0)\n            action, log_prob, value = model.act(state_t)\n            \n            log_probs.append(log_prob)\n            values.append(value)\n            \n            state, reward, terminated, truncated, _ = env.step(action.item())\n            rewards.append(reward)\n            done = terminated or truncated\n        \n        # Calculate returns and advantages\n        returns = []\n        R = 0\n        for r in reversed(rewards):\n            R = r + gamma * R\n            returns.insert(0, R)\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n        \n        values = torch.cat(values)\n        log_probs = torch.cat(log_probs)\n        \n        # Advantage = Return - Value (how much better than expected)\n        advantages = returns - values.squeeze()\n        \n        # Policy loss (maximize reward for good actions)\n        policy_loss = -(log_probs * advantages.detach()).mean()\n        \n        # Value loss (accurate value prediction)\n        value_loss = F.mse_loss(values.squeeze(), returns)\n        \n        # Combined loss (often add entropy bonus for exploration)\n        loss = policy_loss + 0.5 * value_loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if episode % 50 == 0:\n            print(f\"Episode {episode}, Reward: {sum(rewards):.0f}, Policy Loss: {policy_loss.item():.4f}\")\n    \n    return model\n\n# Training would be done like:\n# ac_model = ActorCritic(4, 2)\n# optimizer = optim.Adam(ac_model.parameters(), lr=0.001)\n# train_actor_critic(env, ac_model, optimizer)\n\nprint(\"Actor-Critic training loop defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4pwv3u1s4tc",
   "source": "## 6. Proximal Policy Optimization (PPO)\n\nPPO is the most popular RL algorithm in 2025 (used to train ChatGPT via RLHF!).\n\nKey insight: Limit how much the policy can change in one update to ensure stable training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "85s0qblu0fm",
   "source": "def ppo_loss(old_log_probs, new_log_probs, advantages, epsilon=0.2):\n    \"\"\"\n    PPO Clipped Objective.\n    \n    Instead of directly using policy gradient, PPO:\n    1. Computes ratio: r(θ) = π_new(a|s) / π_old(a|s)\n    2. Clips the ratio to [1-ε, 1+ε]\n    3. Takes minimum of clipped and unclipped objective\n    \n    This prevents the policy from changing too much in one update.\n    \"\"\"\n    # Probability ratio\n    ratio = torch.exp(new_log_probs - old_log_probs)\n    \n    # Unclipped objective\n    obj_unclipped = ratio * advantages\n    \n    # Clipped objective\n    obj_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n    \n    # PPO uses the minimum (pessimistic bound)\n    loss = -torch.min(obj_unclipped, obj_clipped).mean()\n    \n    return loss\n\n# Demonstrate clipping behavior\nprint(\"PPO Clipping Visualization:\")\nprint(\"=\" * 50)\n\nimport matplotlib.pyplot as plt\n\nratios = torch.linspace(0.5, 1.5, 100)\nadvantages_pos = torch.ones_like(ratios) * 1.0  # Positive advantage\nadvantages_neg = torch.ones_like(ratios) * -1.0  # Negative advantage\n\nepsilon = 0.2\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Positive advantage\nclipped_pos = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\nobj_unclipped_pos = ratios * advantages_pos\nobj_clipped_pos = clipped_pos * advantages_pos\nobj_ppo_pos = torch.min(obj_unclipped_pos, obj_clipped_pos)\n\naxes[0].plot(ratios.numpy(), obj_unclipped_pos.numpy(), 'b--', label='Unclipped')\naxes[0].plot(ratios.numpy(), obj_clipped_pos.numpy(), 'r--', label='Clipped')\naxes[0].plot(ratios.numpy(), obj_ppo_pos.numpy(), 'g-', linewidth=2, label='PPO (min)')\naxes[0].axvline(x=1.0, color='gray', linestyle=':')\naxes[0].set_title('Positive Advantage (A > 0)')\naxes[0].set_xlabel('Probability Ratio')\naxes[0].set_ylabel('Objective')\naxes[0].legend()\n\n# Negative advantage\nclipped_neg = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\nobj_unclipped_neg = ratios * advantages_neg\nobj_clipped_neg = clipped_neg * advantages_neg\nobj_ppo_neg = torch.min(obj_unclipped_neg, obj_clipped_neg)\n\naxes[1].plot(ratios.numpy(), obj_unclipped_neg.numpy(), 'b--', label='Unclipped')\naxes[1].plot(ratios.numpy(), obj_clipped_neg.numpy(), 'r--', label='Clipped')\naxes[1].plot(ratios.numpy(), obj_ppo_neg.numpy(), 'g-', linewidth=2, label='PPO (min)')\naxes[1].axvline(x=1.0, color='gray', linestyle=':')\naxes[1].set_title('Negative Advantage (A < 0)')\naxes[1].set_xlabel('Probability Ratio')\naxes[1].set_ylabel('Objective')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: PPO prevents extreme policy updates by clipping the ratio.\")\nprint(\"This makes training much more stable than vanilla policy gradient.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zftaqvs8ehl",
   "source": "## 7. Reward Shaping\n\n**Sparse rewards** (e.g., +1 only at goal) make learning hard. **Reward shaping** adds intermediate rewards.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "flh1x99ldz",
   "source": "# Reward Shaping Examples\nprint(\"\"\"\nREWARD SHAPING STRATEGIES\n=========================\n\n1. Distance-based rewards:\n   reward = -distance_to_goal\n   \n2. Progress rewards:\n   reward = previous_distance - current_distance\n   \n3. Potential-based shaping (theoretically sound):\n   F(s, s') = γ * Φ(s') - Φ(s)\n   where Φ is a potential function\n   \n4. Curriculum learning:\n   Start with easy tasks, gradually increase difficulty\n\nCommon Pitfalls:\n- Reward hacking: Agent finds unintended shortcuts\n- Overfit to shaped reward, ignore true objective\n- Dense rewards can distract from sparse goal\n\nExample: Self-driving car\n- Sparse: +1 for reaching destination, -1 for crash\n- Shaped: +0.1 for staying in lane, -0.5 for near-miss, +0.01 per meter forward\n\nBest Practice: Start with sparse, add shaping only if needed.\n\"\"\")\n\ndef shaped_reward(state, action, next_state, done, sparse_reward):\n    \"\"\"\n    Example of potential-based reward shaping.\n    This is guaranteed not to change the optimal policy!\n    \"\"\"\n    gamma = 0.99\n    \n    # Potential function (example: negative distance to goal)\n    def potential(s):\n        goal = np.array([0, 0])\n        return -np.linalg.norm(s[:2] - goal)  # Closer to goal = higher potential\n    \n    # Potential-based shaping bonus\n    if done:\n        shaping_bonus = -gamma * potential(state)  # No next state\n    else:\n        shaping_bonus = gamma * potential(next_state) - potential(state)\n    \n    return sparse_reward + shaping_bonus\n\nprint(\"Potential-based shaping preserves optimal policy (provably safe)!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cfz5xijxvnv",
   "source": "## 8. FAANG Interview Questions\n\n### Q1: Compare REINFORCE, Actor-Critic, and PPO. When would you use each?\n\n**Answer**:\n\n| Algorithm | Pros | Cons | Use Case |\n|-----------|------|------|----------|\n| **REINFORCE** | Simple, easy to implement | High variance, sample inefficient | Learning, prototyping |\n| **Actor-Critic** | Lower variance (baseline), online updates | Still unstable | Continuous control |\n| **PPO** | Stable, efficient, works well in practice | Hyperparameter sensitive | Production, RLHF |\n\nDecision framework:\n1. **Learning/Prototyping**: REINFORCE (simple baseline)\n2. **Continuous control**: Actor-Critic (TD learning)\n3. **Production/RLHF**: PPO (stable, widely used)\n\n---\n\n### Q2: Explain the exploration-exploitation tradeoff. How do different algorithms handle it?\n\n**Answer**:\n- **Exploitation**: Use current best action (greedy)\n- **Exploration**: Try new actions to find better strategies\n\n**Methods**:\n1. **ε-greedy**: Random action with probability ε (simple, widely used)\n2. **Softmax/Boltzmann**: Sample from action distribution (natural for policy gradient)\n3. **UCB (Upper Confidence Bound)**: Optimism in face of uncertainty (principled)\n4. **Entropy regularization**: Add entropy bonus to encourage diverse actions (PPO uses this)\n5. **Intrinsic motivation**: Reward curiosity/novelty (complex environments)\n\nIn policy gradient methods, exploration comes naturally from sampling actions from the policy distribution.\n\n---\n\n### Q3: What is RLHF (Reinforcement Learning from Human Feedback) and how does it work?\n\n**Answer**: RLHF is how ChatGPT and Claude are trained to be helpful.\n\n**Three-stage process**:\n1. **Supervised Fine-Tuning (SFT)**: Train on human demonstrations\n2. **Reward Model Training**: Learn to predict human preferences from comparisons\n3. **RL Fine-Tuning**: Use PPO to optimize policy against reward model\n\n**Key components**:\n- **Reward Model**: R(prompt, response) → scalar score\n- **KL Penalty**: Prevent policy from diverging too far from SFT\n- **PPO**: Stable policy optimization\n\n**Challenges**:\n- Reward hacking (model finds loopholes)\n- Reward model errors (wrong preferences)\n- Distribution shift (training ≠ deployment)\n\n---\n\n### Q4: How do you handle sparse rewards in RL?\n\n**Answer**:\n\n1. **Reward Shaping**: Add intermediate rewards\n   - Risk: Can change optimal policy if not careful\n   - Safe: Use potential-based shaping\n\n2. **Curriculum Learning**: Start easy, increase difficulty\n   - Example: Short mazes → Long mazes\n\n3. **Hindsight Experience Replay (HER)**: \n   - Failed attempts become successes with different goals\n   - \"I didn't reach A, but I reached B!\"\n\n4. **Intrinsic Motivation**:\n   - Curiosity-driven exploration\n   - Reward visiting new states\n\n5. **Demonstration Learning**:\n   - Imitation learning from expert\n   - Combine with RL (DQfD, GAIL)\n\n---\n\n### Q5: Explain the credit assignment problem in RL.\n\n**Answer**: Credit assignment is determining which actions contributed to a reward.\n\n**The Problem**:\n- Reward comes at end of episode (delayed)\n- Many actions preceded the reward\n- Which ones were actually responsible?\n\n**Solutions**:\n\n1. **Discounting (γ)**:\n   - Recent actions weighted more\n   - $R = r_0 + \\gamma r_1 + \\gamma^2 r_2 + ...$\n\n2. **Temporal Difference (TD)**:\n   - Bootstrap from value estimates\n   - Update after each step, not episode\n\n3. **Advantage Function**:\n   - A(s,a) = Q(s,a) - V(s)\n   - Measures action quality relative to average\n\n4. **Generalized Advantage Estimation (GAE)**:\n   - Balance bias-variance in advantage estimation\n   - λ interpolates between MC and TD\n\n5. **Attention mechanisms**:\n   - Transformers in RL (Decision Transformer)\n   - Learn to attend to relevant past states",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "hdbci3hrxl",
   "source": "## Key Takeaways\n\n1. **RL is different**: No labels, just rewards from environment interaction.\n2. **Policy Gradient (REINFORCE)**: Directly optimizes the policy network to maximize expected reward.\n3. **Value Functions**: V(s) and Q(s,a) estimate expected future rewards.\n4. **Advantage**: A(s,a) = Q(s,a) - V(s) measures how much better an action is than average.\n5. **Actor-Critic**: Combines policy (actor) and value (critic) networks for lower variance.\n6. **PPO**: Clips policy updates for stability - the go-to algorithm for production RL.\n7. **Exploration vs Exploitation**: Balance trying new things with using known strategies.\n8. **RLHF**: How modern LLMs are aligned with human preferences using PPO.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}