{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Reinforcement Learning (RL)\n",
    "\n",
    "Reinforcement Learning is a paradigm where an **agent** learns to make decisions by interacting with an **environment** to maximize a **reward**. Unlike supervised learning (where you have labels), in RL, the feedback is delayed and sparse.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Vocabulary of RL (Agent, Environment, State, Action, Reward)\n",
    "- Implement a Q-Learning Agent (Tabular)\n",
    "- Implement a Policy Gradient Agent (REINFORCE) using PyTorch\n",
    "- Solve the `CartPole-v1` environment from Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "Before coding, let's define the key terms:\n",
    "\n",
    "- **Agent**: The learner (the model).\n",
    "- **Environment**: The world the agent interacts with (e.g., a game, a robot simulation).\n",
    "- **State ($s$)**: The current situation of the agent (e.g., position, velocity).\n",
    "- **Action ($a$)**: What the agent does (e.g., move left, jump).\n",
    "- **Reward ($r$)**: Feedback from the environment (e.g., +1 for surviving, -10 for crashing).\n",
    "- **Policy ($\\pi$)**: The strategy the agent uses to decide an action given a state ($s \\to a$).\n",
    "- **Episode**: One full run of the game/task from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym # Standard RL library (formerly OpenAI Gym)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready for RL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Environment: CartPole\n",
    "\n",
    "We will use `CartPole-v1`. The goal is to balance a pole on a cart.\n",
    "- **State**: [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\n",
    "- **Actions**: 0 (Push Left), 1 (Push Right)\n",
    "- **Reward**: +1 for every step the pole stays upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state, info = env.reset()\n",
    "print(f\"Initial State: {state}\")\n",
    "print(f\"Action Space: {env.action_space}\") # Discrete(2) -> 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient (REINFORCE)\n",
    "\n",
    "In Deep RL, we use a Neural Network to approximate the policy $\\pi(a|s)$.\n",
    "\n",
    "### The Network\n",
    "Input: State (4 values) -> Hidden Layer -> Output: Probability of each Action (2 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize\n",
    "policy = PolicyNetwork(s_size=4, a_size=2, h_size=16)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop (REINFORCE Algorithm)\n",
    "\n",
    "1. **Collect Trajectory**: Play an entire episode using the current policy.\n",
    "2. **Calculate Returns**: Compute the total discounted reward for each step.\n",
    "3. **Update Policy**: Increase probability of actions that led to high rewards.\n",
    "\n",
    "$$ Loss = - \\sum \\log \\pi(a_t|s_t) \\times R_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy, optimizer, n_episodes=500):\n",
    "    gamma = 0.99 # Discount factor\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        # 1. Collect Trajectory\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            probs = policy(state_t)\n",
    "            \n",
    "            # Sample action from probability distribution\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "            \n",
    "            log_probs.append(m.log_prob(action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        # 2. Calculate Returns (Discounted Reward)\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Normalize\n",
    "        \n",
    "        # 3. Update Policy\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n",
    "\n",
    "# Run training\n",
    "# reinforce(env, policy, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RL is different**: No labels, just rewards.\n",
    "2. **Policy Gradient**: Directly optimizes the policy network to maximize expected reward.\n",
    "3. **Exploration vs Exploitation**: The agent needs to try random things (exploration) to find good strategies, but also use what it knows (exploitation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
