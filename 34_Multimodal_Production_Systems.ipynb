{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Production Systems\n",
    "\n",
    "This notebook covers production-grade multimodal ML systems - essential for FAANG ML engineers building next-generation AI applications.\n",
    "\n",
    "## Topics Covered\n",
    "1. **Vision-Language Models** - CLIP-style architectures\n",
    "2. **Multimodal Embeddings** - Cross-modal representations\n",
    "3. **Image-Text Retrieval** - Production search systems\n",
    "4. **Audio Processing** - Speech and audio ML\n",
    "5. **Video Understanding** - Temporal multimodal models\n",
    "6. **Production Deployment** - Serving multimodal systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vision-Language Models (CLIP-style)\n",
    "\n",
    "Contrastive Language-Image Pre-training for unified vision-text understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) encoder for images.\n",
    "    Simplified implementation for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        projection_dim: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Class token and position embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Layer norm and projection\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, projection_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: image -> embedding\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding: (B, C, H, W) -> (B, num_patches, embed_dim)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Get CLS token output and project\n",
    "        x = self.ln(x[:, 0])\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder for text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50000,\n",
    "        max_seq_len: int = 77,\n",
    "        embed_dim: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 12,\n",
    "        projection_dim: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Layer norm and projection\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, projection_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: tokens -> embedding\"\"\"\n",
    "        seq_len = input_ids.shape[1]\n",
    "        \n",
    "        # Token + position embedding\n",
    "        x = self.token_embed(input_ids)\n",
    "        x = x + self.pos_embed[:, :seq_len, :]\n",
    "        \n",
    "        # Create attention mask for transformer\n",
    "        if attention_mask is not None:\n",
    "            # Convert to transformer format (True = ignore)\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Pool: take EOS token (last non-padded token)\n",
    "        if attention_mask is not None:\n",
    "            # Get position of last real token\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "            pooled = x[torch.arange(x.shape[0]), seq_lengths]\n",
    "        else:\n",
    "            pooled = x[:, -1]\n",
    "        \n",
    "        # Layer norm and project\n",
    "        pooled = self.ln(pooled)\n",
    "        pooled = self.projection(pooled)\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CLIP-style vision-language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config: Dict[str, Any] = None,\n",
    "        text_config: Dict[str, Any] = None,\n",
    "        projection_dim: int = 512,\n",
    "        temperature: float = 0.07\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        vision_config = vision_config or {}\n",
    "        text_config = text_config or {}\n",
    "        \n",
    "        self.vision_encoder = VisionEncoder(\n",
    "            projection_dim=projection_dim,\n",
    "            **vision_config\n",
    "        )\n",
    "        self.text_encoder = TextEncoder(\n",
    "            projection_dim=projection_dim,\n",
    "            **text_config\n",
    "        )\n",
    "        \n",
    "        # Learnable temperature\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))\n",
    "    \n",
    "    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode images to embeddings\"\"\"\n",
    "        return F.normalize(self.vision_encoder(images), dim=-1)\n",
    "    \n",
    "    def encode_text(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode text to embeddings\"\"\"\n",
    "        return F.normalize(\n",
    "            self.text_encoder(input_ids, attention_mask),\n",
    "            dim=-1\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass with contrastive loss\"\"\"\n",
    "        # Get embeddings\n",
    "        image_embeds = self.encode_image(images)\n",
    "        text_embeds = self.encode_text(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.T\n",
    "        logits_per_text = logits_per_image.T\n",
    "        \n",
    "        return {\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds,\n",
    "            \"logits_per_image\": logits_per_image,\n",
    "            \"logits_per_text\": logits_per_text,\n",
    "            \"logit_scale\": logit_scale\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, outputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Compute contrastive loss\"\"\"\n",
    "        batch_size = outputs[\"logits_per_image\"].shape[0]\n",
    "        labels = torch.arange(batch_size, device=outputs[\"logits_per_image\"].device)\n",
    "        \n",
    "        loss_i2t = F.cross_entropy(outputs[\"logits_per_image\"], labels)\n",
    "        loss_t2i = F.cross_entropy(outputs[\"logits_per_text\"], labels)\n",
    "        \n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "# Example: CLIP Model\n",
    "clip_model = CLIPModel(\n",
    "    vision_config={\"image_size\": 224, \"patch_size\": 16, \"num_layers\": 4},\n",
    "    text_config={\"vocab_size\": 10000, \"num_layers\": 4},\n",
    "    projection_dim=256\n",
    ")\n",
    "\n",
    "# Simulate batch\n",
    "batch_size = 4\n",
    "images = torch.randn(batch_size, 3, 224, 224)\n",
    "input_ids = torch.randint(0, 10000, (batch_size, 32))\n",
    "attention_mask = torch.ones(batch_size, 32)\n",
    "\n",
    "outputs = clip_model(images, input_ids, attention_mask)\n",
    "loss = clip_model.compute_loss(outputs)\n",
    "\n",
    "print(f\"Image embeddings shape: {outputs['image_embeds'].shape}\")\n",
    "print(f\"Text embeddings shape: {outputs['text_embeds'].shape}\")\n",
    "print(f\"Contrastive loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multimodal Embeddings & Fusion\n",
    "\n",
    "Combining information from multiple modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalityFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-modal fusion strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dims: Dict[str, int],\n",
    "        fusion_dim: int = 512,\n",
    "        fusion_type: str = \"concat\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fusion_type = fusion_type\n",
    "        self.modalities = list(embed_dims.keys())\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        self.projections = nn.ModuleDict({\n",
    "            mod: nn.Linear(dim, fusion_dim)\n",
    "            for mod, dim in embed_dims.items()\n",
    "        })\n",
    "        \n",
    "        if fusion_type == \"concat\":\n",
    "            self.fusion_layer = nn.Linear(\n",
    "                fusion_dim * len(embed_dims),\n",
    "                fusion_dim\n",
    "            )\n",
    "        elif fusion_type == \"attention\":\n",
    "            self.cross_attention = nn.MultiheadAttention(\n",
    "                embed_dim=fusion_dim,\n",
    "                num_heads=8,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
    "        elif fusion_type == \"gated\":\n",
    "            self.gates = nn.ModuleDict({\n",
    "                mod: nn.Sequential(\n",
    "                    nn.Linear(fusion_dim, fusion_dim),\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "                for mod in embed_dims.keys()\n",
    "            })\n",
    "            self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        modality_embeds: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fuse embeddings from multiple modalities.\n",
    "        \n",
    "        Args:\n",
    "            modality_embeds: Dict of modality name -> embedding tensor\n",
    "        \"\"\"\n",
    "        # Project all modalities to common dimension\n",
    "        projected = {\n",
    "            mod: self.projections[mod](embed)\n",
    "            for mod, embed in modality_embeds.items()\n",
    "        }\n",
    "        \n",
    "        if self.fusion_type == \"concat\":\n",
    "            # Simple concatenation\n",
    "            fused = torch.cat(\n",
    "                [projected[mod] for mod in self.modalities],\n",
    "                dim=-1\n",
    "            )\n",
    "            return self.fusion_layer(fused)\n",
    "        \n",
    "        elif self.fusion_type == \"attention\":\n",
    "            # Cross-modal attention\n",
    "            # Stack modalities as sequence\n",
    "            stacked = torch.stack(\n",
    "                [projected[mod] for mod in self.modalities],\n",
    "                dim=1\n",
    "            )  # (B, num_modalities, fusion_dim)\n",
    "            \n",
    "            # Self-attention across modalities\n",
    "            attended, _ = self.cross_attention(stacked, stacked, stacked)\n",
    "            \n",
    "            # Pool and project\n",
    "            pooled = attended.mean(dim=1)\n",
    "            return self.fusion_layer(pooled)\n",
    "        \n",
    "        elif self.fusion_type == \"gated\":\n",
    "            # Gated fusion\n",
    "            gated_embeds = []\n",
    "            for mod in self.modalities:\n",
    "                gate = self.gates[mod](projected[mod])\n",
    "                gated_embeds.append(gate * projected[mod])\n",
    "            \n",
    "            # Sum gated embeddings\n",
    "            fused = sum(gated_embeds)\n",
    "            return self.fusion_layer(fused)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion type: {self.fusion_type}\")\n",
    "\n",
    "\n",
    "class MultimodalEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end multimodal embedding model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion_dim: int = 512,\n",
    "        fusion_type: str = \"attention\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Modality encoders (simplified)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 256)\n",
    "        )\n",
    "        \n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Embedding(10000, 128),\n",
    "            nn.LSTM(128, 128, batch_first=True),\n",
    "        )\n",
    "        self.text_proj = nn.Linear(128, 256)\n",
    "        \n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 256)\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = ModalityFusion(\n",
    "            embed_dims={\"image\": 256, \"text\": 256, \"audio\": 256},\n",
    "            fusion_dim=fusion_dim,\n",
    "            fusion_type=fusion_type\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        image: torch.Tensor = None,\n",
    "        text: torch.Tensor = None,\n",
    "        audio: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with available modalities\"\"\"\n",
    "        modality_embeds = {}\n",
    "        \n",
    "        if image is not None:\n",
    "            modality_embeds[\"image\"] = self.image_encoder(image)\n",
    "        \n",
    "        if text is not None:\n",
    "            text_output, _ = self.text_encoder(text)\n",
    "            # Take last hidden state\n",
    "            modality_embeds[\"text\"] = self.text_proj(text_output[:, -1, :])\n",
    "        \n",
    "        if audio is not None:\n",
    "            modality_embeds[\"audio\"] = self.audio_encoder(audio)\n",
    "        \n",
    "        if len(modality_embeds) == 0:\n",
    "            raise ValueError(\"At least one modality must be provided\")\n",
    "        \n",
    "        return self.fusion(modality_embeds)\n",
    "\n",
    "\n",
    "# Example: Multimodal Fusion\n",
    "fusion_model = ModalityFusion(\n",
    "    embed_dims={\"image\": 512, \"text\": 256, \"audio\": 128},\n",
    "    fusion_dim=512,\n",
    "    fusion_type=\"attention\"\n",
    ")\n",
    "\n",
    "modality_embeds = {\n",
    "    \"image\": torch.randn(4, 512),\n",
    "    \"text\": torch.randn(4, 256),\n",
    "    \"audio\": torch.randn(4, 128)\n",
    "}\n",
    "\n",
    "fused = fusion_model(modality_embeds)\n",
    "print(f\"Fused embedding shape: {fused.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image-Text Retrieval System\n",
    "\n",
    "Production-ready cross-modal search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Result from multimodal retrieval\"\"\"\n",
    "    item_id: str\n",
    "    score: float\n",
    "    modality: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class MultimodalIndex:\n",
    "    \"\"\"\n",
    "    In-memory multimodal index for fast similarity search.\n",
    "    In production, use FAISS, Milvus, or Pinecone.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings: Dict[str, np.ndarray] = {}  # id -> embedding\n",
    "        self.metadata: Dict[str, Dict[str, Any]] = {}  # id -> metadata\n",
    "        self.modalities: Dict[str, str] = {}  # id -> modality\n",
    "        \n",
    "        # Pre-computed normalized embeddings for fast search\n",
    "        self._normalized_matrix = None\n",
    "        self._id_list = None\n",
    "        self._needs_rebuild = True\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        item_id: str,\n",
    "        embedding: np.ndarray,\n",
    "        modality: str,\n",
    "        metadata: Dict[str, Any] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add item to index\"\"\"\n",
    "        self.embeddings[item_id] = embedding\n",
    "        self.modalities[item_id] = modality\n",
    "        self.metadata[item_id] = metadata or {}\n",
    "        self._needs_rebuild = True\n",
    "    \n",
    "    def add_batch(\n",
    "        self,\n",
    "        item_ids: List[str],\n",
    "        embeddings: np.ndarray,\n",
    "        modality: str,\n",
    "        metadata_list: List[Dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add batch of items\"\"\"\n",
    "        metadata_list = metadata_list or [{} for _ in item_ids]\n",
    "        \n",
    "        for i, item_id in enumerate(item_ids):\n",
    "            self.embeddings[item_id] = embeddings[i]\n",
    "            self.modalities[item_id] = modality\n",
    "            self.metadata[item_id] = metadata_list[i]\n",
    "        \n",
    "        self._needs_rebuild = True\n",
    "    \n",
    "    def _rebuild_index(self) -> None:\n",
    "        \"\"\"Rebuild search index\"\"\"\n",
    "        if not self._needs_rebuild:\n",
    "            return\n",
    "        \n",
    "        self._id_list = list(self.embeddings.keys())\n",
    "        if self._id_list:\n",
    "            matrix = np.stack([self.embeddings[id] for id in self._id_list])\n",
    "            # Normalize for cosine similarity\n",
    "            norms = np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-8\n",
    "            self._normalized_matrix = matrix / norms\n",
    "        else:\n",
    "            self._normalized_matrix = None\n",
    "        \n",
    "        self._needs_rebuild = False\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        top_k: int = 10,\n",
    "        modality_filter: str = None\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Search for similar items\"\"\"\n",
    "        self._rebuild_index()\n",
    "        \n",
    "        if self._normalized_matrix is None:\n",
    "            return []\n",
    "        \n",
    "        # Normalize query\n",
    "        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-8)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = self._normalized_matrix @ query_norm\n",
    "        \n",
    "        # Create results\n",
    "        results = []\n",
    "        indices = np.argsort(similarities)[::-1]\n",
    "        \n",
    "        for idx in indices:\n",
    "            item_id = self._id_list[idx]\n",
    "            \n",
    "            # Apply modality filter\n",
    "            if modality_filter and self.modalities[item_id] != modality_filter:\n",
    "                continue\n",
    "            \n",
    "            results.append(RetrievalResult(\n",
    "                item_id=item_id,\n",
    "                score=float(similarities[idx]),\n",
    "                modality=self.modalities[item_id],\n",
    "                metadata=self.metadata[item_id]\n",
    "            ))\n",
    "            \n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class CrossModalRetriever:\n",
    "    \"\"\"\n",
    "    Cross-modal retrieval system (e.g., text-to-image search).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model: CLIPModel,\n",
    "        embedding_dim: int = 512\n",
    "    ):\n",
    "        self.model = clip_model\n",
    "        self.index = MultimodalIndex(embedding_dim)\n",
    "        self.model.train(False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def index_images(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        image_ids: List[str],\n",
    "        metadata_list: List[Dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Index images for retrieval\"\"\"\n",
    "        embeddings = self.model.encode_image(images).cpu().numpy()\n",
    "        self.index.add_batch(image_ids, embeddings, \"image\", metadata_list)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        top_k: int = 10\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Search images by text query\"\"\"\n",
    "        text_embedding = self.model.encode_text(\n",
    "            input_ids, attention_mask\n",
    "        ).cpu().numpy()[0]\n",
    "        \n",
    "        return self.index.search(\n",
    "            text_embedding,\n",
    "            top_k=top_k,\n",
    "            modality_filter=\"image\"\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search_by_image(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        top_k: int = 10\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Search images by image query\"\"\"\n",
    "        image_embedding = self.model.encode_image(image).cpu().numpy()[0]\n",
    "        return self.index.search(image_embedding, top_k=top_k)\n",
    "\n",
    "\n",
    "# Example: Cross-modal retrieval\n",
    "retriever = CrossModalRetriever(clip_model, embedding_dim=256)\n",
    "\n",
    "# Index some images\n",
    "images = torch.randn(10, 3, 224, 224)\n",
    "image_ids = [f\"img_{i}\" for i in range(10)]\n",
    "metadata = [{\"category\": f\"cat_{i % 3}\"} for i in range(10)]\n",
    "\n",
    "retriever.index_images(images, image_ids, metadata)\n",
    "\n",
    "# Search by text\n",
    "query_ids = torch.randint(0, 10000, (1, 16))\n",
    "results = retriever.search_by_text(query_ids, top_k=5)\n",
    "\n",
    "print(\"Text-to-Image Search Results:\")\n",
    "for r in results:\n",
    "    print(f\"  {r.item_id}: score={r.score:.4f}, metadata={r.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Processing for ML\n",
    "\n",
    "Audio feature extraction and speech processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio feature extraction for ML models.\n",
    "    Extracts mel-spectrograms and learned features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 16000,\n",
    "        n_mels: int = 80,\n",
    "        n_fft: int = 400,\n",
    "        hop_length: int = 160,\n",
    "        embed_dim: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # Learned filterbanks (simplified - in production use torchaudio)\n",
    "        self.mel_filters = nn.Parameter(\n",
    "            torch.randn(n_mels, n_fft // 2 + 1) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Convolutional feature extractor\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(n_mels, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Transformer for temporal modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=512,\n",
    "            nhead=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        \n",
    "        # Projection\n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "    \n",
    "    def compute_spectrogram(\n",
    "        self,\n",
    "        waveform: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute mel-spectrogram from waveform.\n",
    "        Simplified - in production use torchaudio.transforms.MelSpectrogram\n",
    "        \"\"\"\n",
    "        # waveform: (batch, samples)\n",
    "        batch_size = waveform.shape[0]\n",
    "        \n",
    "        # Simulate STFT + mel filterbank\n",
    "        # In production: use torch.stft and proper mel filterbanks\n",
    "        n_frames = waveform.shape[1] // self.hop_length\n",
    "        \n",
    "        # Reshape and apply learned transform\n",
    "        mel_spec = torch.randn(\n",
    "            batch_size, self.n_mels, n_frames,\n",
    "            device=waveform.device\n",
    "        )\n",
    "        \n",
    "        # Log compression\n",
    "        mel_spec = torch.log(mel_spec.abs() + 1e-6)\n",
    "        \n",
    "        return mel_spec\n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract audio embeddings from waveform\"\"\"\n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = self.compute_spectrogram(waveform)\n",
    "        \n",
    "        # Convolutional features\n",
    "        features = self.conv_layers(mel_spec)  # (B, 512, T)\n",
    "        \n",
    "        # Transpose for transformer\n",
    "        features = features.transpose(1, 2)  # (B, T, 512)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        features = self.transformer(features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = features.mean(dim=1)\n",
    "        \n",
    "        # Project to embedding dimension\n",
    "        embedding = self.projection(pooled)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CTC-based speech recognition model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 1000,\n",
    "        n_mels: int = 80,\n",
    "        hidden_dim: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(n_mels, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim // 2,\n",
    "            num_layers=4,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        mel_spec: torch.Tensor,\n",
    "        input_lengths: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            mel_spec: (batch, n_mels, time)\n",
    "            input_lengths: Length of each sequence\n",
    "        \n",
    "        Returns:\n",
    "            Log probabilities: (batch, time, vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        features = self.encoder(mel_spec)  # (B, hidden, T)\n",
    "        features = features.transpose(1, 2)  # (B, T, hidden)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Output logits\n",
    "        logits = self.output(lstm_out)\n",
    "        \n",
    "        # Log softmax for CTC\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        return log_probs\n",
    "    \n",
    "    def decode_greedy(self, log_probs: torch.Tensor) -> List[List[int]]:\n",
    "        \"\"\"Greedy CTC decoding\"\"\"\n",
    "        # Get most likely tokens\n",
    "        predictions = torch.argmax(log_probs, dim=-1)  # (B, T)\n",
    "        \n",
    "        decoded = []\n",
    "        for seq in predictions:\n",
    "            # Remove consecutive duplicates\n",
    "            tokens = []\n",
    "            prev = -1\n",
    "            for token in seq.tolist():\n",
    "                if token != prev and token != 0:  # 0 = blank\n",
    "                    tokens.append(token)\n",
    "                prev = token\n",
    "            decoded.append(tokens)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Example: Audio Processing\n",
    "audio_encoder = AudioFeatureExtractor(embed_dim=256)\n",
    "\n",
    "# Simulate waveform\n",
    "waveform = torch.randn(4, 16000)  # 1 second at 16kHz\n",
    "audio_embedding = audio_encoder(waveform)\n",
    "print(f\"Audio embedding shape: {audio_embedding.shape}\")\n",
    "\n",
    "# Speech recognition example\n",
    "asr_model = SpeechRecognitionModel(vocab_size=100)\n",
    "mel_spec = torch.randn(4, 80, 100)  # Simulated mel spectrogram\n",
    "log_probs = asr_model(mel_spec)\n",
    "decoded = asr_model.decode_greedy(log_probs)\n",
    "print(f\"ASR output shape: {log_probs.shape}\")\n",
    "print(f\"Decoded tokens (first sample): {decoded[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Video Understanding\n",
    "\n",
    "Temporal modeling for video content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Video encoder with temporal modeling.\n",
    "    Uses a frame encoder + temporal transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_encoder: nn.Module = None,\n",
    "        frame_dim: int = 512,\n",
    "        num_frames: int = 16,\n",
    "        temporal_dim: int = 512,\n",
    "        num_temporal_layers: int = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Frame encoder (uses pretrained vision model)\n",
    "        if frame_encoder is None:\n",
    "            self.frame_encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64, frame_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.frame_encoder = frame_encoder\n",
    "        \n",
    "        # Temporal position embedding\n",
    "        self.temporal_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_frames, temporal_dim)\n",
    "        )\n",
    "        \n",
    "        # Projection to temporal dim\n",
    "        self.frame_proj = nn.Linear(frame_dim, temporal_dim)\n",
    "        \n",
    "        # Temporal transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=temporal_dim,\n",
    "            nhead=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_temporal_layers\n",
    "        )\n",
    "        \n",
    "        # CLS token for video-level representation\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, temporal_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(temporal_dim, temporal_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        video: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode video.\n",
    "        \n",
    "        Args:\n",
    "            video: (batch, num_frames, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            dict with video_embedding and frame_embeddings\n",
    "        \"\"\"\n",
    "        batch_size, num_frames = video.shape[:2]\n",
    "        \n",
    "        # Flatten batch and frames\n",
    "        frames = video.view(-1, *video.shape[2:])  # (B*T, C, H, W)\n",
    "        \n",
    "        # Encode frames\n",
    "        frame_features = self.frame_encoder(frames)  # (B*T, frame_dim)\n",
    "        frame_features = frame_features.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Project to temporal dimension\n",
    "        frame_features = self.frame_proj(frame_features)\n",
    "        \n",
    "        # Add temporal position embedding\n",
    "        frame_features = frame_features + self.temporal_pos_embed[:, :num_frames, :]\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        features = torch.cat([cls_tokens, frame_features], dim=1)\n",
    "        \n",
    "        # Temporal transformer\n",
    "        features = self.temporal_transformer(features)\n",
    "        \n",
    "        # Get video-level embedding from CLS token\n",
    "        video_embedding = self.output_proj(features[:, 0])\n",
    "        frame_embeddings = features[:, 1:]\n",
    "        \n",
    "        return {\n",
    "            \"video_embedding\": video_embedding,\n",
    "            \"frame_embeddings\": frame_embeddings\n",
    "        }\n",
    "\n",
    "\n",
    "class VideoTextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Video-text multimodal model for video understanding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        video_dim: int = 512,\n",
    "        text_dim: int = 512,\n",
    "        projection_dim: int = 256,\n",
    "        num_classes: int = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.video_encoder = VideoEncoder(temporal_dim=video_dim)\n",
    "        \n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Embedding(10000, 256),\n",
    "            nn.LSTM(256, text_dim // 2, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "        self.text_proj = nn.Linear(text_dim, projection_dim)\n",
    "        \n",
    "        # Video projection\n",
    "        self.video_proj = nn.Linear(video_dim, projection_dim)\n",
    "        \n",
    "        # Classification head (if needed)\n",
    "        if num_classes:\n",
    "            self.classifier = nn.Linear(projection_dim, num_classes)\n",
    "        else:\n",
    "            self.classifier = None\n",
    "        \n",
    "        # Temperature for contrastive learning\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    \n",
    "    def encode_video(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode video to embedding\"\"\"\n",
    "        video_output = self.video_encoder(video)\n",
    "        video_embedding = self.video_proj(video_output[\"video_embedding\"])\n",
    "        return F.normalize(video_embedding, dim=-1)\n",
    "    \n",
    "    def encode_text(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode text to embedding\"\"\"\n",
    "        embedded = self.text_encoder[0](input_ids)\n",
    "        lstm_out, _ = self.text_encoder[1](embedded)\n",
    "        # Take last hidden state\n",
    "        text_features = lstm_out[:, -1, :]\n",
    "        text_embedding = self.text_proj(text_features)\n",
    "        return F.normalize(text_embedding, dim=-1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        video: torch.Tensor,\n",
    "        input_ids: torch.Tensor = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        video_embedding = self.encode_video(video)\n",
    "        \n",
    "        outputs = {\"video_embedding\": video_embedding}\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            text_embedding = self.encode_text(input_ids)\n",
    "            outputs[\"text_embedding\"] = text_embedding\n",
    "            \n",
    "            # Compute similarity\n",
    "            logit_scale = self.logit_scale.exp()\n",
    "            outputs[\"logits\"] = logit_scale * video_embedding @ text_embedding.T\n",
    "        \n",
    "        if self.classifier is not None:\n",
    "            outputs[\"class_logits\"] = self.classifier(video_embedding)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Example: Video Understanding\n",
    "video_model = VideoTextModel(num_classes=10)\n",
    "\n",
    "# Simulate video input (4 videos, 16 frames each)\n",
    "video = torch.randn(4, 16, 3, 224, 224)\n",
    "text = torch.randint(0, 10000, (4, 32))\n",
    "\n",
    "outputs = video_model(video, text)\n",
    "print(f\"Video embedding shape: {outputs['video_embedding'].shape}\")\n",
    "print(f\"Text embedding shape: {outputs['text_embedding'].shape}\")\n",
    "print(f\"Classification logits shape: {outputs['class_logits'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Deployment Patterns\n",
    "\n",
    "Deploying multimodal systems at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalRequest:\n",
    "    \"\"\"Request for multimodal inference\"\"\"\n",
    "    request_id: str\n",
    "    modalities: Dict[str, Any]  # modality_name -> data\n",
    "    task: str  # 'embedding', 'retrieval', 'classification'\n",
    "    parameters: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalResponse:\n",
    "    \"\"\"Response from multimodal inference\"\"\"\n",
    "    request_id: str\n",
    "    results: Dict[str, Any]\n",
    "    latency_ms: float\n",
    "    modalities_used: List[str]\n",
    "\n",
    "\n",
    "class MultimodalServingPipeline:\n",
    "    \"\"\"\n",
    "    Production serving pipeline for multimodal models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        preprocessors: Dict[str, Callable] = None,\n",
    "        batch_size: int = 16,\n",
    "        max_wait_ms: float = 50\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.model.train(False)\n",
    "        self.preprocessors = preprocessors or {}\n",
    "        self.batch_size = batch_size\n",
    "        self.max_wait_ms = max_wait_ms\n",
    "        \n",
    "        # Request queue for batching\n",
    "        self.request_queue: List[MultimodalRequest] = []\n",
    "        self.metrics = {\n",
    "            \"total_requests\": 0,\n",
    "            \"total_latency_ms\": 0,\n",
    "            \"batch_sizes\": []\n",
    "        }\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        request: MultimodalRequest\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Preprocess request data\"\"\"\n",
    "        processed = {}\n",
    "        \n",
    "        for modality, data in request.modalities.items():\n",
    "            if modality in self.preprocessors:\n",
    "                processed[modality] = self.preprocessors[modality](data)\n",
    "            else:\n",
    "                # Default: assume already tensor\n",
    "                if isinstance(data, np.ndarray):\n",
    "                    processed[modality] = torch.from_numpy(data)\n",
    "                elif isinstance(data, torch.Tensor):\n",
    "                    processed[modality] = data\n",
    "                else:\n",
    "                    processed[modality] = torch.tensor(data)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def process_single(\n",
    "        self,\n",
    "        request: MultimodalRequest\n",
    "    ) -> MultimodalResponse:\n",
    "        \"\"\"Process a single request\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Preprocess\n",
    "        processed = self.preprocess(request)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        batched = {k: v.unsqueeze(0) for k, v in processed.items()}\n",
    "        \n",
    "        # Inference\n",
    "        outputs = self._run_inference(batched, request.task)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        results = {k: v[0] if isinstance(v, torch.Tensor) else v \n",
    "                   for k, v in outputs.items()}\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        self.metrics[\"total_requests\"] += 1\n",
    "        self.metrics[\"total_latency_ms\"] += latency_ms\n",
    "        \n",
    "        return MultimodalResponse(\n",
    "            request_id=request.request_id,\n",
    "            results=self._postprocess_results(results),\n",
    "            latency_ms=latency_ms,\n",
    "            modalities_used=list(request.modalities.keys())\n",
    "        )\n",
    "    \n",
    "    def _run_inference(\n",
    "        self,\n",
    "        inputs: Dict[str, torch.Tensor],\n",
    "        task: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run model inference\"\"\"\n",
    "        # Route to appropriate model method based on task\n",
    "        if task == \"embedding\":\n",
    "            if \"image\" in inputs:\n",
    "                return {\"embedding\": self.model.encode_image(inputs[\"image\"])}\n",
    "            elif \"text\" in inputs:\n",
    "                return {\"embedding\": self.model.encode_text(inputs[\"text\"])}\n",
    "        \n",
    "        # Default: full forward pass\n",
    "        return self.model(**inputs)\n",
    "    \n",
    "    def _postprocess_results(\n",
    "        self,\n",
    "        results: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Convert results to serializable format\"\"\"\n",
    "        processed = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                processed[key] = value.cpu().numpy().tolist()\n",
    "            else:\n",
    "                processed[key] = value\n",
    "        return processed\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get serving metrics\"\"\"\n",
    "        total = self.metrics[\"total_requests\"]\n",
    "        return {\n",
    "            \"total_requests\": total,\n",
    "            \"avg_latency_ms\": (\n",
    "                self.metrics[\"total_latency_ms\"] / total if total > 0 else 0\n",
    "            ),\n",
    "            \"avg_batch_size\": (\n",
    "                np.mean(self.metrics[\"batch_sizes\"]) \n",
    "                if self.metrics[\"batch_sizes\"] else 0\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "class MultimodalModelOptimizer:\n",
    "    \"\"\"\n",
    "    Optimization techniques for multimodal models.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_model(\n",
    "        model: nn.Module,\n",
    "        quantization_type: str = \"dynamic\"\n",
    "    ) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Quantize model for faster inference.\n",
    "        \"\"\"\n",
    "        if quantization_type == \"dynamic\":\n",
    "            return torch.quantization.quantize_dynamic(\n",
    "                model,\n",
    "                {nn.Linear, nn.LSTM},\n",
    "                dtype=torch.qint8\n",
    "            )\n",
    "        else:\n",
    "            # For static quantization, would need calibration data\n",
    "            return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_onnx(\n",
    "        model: nn.Module,\n",
    "        sample_inputs: Dict[str, torch.Tensor],\n",
    "        output_path: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Export model to ONNX format.\n",
    "        \"\"\"\n",
    "        # For demo purposes, just print\n",
    "        print(f\"Exporting model to ONNX: {output_path}\")\n",
    "        print(f\"Input shapes: {[(k, v.shape) for k, v in sample_inputs.items()]}\")\n",
    "        \n",
    "        # In production:\n",
    "        # torch.onnx.export(\n",
    "        #     model,\n",
    "        #     tuple(sample_inputs.values()),\n",
    "        #     output_path,\n",
    "        #     input_names=list(sample_inputs.keys()),\n",
    "        #     dynamic_axes={k: {0: 'batch'} for k in sample_inputs.keys()}\n",
    "        # )\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_embeddings(\n",
    "        model: nn.Module,\n",
    "        data_loader,\n",
    "        modality: str,\n",
    "        cache_path: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Pre-compute and cache embeddings for static content.\n",
    "        \"\"\"\n",
    "        model.train(False)\n",
    "        all_embeddings = []\n",
    "        all_ids = []\n",
    "        \n",
    "        # Simulate caching\n",
    "        print(f\"Caching {modality} embeddings to {cache_path}\")\n",
    "        print(\"Would iterate through data_loader and compute embeddings...\")\n",
    "\n",
    "\n",
    "# Example: Production Serving\n",
    "serving_pipeline = MultimodalServingPipeline(\n",
    "    model=clip_model,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Simulate request\n",
    "request = MultimodalRequest(\n",
    "    request_id=\"req_001\",\n",
    "    modalities={\n",
    "        \"image\": torch.randn(3, 224, 224)\n",
    "    },\n",
    "    task=\"embedding\"\n",
    ")\n",
    "\n",
    "response = serving_pipeline.process_single(request)\n",
    "print(f\"\\nServing Response:\")\n",
    "print(f\"  Request ID: {response.request_id}\")\n",
    "print(f\"  Latency: {response.latency_ms:.2f}ms\")\n",
    "print(f\"  Modalities: {response.modalities_used}\")\n",
    "print(f\"  Embedding dims: {len(response.results['embedding'])}\")\n",
    "\n",
    "print(f\"\\nServing Metrics: {serving_pipeline.get_metrics()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAANG Interview Questions\n",
    "\n",
    "### Q1: How would you design a multimodal search system (e.g., search images with text)?\n",
    "\n",
    "**Answer:**\n",
    "I would design a CLIP-style contrastive learning architecture:\n",
    "\n",
    "1. **Encoders**: Separate encoders for each modality (ViT for images, Transformer for text)\n",
    "2. **Shared Embedding Space**: Project all modalities to same dimension with normalization\n",
    "3. **Training**: Contrastive loss (InfoNCE) on image-text pairs\n",
    "4. **Indexing**: Pre-compute image embeddings, store in vector database (FAISS/Milvus)\n",
    "5. **Retrieval**: Encode query text, find nearest neighbors in embedding space\n",
    "6. **Optimization**: \n",
    "   - Quantize embeddings (int8) for memory efficiency\n",
    "   - Use approximate nearest neighbor (HNSW) for speed\n",
    "   - Cache hot embeddings in Redis\n",
    "\n",
    "### Q2: What are the challenges of deploying multimodal models in production?\n",
    "\n",
    "**Answer:**\n",
    "Key challenges:\n",
    "\n",
    "1. **Latency**: Multiple modality encoders increase inference time\n",
    "   - Solution: Parallel encoding, model distillation, caching\n",
    "\n",
    "2. **Memory**: Large models and embeddings\n",
    "   - Solution: Quantization, model pruning, gradient checkpointing\n",
    "\n",
    "3. **Missing Modalities**: Not all inputs have all modalities\n",
    "   - Solution: Modality-agnostic training, graceful degradation\n",
    "\n",
    "4. **Synchronization**: Aligning temporal modalities (audio-video)\n",
    "   - Solution: Cross-modal attention, contrastive pre-training\n",
    "\n",
    "5. **Data Pipeline**: Complex preprocessing for multiple formats\n",
    "   - Solution: Modular preprocessors, streaming pipelines\n",
    "\n",
    "### Q3: How do you handle temporal alignment in video-audio models?\n",
    "\n",
    "**Answer:**\n",
    "Temporal alignment strategies:\n",
    "\n",
    "1. **Synchronized Sampling**: Sample frames and audio at fixed intervals\n",
    "2. **Cross-Modal Attention**: Let modalities attend to each other's features\n",
    "3. **Contrastive Learning**: Train with synchronized pairs vs. misaligned negatives\n",
    "4. **Temporal Transformers**: Model temporal dependencies across modalities\n",
    "5. **Feature Interpolation**: Resample features to common temporal resolution\n",
    "\n",
    "Key considerations:\n",
    "- Audio typically has higher temporal resolution (16kHz) vs. video (30fps)\n",
    "- Use mel-spectrograms to bridge audio-visual representations\n",
    "- Consider temporal jittering for robust training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Vision-Language Models**: CLIP-style contrastive learning\n",
    "2. **Multimodal Fusion**: Concat, attention, and gated fusion strategies\n",
    "3. **Cross-Modal Retrieval**: Production search systems with vector indexing\n",
    "4. **Audio Processing**: Feature extraction and speech recognition\n",
    "5. **Video Understanding**: Temporal modeling with transformers\n",
    "6. **Production Deployment**: Serving pipelines and optimization\n",
    "\n",
    "### Key Takeaways for FAANG Interviews:\n",
    "- Multimodal models require unified embedding spaces for cross-modal tasks\n",
    "- Contrastive learning is the dominant paradigm for vision-language models\n",
    "- Fusion strategies depend on task: early fusion for dense prediction, late fusion for classification\n",
    "- Production systems need efficient indexing, caching, and batching\n",
    "- Handle missing modalities gracefully in real-world deployments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
