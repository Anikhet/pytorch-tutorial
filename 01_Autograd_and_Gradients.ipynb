{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Autograd and Gradients\n",
    "\n",
    "This notebook covers one of PyTorch's most powerful features: **automatic differentiation**. This is what makes training neural networks possible!\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what gradients are and why they're important\n",
    "- Learn how PyTorch automatically computes gradients\n",
    "- Use `requires_grad` to enable gradient tracking\n",
    "- Compute gradients manually and automatically\n",
    "- Understand the relationship between gradients and backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Gradient?\n",
    "\n",
    "In simple terms, a **gradient** tells us:\n",
    "- **Which direction** to change our parameters to improve our model\n",
    "- **How much** to change them\n",
    "\n",
    "Think of it like hiking: the gradient points uphill. In machine learning, we want to go \"downhill\" (minimize error), so we move in the **opposite direction** of the gradient.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a function f(x), the gradient (derivative) tells us the rate of change:\n",
    "- If gradient is positive â†’ function increases as x increases\n",
    "- If gradient is negative â†’ function decreases as x increases\n",
    "- If gradient is zero â†’ we're at a minimum or maximum\n",
    "\n",
    "**In neural networks**: We use gradients to update weights and biases to minimize the loss (error).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "Let's import what we need and set up for gradient computation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.autograd' has no attribute 'is_available'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutograd available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.autograd' has no attribute 'is_available'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding requires_grad\n",
    "\n",
    "To compute gradients, PyTorch needs to track operations on tensors. We enable this with `requires_grad=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regular tensor (no gradient tracking)\n",
    "x_normal = torch.tensor(2.0)\n",
    "print(\"Normal tensor:\", x_normal)\n",
    "print(\"requires_grad:\", x_normal.requires_grad)\n",
    "print()\n",
    "\n",
    "# Create a tensor with gradient tracking enabled\n",
    "# This tells PyTorch: \"I want to compute gradients with respect to this tensor\"\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(\"Tensor with gradient tracking:\")\n",
    "print(\"Value:\", x)\n",
    "print(\"requires_grad:\", x.requires_grad)\n",
    "print()\n",
    "\n",
    "# Note: When requires_grad=True, PyTorch builds a computation graph\n",
    "# This graph tracks all operations so gradients can be computed later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example: Computing a Gradient\n",
    "\n",
    "Let's start with a simple function: y = xÂ²\n",
    "\n",
    "We'll compute the gradient of y with respect to x:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that we want to compute gradients for\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(\"x =\", x)\n",
    "print()\n",
    "\n",
    "# Define a function: y = x^2\n",
    "y = x ** 2\n",
    "print(\"y = x^2 =\", y)\n",
    "print()\n",
    "\n",
    "# Now compute the gradient!\n",
    "# This computes dy/dx (derivative of y with respect to x)\n",
    "y.backward()  # This triggers gradient computation\n",
    "\n",
    "# Access the gradient\n",
    "print(\"Gradient dy/dx =\", x.grad)\n",
    "print()\n",
    "\n",
    "# Let's verify: For y = x^2, dy/dx = 2x\n",
    "# At x = 3, dy/dx = 2 * 3 = 6\n",
    "print(\"Manual calculation: 2 * x = 2 * 3 =\", 2 * 3)\n",
    "print(\"PyTorch gradient:\", x.grad.item())\n",
    "print(\"Match!\", abs(x.grad.item() - 6) < 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Zeroing Gradients\n",
    "\n",
    "**Critical point**: Gradients accumulate! If you call `.backward()` multiple times, gradients add up. Always zero gradients before computing new ones (we'll see why this matters in training):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing gradient accumulation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# First backward pass\n",
    "y.backward()\n",
    "print(\"After first backward():\", x.grad)\n",
    "\n",
    "# Second backward pass (gradients accumulate!)\n",
    "y.backward()  # This will add to existing gradients\n",
    "print(\"After second backward():\", x.grad)  # Notice it doubled!\n",
    "\n",
    "# Zero the gradients\n",
    "x.grad.zero_()  # The underscore means \"in-place operation\"\n",
    "print(\"After zeroing:\", x.grad)\n",
    "\n",
    "# Now compute again\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(\"After new backward():\", x.grad)  # Back to correct value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Example: Multiple Variables\n",
    "\n",
    "Let's compute gradients for a function with multiple variables: z = xÂ² + yÂ²\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print()\n",
    "\n",
    "# Define function: z = x^2 + y^2\n",
    "z = x ** 2 + y ** 2\n",
    "print(\"z = x^2 + y^2 =\", z)\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access gradients for both variables\n",
    "print(\"Gradient with respect to x (dz/dx):\", x.grad)\n",
    "print(\"Gradient with respect to y (dz/dy):\", y.grad)\n",
    "print()\n",
    "\n",
    "# Verify manually:\n",
    "# dz/dx = 2x = 2 * 2 = 4\n",
    "# dz/dy = 2y = 2 * 3 = 6\n",
    "print(\"Manual check:\")\n",
    "print(\"dz/dx = 2x =\", 2 * 2)\n",
    "print(\"dz/dy = 2y =\", 2 * 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Gradients\n",
    "\n",
    "Let's visualize what gradients mean by plotting a function and its gradient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize y = x^2 and its gradient at different points\n",
    "x_values = np.linspace(-3, 3, 100)\n",
    "y_values = x_values ** 2\n",
    "\n",
    "# Compute gradients at a few points\n",
    "gradient_points = [-2, -1, 0, 1, 2]\n",
    "gradients = []\n",
    "\n",
    "for point in gradient_points:\n",
    "    x = torch.tensor(float(point), requires_grad=True)\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    gradients.append(x.grad.item())\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, y_values, 'b-', linewidth=2, label='y = xÂ²')\n",
    "plt.scatter(gradient_points, [p**2 for p in gradient_points], \n",
    "           color='red', s=100, zorder=5, label='Points where we computed gradients')\n",
    "\n",
    "# Draw gradient arrows (tangent lines)\n",
    "for i, point in enumerate(gradient_points):\n",
    "    # Gradient is the slope: dy/dx = 2x\n",
    "    slope = gradients[i]\n",
    "    # Draw a small line showing the gradient direction\n",
    "    x_line = np.array([point - 0.5, point + 0.5])\n",
    "    y_line = point**2 + slope * (x_line - point)\n",
    "    plt.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Function y = xÂ² and its Gradients', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: At x=0, the gradient is 0 (minimum point)\")\n",
    "print(\"For x>0, gradient is positive (function increases)\")\n",
    "print(\"For x<0, gradient is negative (function decreases)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent: The Core of Training\n",
    "\n",
    "**Gradient Descent** is the algorithm used to train neural networks. The idea is simple:\n",
    "\n",
    "1. Compute the gradient (which direction to go)\n",
    "2. Move in the **opposite** direction (to minimize loss)\n",
    "3. Repeat until we reach a minimum\n",
    "\n",
    "Let's implement a simple gradient descent to find the minimum of y = xÂ²:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent to find minimum of y = x^2\n",
    "# We know the minimum is at x = 0, but let's find it using gradient descent!\n",
    "\n",
    "# Starting point (we'll start far from the minimum)\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "# Learning rate: how big steps we take\n",
    "# Too large: might overshoot the minimum\n",
    "# Too small: takes too long to converge\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Store history for visualization\n",
    "history = []\n",
    "\n",
    "print(\"Gradient Descent to minimize y = xÂ²\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Starting point: x = {x.item():.2f}\")\n",
    "\n",
    "# Perform gradient descent for several steps\n",
    "for step in range(20):\n",
    "    # Compute function value\n",
    "    y = x ** 2\n",
    "    \n",
    "    # Compute gradient\n",
    "    y.backward()\n",
    "    \n",
    "    # Update x: move in opposite direction of gradient\n",
    "    # x_new = x_old - learning_rate * gradient\n",
    "    with torch.no_grad():  # We don't want to track gradients for this update\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Zero gradients for next iteration\n",
    "    x.grad.zero_()\n",
    "    \n",
    "    # Store for visualization\n",
    "    history.append((x.item(), y.item()))\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: x = {x.item():.4f}, y = {y.item():.4f}, gradient = {x.grad if x.grad is None else 'zeroed'}\")\n",
    "\n",
    "print(f\"\\nFinal: x = {x.item():.4f}, y = {y.item():.4f}\")\n",
    "print(f\"Expected minimum: x = 0.0, y = 0.0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Gradient Descent\n",
    "\n",
    "Let's see how gradient descent converges to the minimum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y values from history\n",
    "x_history = [h[0] for h in history]\n",
    "y_history = [h[1] for h in history]\n",
    "\n",
    "# Plot the function and the descent path\n",
    "x_plot = np.linspace(-1, 5.5, 100)\n",
    "y_plot = x_plot ** 2\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Function and path\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='y = xÂ²')\n",
    "plt.plot(x_history, y_history, 'ro-', linewidth=2, markersize=8, label='Gradient Descent Path')\n",
    "plt.scatter([x_history[0]], [y_history[0]], color='green', s=200, marker='*', \n",
    "           label='Start', zorder=5)\n",
    "plt.scatter([x_history[-1]], [y_history[-1]], color='red', s=200, marker='*', \n",
    "           label='End', zorder=5)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Gradient Descent Path', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(y_history)), y_history, 'ro-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('y value', fontsize=12)\n",
    "plt.title('Convergence to Minimum', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the algorithm converges to x=0, y=0 (the minimum)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Computation Graph\n",
    "\n",
    "PyTorch builds a **computation graph** to track operations. This graph is used to compute gradients via backpropagation.\n",
    "\n",
    "Let's understand this with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: z = (x + y) * (x - y)\n",
    "# Let's trace through the computation graph\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(f\"x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y.item()}, requires_grad = {y.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Intermediate computations\n",
    "a = x + y  # a = 3 + 2 = 5\n",
    "b = x - y  # b = 3 - 2 = 1\n",
    "\n",
    "print(\"Intermediate values:\")\n",
    "print(f\"a = x + y = {a.item()}\")\n",
    "print(f\"b = x - y = {b.item()}\")\n",
    "print(f\"a.requires_grad = {a.requires_grad}\")  # Automatically True because it depends on x, y\n",
    "print()\n",
    "\n",
    "# Final computation\n",
    "z = a * b  # z = 5 * 1 = 5\n",
    "\n",
    "print(\"Final value:\")\n",
    "print(f\"z = a * b = {z.item()}\")\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(f\"dz/dx = {x.grad.item()}\")\n",
    "print(f\"dz/dy = {y.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Manual verification:\n",
    "# z = (x+y)(x-y) = xÂ² - yÂ²\n",
    "# dz/dx = 2x = 2*3 = 6\n",
    "# dz/dy = -2y = -2*2 = -4\n",
    "print(\"Manual check:\")\n",
    "print(f\"dz/dx = 2x = {2*3}\")\n",
    "print(f\"dz/dy = -2y = {-2*2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching from the Computation Graph\n",
    "\n",
    "Sometimes you want to stop tracking gradients. Use `.detach()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(\"Before detaching:\")\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")\n",
    "print(f\"y.grad_fn = {y.grad_fn}\")  # grad_fn shows the operation that created this tensor\n",
    "print()\n",
    "\n",
    "# Detach: creates a new tensor without gradient tracking\n",
    "y_detached = y.detach()\n",
    "\n",
    "print(\"After detaching:\")\n",
    "print(f\"y_detached.requires_grad = {y_detached.requires_grad}\")\n",
    "print(f\"y_detached.grad_fn = {y_detached.grad_fn}\")  # None because no gradient tracking\n",
    "print()\n",
    "\n",
    "# Common use case: when you want to use tensor values without tracking gradients\n",
    "# For example, when evaluating a model (we'll see this in training notebooks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.no_grad()\n",
    "\n",
    "The `torch.no_grad()` context manager disables gradient tracking. This is useful when:\n",
    "- Evaluating models (we don't need gradients)\n",
    "- Updating parameters (we don't want to track the update itself)\n",
    "- Saving memory and computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing values without tracking gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Normal computation (tracks gradients)\n",
    "y1 = x ** 2\n",
    "print(\"With gradient tracking:\")\n",
    "print(f\"y1.requires_grad = {y1.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Using no_grad context (doesn't track gradients)\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 2\n",
    "    print(\"Inside no_grad context:\")\n",
    "    print(f\"y2.requires_grad = {y2.requires_grad}\")\n",
    "    print(f\"y2 value = {y2.item()}\")\n",
    "\n",
    "print()\n",
    "print(\"Outside no_grad context:\")\n",
    "print(f\"y2.requires_grad = {y2.requires_grad}\")  # Still False\n",
    "print()\n",
    "\n",
    "# Common use: when evaluating models or updating parameters\n",
    "# We'll use this extensively in training loops!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Multiple Outputs\n",
    "\n",
    "Sometimes you compute gradients for multiple outputs from the same input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multiple outputs from one input\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Multiple outputs\n",
    "y1 = x ** 2\n",
    "y2 = x ** 3\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output 1 (xÂ²):\", y1)\n",
    "print(\"Output 2 (xÂ³):\", y2)\n",
    "print()\n",
    "\n",
    "# If we want gradients for both, we can compute them separately\n",
    "# Or use a combined output\n",
    "\n",
    "# Method 1: Compute gradients for y1\n",
    "y1.sum().backward(retain_graph=True)  # retain_graph keeps graph for second backward\n",
    "print(\"Gradients after y1.backward():\")\n",
    "print(\"x.grad =\", x.grad)\n",
    "\n",
    "# Zero and compute for y2\n",
    "x.grad.zero_()\n",
    "y2.sum().backward()\n",
    "print(\"\\nGradients after y2.backward():\")\n",
    "print(\"x.grad =\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Simple Gradient\n",
    "1. Create a tensor `x = 4.0` with `requires_grad=True`\n",
    "2. Compute `y = 3xÂ² + 2x + 1`\n",
    "3. Compute the gradient dy/dx\n",
    "4. Verify manually: dy/dx = 6x + 2 = 6(4) + 2 = 26\n",
    "\n",
    "### Exercise 2: Multiple Variables\n",
    "1. Create `x = 2.0` and `y = 3.0`, both with `requires_grad=True`\n",
    "2. Compute `z = xÂ²y + xyÂ²`\n",
    "3. Compute gradients with respect to both x and y\n",
    "4. Verify: dz/dx = 2xy + yÂ², dz/dy = xÂ² + 2xy\n",
    "\n",
    "### Exercise 3: Gradient Descent\n",
    "1. Implement gradient descent to minimize `f(x) = (x - 3)Â²`\n",
    "2. Start at x = 0\n",
    "3. Use learning rate = 0.1\n",
    "4. Run for 20 iterations\n",
    "5. Verify you converge to x = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to Exercises\n",
    "\n",
    "### Exercise 1 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = 3 * x ** 2 + 2 * x + 1\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = 3xÂ² + 2x + 1 = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient dy/dx = {x.grad.item()}\")\n",
    "print(f\"Manual check: 6x + 2 = {6*4 + 2}\")\n",
    "print(f\"Match: {abs(x.grad.item() - 26) < 0.0001}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "z = x ** 2 * y + x * y ** 2\n",
    "\n",
    "print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "print(f\"z = xÂ²y + xyÂ² = {z.item()}\")\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\nGradient dz/dx = {x.grad.item()}\")\n",
    "print(f\"Gradient dz/dy = {y.grad.item()}\")\n",
    "\n",
    "print(f\"\\nManual check:\")\n",
    "print(f\"dz/dx = 2xy + yÂ² = 2(2)(3) + 3Â² = {2*2*3 + 3*3}\")\n",
    "print(f\"dz/dy = xÂ² + 2xy = 2Â² + 2(2)(3) = {2*2 + 2*2*3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution: Gradient descent for f(x) = (x - 3)Â²\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"Gradient Descent to minimize f(x) = (x - 3)Â²\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for step in range(20):\n",
    "    f = (x - 3) ** 2\n",
    "    f.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    x.grad.zero_()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: x = {x.item():.4f}, f(x) = {f.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: x = {x.item():.4f} (expected: 3.0)\")\n",
    "print(f\"Converged: {abs(x.item() - 3.0) < 0.1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradients** tell us how to update parameters to minimize loss\n",
    "2. **requires_grad=True** enables gradient tracking for a tensor\n",
    "3. **.backward()** computes gradients using backpropagation\n",
    "4. **Gradient Descent**: Update parameters by moving opposite to the gradient\n",
    "5. **Always zero gradients** (`.grad.zero_()`) before computing new ones\n",
    "6. **torch.no_grad()** disables gradient tracking (faster, less memory)\n",
    "7. **Computation Graph**: PyTorch automatically builds a graph to track operations\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook, we'll learn about:\n",
    "- **Building Neural Networks**: Using `nn.Module` to create network architectures\n",
    "- **Layers**: Linear, convolutional, activation functions, and more\n",
    "- **Putting it together**: Building your first neural network\n",
    "\n",
    "The gradient computation we learned here is exactly what happens when training neural networks - PyTorch automatically computes gradients for all parameters!\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You now understand automatic differentiation! ðŸŽ‰**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
