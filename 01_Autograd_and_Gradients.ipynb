{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bbf986",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Autograd and Gradients\n",
    "\n",
    "Welcome to the \"Engine Room\" of PyTorch. \n",
    "\n",
    "In this notebook, we will learn how machines actually learn. But before we talk about mountains or hikers, we need to define the **Language of Learning**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Loss Function\" and \"Learning Rate\"?\n",
    "- **The Intuition**: The \"Blindfolded Hiker\" analogy (Now that we know the terms).\n",
    "- **The Mechanism**: The \"Recording Tape\" analogy (requires_grad).\n",
    "- **The Math**: What exactly is a gradient? (Sensitivity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efe11f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4931d",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Before we can understand *how* AI learns, we need to agree on what \"Learning\" means.\n",
    "\n",
    "### 1. The Loss Function (The Scorecard)\n",
    "Imagine you are taking a test. You get a score of 80/100. Your \"Error\" is 20.\n",
    "In AI, we call this \"Error\" the **Loss**.\n",
    "\n",
    "- **High Loss** = The model is doing a bad job (e.g., calling a Cat a Dog).\n",
    "- **Low Loss** = The model is doing a good job.\n",
    "- **Goal**: Make the Loss as close to **0** as possible.\n",
    "\n",
    "### 2. The Learning Rate (The Step Size)\n",
    "When the model realizes it made a mistake, it needs to change its numbers (weights) to fix it.\n",
    "But *how much* should it change them?\n",
    "\n",
    "- **Learning Rate**: A tiny number (usually 0.01 or 0.001) that controls how big of a change we make.\n",
    "- **Too Big**: You might overshoot the correct answer.\n",
    "- **Too Small**: It will take forever to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638a197",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (The Blindfolded Hiker)\n",
    "\n",
    "Now that we know the terms, let's visualize them.\n",
    "\n",
    "Imagine you are standing on a mountain at night. You are blindfolded. Your goal is to get to the very bottom of the valley.\n",
    "\n",
    "- **The Mountain**: This is the **Loss Function**. High elevation means High Error. Low elevation means Low Error.\n",
    "- **Your Location**: This represents the current **Parameters** (Weights) of your model.\n",
    "- **The Slope**: This is the **Gradient**. It tells you which way is \"Up\".\n",
    "- **The Step**: This is the **Learning Rate**. You take a step *opposite* to the slope to go down.\n",
    "\n",
    "**The Algorithm (Gradient Descent):**\n",
    "1. Feel the slope with your feet (Calculate Gradient).\n",
    "2. Turn around to face downhill (Negative Gradient).\n",
    "3. Take a small step (Learning Rate).\n",
    "4. Repeat until you are at the bottom (Loss = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a64e9",
   "metadata": {},
   "source": [
    "## Part 3: The Mechanism (The Recording Tape)\n",
    "\n",
    "How does PyTorch know the slope of a complex mountain?\n",
    "\n",
    "Imagine a cassette tape recorder. \n",
    "\n",
    "When you create a tensor with `requires_grad=True`, you press **RECORD**.\n",
    "PyTorch silently watches every operation you do (addition, multiplication, power) and writes it down on a \"tape\" (the Computation Graph).\n",
    "\n",
    "When you call `.backward()`, PyTorch **rewinds the tape** and calculates the derivatives step-by-step using the Chain Rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0770f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.0\n",
      "Output: 18.75\n",
      "Function that created 'out': <DivBackward0 object at 0x13ccb3ca0>\n",
      "Function that created 'z':   <MulBackward0 object at 0x13ccb3ca0>\n",
      "Function that created 'y':   <AddBackward0 object at 0x13ccb3ca0>\n"
     ]
    }
   ],
   "source": [
    "# Let's see the \"Recording Tape\" in action\n",
    "\n",
    "# 1. Create a tensor and press RECORD\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# 2. Do some operations (PyTorch is recording!)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z / 4\n",
    "\n",
    "print(f\"Output: {out}\")\n",
    "\n",
    "# 3. Look at the tape (The 'grad_fn')\n",
    "# This tells us what operation created this variable\n",
    "print(f\"Function that created 'out': {out.grad_fn}\")\n",
    "print(f\"Function that created 'z':   {z.grad_fn}\")\n",
    "print(f\"Function that created 'y':   {y.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05274525",
   "metadata": {},
   "source": [
    "## Part 4: The Math (Sensitivity)\n",
    "\n",
    "What does `x.grad = 4.5` actually mean?\n",
    "\n",
    "It means **Sensitivity**.\n",
    "\n",
    "If `x.grad` is 4.5, it means:\n",
    "\"If I increase `x` by a tiny amount (0.001), the output `y` will increase by **4.5 times** that amount.\"\n",
    "\n",
    "$$ \\frac{dy}{dx} \\approx \\frac{\\Delta y}{\\Delta x} $$\n",
    "\n",
    "Let's prove this with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51aef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At x=4, the gradient is 8.0\n",
      "This means if we nudge x by +0.001, y should change by approx 0.008 (8 * 0.001)\n",
      "Actual change in y:    0.00800\n",
      "Predicted change (8*d): 0.00800\n",
      "See? The gradient predicts how the output responds to input changes!\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function y = x^2\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "gradient = x.grad.item()\n",
    "\n",
    "print(f\"At x=4, the gradient is {gradient}\")\n",
    "print(\"This means if we nudge x by +0.001, y should change by approx 0.008 (8 * 0.001)\")\n",
    "\n",
    "# Let's verify manually\n",
    "delta = 0.001\n",
    "x_nudged = 4.0 + delta\n",
    "y_nudged = x_nudged ** 2\n",
    "\n",
    "y_original = 4.0 ** 2\n",
    "actual_change = y_nudged - y_original\n",
    "predicted_change = gradient * delta\n",
    "\n",
    "print(f\"Actual change in y:    {actual_change:.5f}\")\n",
    "print(f\"Predicted change (8*d): {predicted_change:.5f}\")\n",
    "print(\"See? The gradient predicts how the output responds to input changes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a347a7",
   "metadata": {},
   "source": [
    "## Part 5: The \"Gotcha\" - Accumulating Gradients\n",
    "\n",
    "This is the #1 bug for beginners.\n",
    "\n",
    "PyTorch **accumulates** (adds) gradients by default. It doesn't replace them. This is useful for RNNs, but bad for standard training.\n",
    "\n",
    "**ALWAYS zero your gradients before the next step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2636cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Gradients: tensor([3., 3.])\n",
      "Step 1, Gradients: tensor([6., 6.])\n",
      "Step 2, Gradients: tensor([9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(2, requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    loss = (weights * 3).sum()\n",
    "    loss.backward()\n",
    "    print(f\"Step {i}, Gradients: {weights.grad}\")\n",
    "    \n",
    "    # If we don't zero them, they just keep growing!\n",
    "    # Uncomment the line below to fix it:\n",
    "    # weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015b599",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **Loss Function** = The Scorecard (Lower is better).\n",
    "2. **Learning Rate** = The Step Size (Don't be too greedy).\n",
    "3. **Gradient** = The Slope (Direction of steepest ascent).\n",
    "4. **Gradient Descent** = Go opposite the gradient to find the bottom.\n",
    "5. **requires_grad=True** = Turn on the \"Recording Tape\".\n",
    "\n",
    "You are now ready to build Neural Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
