{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases and Similarity Search\n",
    "\n",
    "## Overview\n",
    "This notebook covers production-grade vector database implementations using:\n",
    "- **Milvus**: Distributed vector database for billion-scale searches\n",
    "- **Qdrant**: High-performance vector search engine\n",
    "- **FAISS**: Facebook's similarity search library\n",
    "\n",
    "## Why Vector Databases?\n",
    "- **Semantic Search**: Find similar items based on meaning, not keywords\n",
    "- **Recommendation Systems**: Find products/content similar to user preferences\n",
    "- **RAG Systems**: Retrieve relevant context for LLM prompts\n",
    "- **Anomaly Detection**: Identify outliers in high-dimensional spaces\n",
    "\n",
    "## Interview Focus\n",
    "- Indexing strategies (IVF, HNSW, LSH)\n",
    "- Distance metrics (L2, cosine, dot product)\n",
    "- Scalability and sharding\n",
    "- Production deployment patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation\n",
    "# pip install pymilvus qdrant-client faiss-cpu sentence-transformers\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Embeddings with MiniLM\n",
    "\n",
    "First, we'll generate embeddings using a production-grade model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EmbeddingService:\n",
    "    \"\"\"Production embedding service with caching and batch processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.cache = {}\n",
    "        \n",
    "    def embed(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings with batching for efficiency.\"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True  # For cosine similarity\n",
    "        )\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Fast single query embedding with caching.\"\"\"\n",
    "        if query in self.cache:\n",
    "            return self.cache[query]\n",
    "        \n",
    "        embedding = self.model.encode([query], normalize_embeddings=True)[0]\n",
    "        self.cache[query] = embedding\n",
    "        return embedding\n",
    "\n",
    "# Initialize service\n",
    "embedding_service = EmbeddingService()\n",
    "print(f\"Embedding dimension: {embedding_service.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample dataset: Product descriptions\n",
    "documents = [\n",
    "    \"High-performance laptop with 32GB RAM and RTX 4090 GPU\",\n",
    "    \"Wireless noise-cancelling headphones with 30-hour battery\",\n",
    "    \"Ergonomic office chair with lumbar support and adjustable height\",\n",
    "    \"4K gaming monitor with 144Hz refresh rate and HDR\",\n",
    "    \"Mechanical keyboard with RGB backlighting and Cherry MX switches\",\n",
    "    \"Portable SSD with 2TB storage and USB-C connectivity\",\n",
    "    \"Standing desk converter with electric height adjustment\",\n",
    "    \"Webcam with 1080p resolution and auto-focus capability\",\n",
    "    \"Docking station with dual 4K display support\",\n",
    "    \"Smartphone with 256GB storage and 5G connectivity\"\n",
    "] * 100  # Replicate for larger dataset\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_service.embed(documents)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: FAISS - Fast Similarity Search\n",
    "\n",
    "FAISS is Meta's library optimized for billion-scale similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FAISSVectorStore:\n",
    "    \"\"\"Production FAISS implementation with multiple index types.\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int, index_type: str = 'flat'):\n",
    "        self.dimension = dimension\n",
    "        self.index_type = index_type\n",
    "        self.index = self._create_index()\n",
    "        self.documents = []\n",
    "        \n",
    "    def _create_index(self):\n",
    "        \"\"\"Create appropriate FAISS index based on scale and requirements.\"\"\"\n",
    "        if self.index_type == 'flat':\n",
    "            # Exact search, best for <1M vectors\n",
    "            return faiss.IndexFlatIP(self.dimension)  # Inner Product (cosine if normalized)\n",
    "        \n",
    "        elif self.index_type == 'ivf':\n",
    "            # Inverted File Index, good for 1M-10M vectors\n",
    "            quantizer = faiss.IndexFlatIP(self.dimension)\n",
    "            nlist = 100  # Number of clusters\n",
    "            return faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "        \n",
    "        elif self.index_type == 'hnsw':\n",
    "            # Hierarchical Navigable Small World, best for <10M vectors\n",
    "            M = 32  # Number of connections\n",
    "            return faiss.IndexHNSWFlat(self.dimension, M)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {self.index_type}\")\n",
    "    \n",
    "    def add(self, embeddings: np.ndarray, documents: List[str]):\n",
    "        \"\"\"Add vectors to the index.\"\"\"\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        \n",
    "        if self.index_type == 'ivf' and not self.index.is_trained:\n",
    "            print(\"Training IVF index...\")\n",
    "            self.index.train(embeddings)\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(documents)\n",
    "        print(f\"Index now contains {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Search for top-k similar documents.\"\"\"\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        if self.index_type == 'ivf':\n",
    "            self.index.nprobe = 10  # Number of clusters to search\n",
    "        \n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid result\n",
    "                results.append((self.documents[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark(self, query_embedding: np.ndarray, k: int = 5, n_queries: int = 100):\n",
    "        \"\"\"Benchmark search performance.\"\"\"\n",
    "        start = time.time()\n",
    "        for _ in range(n_queries):\n",
    "            self.search(query_embedding, k)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        qps = n_queries / elapsed\n",
    "        latency_ms = (elapsed / n_queries) * 1000\n",
    "        \n",
    "        return {\n",
    "            'qps': qps,\n",
    "            'latency_ms': latency_ms,\n",
    "            'total_time': elapsed\n",
    "        }\n",
    "\n",
    "# Test FAISS with different index types\n",
    "print(\"\\n=== FAISS Flat Index ===\")\n",
    "faiss_flat = FAISSVectorStore(embedding_service.dimension, 'flat')\n",
    "faiss_flat.add(embeddings, documents)\n",
    "\n",
    "query = \"laptop with powerful GPU for machine learning\"\n",
    "query_emb = embedding_service.embed_query(query)\n",
    "\n",
    "results = faiss_flat.search(query_emb, k=3)\n",
    "print(\"\\nTop 3 results:\")\n",
    "for doc, score in results:\n",
    "    print(f\"  Score: {score:.4f} | {doc}\")\n",
    "\n",
    "# Benchmark\n",
    "metrics = faiss_flat.benchmark(query_emb)\n",
    "print(f\"\\nPerformance: {metrics['qps']:.2f} QPS, {metrics['latency_ms']:.2f}ms latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Milvus - Distributed Vector Database\n",
    "\n",
    "Milvus is designed for production deployments with billion-scale vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "\n",
    "class MilvusVectorStore:\n",
    "    \"\"\"Production Milvus implementation with auto-scaling and monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str, dimension: int, host: str = 'localhost', port: int = 19530):\n",
    "        self.collection_name = collection_name\n",
    "        self.dimension = dimension\n",
    "        \n",
    "        # Connect to Milvus\n",
    "        connections.connect(alias=\"default\", host=host, port=port)\n",
    "        \n",
    "        # Create collection if not exists\n",
    "        self._create_collection()\n",
    "    \n",
    "    def _create_collection(self):\n",
    "        \"\"\"Create collection with optimized schema.\"\"\"\n",
    "        if utility.has_collection(self.collection_name):\n",
    "            self.collection = Collection(self.collection_name)\n",
    "            print(f\"Loaded existing collection: {self.collection_name}\")\n",
    "            return\n",
    "        \n",
    "        # Define schema\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=self.dimension),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=1000)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=\"Product search collection\")\n",
    "        \n",
    "        self.collection = Collection(self.collection_name, schema)\n",
    "        print(f\"Created collection: {self.collection_name}\")\n",
    "        \n",
    "        # Create index for fast search\n",
    "        index_params = {\n",
    "            \"metric_type\": \"IP\",  # Inner Product (cosine for normalized vectors)\n",
    "            \"index_type\": \"IVF_FLAT\",\n",
    "            \"params\": {\"nlist\": 128}\n",
    "        }\n",
    "        self.collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "        print(\"Created index\")\n",
    "    \n",
    "    def add(self, embeddings: np.ndarray, documents: List[str]):\n",
    "        \"\"\"Batch insert with automatic partitioning.\"\"\"\n",
    "        data = [\n",
    "            embeddings.tolist(),\n",
    "            documents\n",
    "        ]\n",
    "        \n",
    "        self.collection.insert(data)\n",
    "        self.collection.flush()\n",
    "        print(f\"Inserted {len(documents)} documents\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5, filter_expr: str = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Hybrid search with optional filtering.\"\"\"\n",
    "        self.collection.load()\n",
    "        \n",
    "        search_params = {\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {\"nprobe\": 16}\n",
    "        }\n",
    "        \n",
    "        results = self.collection.search(\n",
    "            data=[query_embedding.tolist()],\n",
    "            anns_field=\"embedding\",\n",
    "            param=search_params,\n",
    "            limit=k,\n",
    "            expr=filter_expr,\n",
    "            output_fields=[\"text\"]\n",
    "        )\n",
    "        \n",
    "        output = []\n",
    "        for hit in results[0]:\n",
    "            output.append((hit.entity.get('text'), hit.score))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get collection statistics.\"\"\"\n",
    "        self.collection.flush()\n",
    "        return {\n",
    "            'num_entities': self.collection.num_entities,\n",
    "            'index_type': 'IVF_FLAT',\n",
    "            'metric_type': 'IP'\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Drop collection and disconnect.\"\"\"\n",
    "        utility.drop_collection(self.collection_name)\n",
    "        connections.disconnect(\"default\")\n",
    "\n",
    "# Example usage (requires Milvus server)\n",
    "# Uncomment to run with Milvus:\n",
    "# milvus_store = MilvusVectorStore('products', embedding_service.dimension)\n",
    "# milvus_store.add(embeddings[:100], documents[:100])\n",
    "# results = milvus_store.search(query_emb, k=5)\n",
    "# print(milvus_store.get_stats())\n",
    "\n",
    "print(\"\\nMilvus example ready (requires running Milvus server)\")\n",
    "print(\"Start Milvus: docker run -d --name milvus -p 19530:19530 milvusdb/milvus:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Qdrant - High-Performance Vector Search\n",
    "\n",
    "Qdrant provides filtering, payloads, and easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "class QdrantVectorStore:\n",
    "    \"\"\"Production Qdrant implementation with filtering and metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str, dimension: int, in_memory: bool = True):\n",
    "        self.collection_name = collection_name\n",
    "        self.dimension = dimension\n",
    "        \n",
    "        # Initialize client (in-memory or server)\n",
    "        if in_memory:\n",
    "            self.client = QdrantClient(\":memory:\")\n",
    "        else:\n",
    "            self.client = QdrantClient(host=\"localhost\", port=6333)\n",
    "        \n",
    "        # Create collection\n",
    "        self._create_collection()\n",
    "    \n",
    "    def _create_collection(self):\n",
    "        \"\"\"Create collection with HNSW index.\"\"\"\n",
    "        self.client.recreate_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.dimension,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Created Qdrant collection: {self.collection_name}\")\n",
    "    \n",
    "    def add(self, embeddings: np.ndarray, documents: List[str], metadata: List[dict] = None):\n",
    "        \"\"\"Add vectors with rich metadata for filtering.\"\"\"\n",
    "        points = []\n",
    "        for idx, (emb, doc) in enumerate(zip(embeddings, documents)):\n",
    "            payload = {\"text\": doc}\n",
    "            if metadata and idx < len(metadata):\n",
    "                payload.update(metadata[idx])\n",
    "            \n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=idx,\n",
    "                    vector=emb.tolist(),\n",
    "                    payload=payload\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"Added {len(points)} points to Qdrant\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5, filter_conditions: dict = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Semantic search with optional metadata filtering.\"\"\"\n",
    "        query_filter = None\n",
    "        if filter_conditions:\n",
    "            # Example: {\"category\": \"electronics\"}\n",
    "            conditions = []\n",
    "            for key, value in filter_conditions.items():\n",
    "                conditions.append(\n",
    "                    FieldCondition(key=key, match=MatchValue(value=value))\n",
    "                )\n",
    "            query_filter = Filter(must=conditions)\n",
    "        \n",
    "        results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_embedding.tolist(),\n",
    "            limit=k,\n",
    "            query_filter=query_filter\n",
    "        )\n",
    "        \n",
    "        return [(hit.payload['text'], hit.score) for hit in results]\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get collection info.\"\"\"\n",
    "        info = self.client.get_collection(self.collection_name)\n",
    "        return {\n",
    "            'vectors_count': info.vectors_count,\n",
    "            'indexed_vectors_count': info.indexed_vectors_count,\n",
    "            'points_count': info.points_count\n",
    "        }\n",
    "\n",
    "# Test Qdrant\n",
    "print(\"\\n=== Qdrant Vector Store ===\")\n",
    "qdrant_store = QdrantVectorStore('products', embedding_service.dimension)\n",
    "\n",
    "# Add with metadata\n",
    "metadata = [{\"category\": \"electronics\", \"price\": 1200 + i} for i in range(len(documents[:100]))]\n",
    "qdrant_store.add(embeddings[:100], documents[:100], metadata)\n",
    "\n",
    "# Search\n",
    "results = qdrant_store.search(query_emb, k=3)\n",
    "print(\"\\nTop 3 results:\")\n",
    "for doc, score in results:\n",
    "    print(f\"  Score: {score:.4f} | {doc}\")\n",
    "\n",
    "# Filtered search\n",
    "print(\"\\nFiltered search (category=electronics):\")\n",
    "filtered_results = qdrant_store.search(query_emb, k=3, filter_conditions={\"category\": \"electronics\"})\n",
    "for doc, score in filtered_results:\n",
    "    print(f\"  Score: {score:.4f} | {doc}\")\n",
    "\n",
    "print(f\"\\nCollection stats: {qdrant_store.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparison and Best Practices\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**FAISS:**\n",
    "- ✅ Best for: <10M vectors, research, prototyping\n",
    "- ✅ Fastest for exact search\n",
    "- ❌ No built-in persistence, filtering, or metadata\n",
    "- ❌ Requires custom infrastructure\n",
    "\n",
    "**Milvus:**\n",
    "- ✅ Best for: 10M-1B+ vectors, enterprise scale\n",
    "- ✅ Distributed, auto-scaling, high availability\n",
    "- ✅ Hybrid search (vector + scalar filtering)\n",
    "- ❌ More complex to deploy and maintain\n",
    "\n",
    "**Qdrant:**\n",
    "- ✅ Best for: 1M-100M vectors, developer-friendly\n",
    "- ✅ Rich filtering, easy deployment, great DX\n",
    "- ✅ Good balance of features and simplicity\n",
    "- ❌ Less proven at billion-scale\n",
    "\n",
    "### Production Checklist:\n",
    "1. **Indexing Strategy**: Choose based on scale and accuracy needs\n",
    "2. **Monitoring**: Track QPS, latency (P50/P95/P99), recall@k\n",
    "3. **Sharding**: Partition data for horizontal scaling\n",
    "4. **Caching**: Cache embeddings and frequent queries\n",
    "5. **Backups**: Regular snapshots and point-in-time recovery\n",
    "6. **Security**: Authentication, encryption at rest/transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Performance comparison\n",
    "import pandas as pd\n",
    "\n",
    "def compare_vector_stores():\n",
    "    \"\"\"Benchmark all three stores.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # FAISS Flat\n",
    "    faiss_metrics = faiss_flat.benchmark(query_emb, k=5, n_queries=100)\n",
    "    results.append({\n",
    "        'Store': 'FAISS (Flat)',\n",
    "        'QPS': f\"{faiss_metrics['qps']:.1f}\",\n",
    "        'Latency (ms)': f\"{faiss_metrics['latency_ms']:.2f}\",\n",
    "        'Index Type': 'Exact',\n",
    "        'Best For': '<1M vectors'\n",
    "    })\n",
    "    \n",
    "    # FAISS HNSW\n",
    "    faiss_hnsw = FAISSVectorStore(embedding_service.dimension, 'hnsw')\n",
    "    faiss_hnsw.add(embeddings, documents)\n",
    "    hnsw_metrics = faiss_hnsw.benchmark(query_emb, k=5, n_queries=100)\n",
    "    results.append({\n",
    "        'Store': 'FAISS (HNSW)',\n",
    "        'QPS': f\"{hnsw_metrics['qps']:.1f}\",\n",
    "        'Latency (ms)': f\"{hnsw_metrics['latency_ms']:.2f}\",\n",
    "        'Index Type': 'Approximate',\n",
    "        'Best For': '1M-10M vectors'\n",
    "    })\n",
    "    \n",
    "    # Qdrant\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        qdrant_store.search(query_emb, k=5)\n",
    "    qdrant_time = time.time() - start\n",
    "    results.append({\n",
    "        'Store': 'Qdrant',\n",
    "        'QPS': f\"{100/qdrant_time:.1f}\",\n",
    "        'Latency (ms)': f\"{(qdrant_time/100)*1000:.2f}\",\n",
    "        'Index Type': 'HNSW',\n",
    "        'Best For': '1M-100M vectors'\n",
    "    })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "comparison = compare_vector_stores()\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Techniques\n",
    "\n",
    "### Hybrid Search\n",
    "Combine vector similarity with traditional filters for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HybridSearchEngine:\n",
    "    \"\"\"Production hybrid search combining vectors + metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: QdrantVectorStore):\n",
    "        self.vector_store = vector_store\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        embedding_service: EmbeddingService,\n",
    "        k: int = 10,\n",
    "        filters: dict = None,\n",
    "        rerank: bool = True\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Multi-stage search with optional reranking.\"\"\"\n",
    "        \n",
    "        # Stage 1: Vector search with filters\n",
    "        query_emb = embedding_service.embed_query(query)\n",
    "        results = self.vector_store.search(\n",
    "            query_emb,\n",
    "            k=k*2 if rerank else k,  # Fetch more for reranking\n",
    "            filter_conditions=filters\n",
    "        )\n",
    "        \n",
    "        if not rerank:\n",
    "            return results[:k]\n",
    "        \n",
    "        # Stage 2: Rerank with cross-encoder (more accurate but slower)\n",
    "        # In production, use a cross-encoder model here\n",
    "        # For now, we'll just return top-k\n",
    "        return results[:k]\n",
    "    \n",
    "    def explain_ranking(self, query: str, doc: str, embedding_service: EmbeddingService) -> dict:\n",
    "        \"\"\"Explain why a document was ranked for a query.\"\"\"\n",
    "        query_emb = embedding_service.embed_query(query)\n",
    "        doc_emb = embedding_service.embed_query(doc)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'document': doc,\n",
    "            'similarity': float(similarity),\n",
    "            'explanation': f\"High overlap in semantic meaning (score: {similarity:.3f})\"\n",
    "        }\n",
    "\n",
    "# Example: Hybrid search\n",
    "hybrid_engine = HybridSearchEngine(qdrant_store)\n",
    "results = hybrid_engine.search(\n",
    "    \"affordable laptop for students\",\n",
    "    embedding_service,\n",
    "    k=5,\n",
    "    filters={\"category\": \"electronics\"}\n",
    ")\n",
    "\n",
    "print(\"\\n=== Hybrid Search Results ===\")\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.4f} | {doc}\")\n",
    "\n",
    "# Explain ranking\n",
    "explanation = hybrid_engine.explain_ranking(\n",
    "    \"gaming laptop\",\n",
    "    \"High-performance laptop with 32GB RAM and RTX 4090 GPU\",\n",
    "    embedding_service\n",
    ")\n",
    "print(f\"\\nRanking explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Choose the right tool**: FAISS for prototypes, Qdrant for SMB, Milvus for enterprise\n",
    "2. **Index selection matters**: Flat for accuracy, HNSW for speed, IVF for scale\n",
    "3. **Monitor performance**: Track QPS, latency, and recall@k\n",
    "4. **Use hybrid search**: Combine vectors with metadata filtering\n",
    "5. **Plan for scale**: Shard data, cache embeddings, use batch operations\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "1. **Explain the trade-off between HNSW and IVF indexes.**\n",
    "   - HNSW: Graph-based, O(log n) search, better recall, more memory\n",
    "   - IVF: Cluster-based, faster indexing, less memory, lower recall\n",
    "\n",
    "2. **How do you handle billion-scale vector search?**\n",
    "   - Shard across nodes, use approximate indexes (IVF/HNSW)\n",
    "   - Quantization (PQ/SQ) to reduce memory\n",
    "   - Two-stage search: coarse filter → fine ranking\n",
    "\n",
    "3. **What's the difference between L2 and cosine similarity?**\n",
    "   - L2: Euclidean distance, sensitive to magnitude\n",
    "   - Cosine: Angle-based, normalized, better for text\n",
    "   - For normalized vectors, IP (inner product) ≈ cosine\n",
    "\n",
    "4. **How do you monitor vector database performance?**\n",
    "   - QPS (queries per second)\n",
    "   - Latency percentiles (P50, P95, P99)\n",
    "   - Recall@k (accuracy vs ground truth)\n",
    "   - Index build time and memory usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
