{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943dae18",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Advanced Topics & Best Practices\n",
    "\n",
    "You can build models. Now let's build them **professionally**.\n",
    "\n",
    "In this notebook, we will learn the tools that make PyTorch scalable and robust.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Dataset\", \"DataLoader\", \"Batch Size\", and \"Checkpoint\"?\n",
    "- **The Intuition**: Why we eat with a spoon (Batches) instead of a shovel.\n",
    "- **The Practice**: Using `torch.utils.data` to handle massive datasets.\n",
    "- **The Safety**: Saving and Loading your model so you don't lose work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ea7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a9fcf",
   "metadata": {},
   "source": "## Part 1: The Vocabulary (Definitions First)\n\n### 1. Dataset (`torch.utils.data.Dataset`)\n- A class that knows where your data is and how to get one item.\n- It must implement `__len__` (How big am I?) and `__getitem__` (Give me item #5).\n- Analogy: A Librarian who knows where every book is.\n\n**The `__getitem__` contract**: This method receives an integer index and must return a single sample. PyTorch never calls `__getitem__` with a list of indices â€” batching is handled by the DataLoader. Your job is to make this method as fast as possible because it's called thousands of times during training.\n\n### 2. DataLoader (`torch.utils.data.DataLoader`)\n- A worker that grabs items from the Dataset and bundles them into Batches.\n- It handles shuffling and parallel loading (using multiple CPU cores).\n- Analogy: A Delivery Driver who packs books into boxes and brings them to you.\n\n**Key parameters you should understand**:\n- `num_workers`: How many CPU processes load data in parallel. Set to 0 for debugging, 2-8 for training. Too many workers waste memory.\n- `pin_memory`: Set to `True` when using a GPU. Pins data in CPU memory for faster GPU transfer.\n- `collate_fn`: Custom function to combine samples into a batch. Needed when samples have different sizes (e.g., variable-length text).\n- `drop_last`: Whether to drop the last incomplete batch. Important for BatchNorm layers that need consistent batch sizes.\n\n### 3. Batch Size\n- How many items the DataLoader puts in one box.\n- **Small Batch (e.g., 1)**: Noisy gradient updates, slow training, but better generalization (acts as regularization).\n- **Large Batch (e.g., 1000)**: Stable gradient updates, memory hungry, but can converge to sharp minima (worse generalization).\n- **Sweet Spot**: Usually 32, 64, or 128 for most tasks.\n\n**Why batch size affects generalization**: Small batches add noise to gradient estimates, which helps the optimizer escape sharp local minima and find flatter minima that generalize better. This is why simply scaling to huge batches (without tricks like learning rate warmup) often hurts test accuracy.\n\n### 4. Checkpoint\n- A file containing the model's weights at a specific point in time.\n- Analogy: A \"Save Game\" file.\n\n**What to save in a checkpoint** (for proper resumption):\n```python\ncheckpoint = {\n    'model_state_dict': model.state_dict(),       # Weights\n    'optimizer_state_dict': optimizer.state_dict(), # Optimizer momentum\n    'epoch': epoch,                                 # Where you stopped\n    'loss': loss,                                   # Current loss\n    'lr_scheduler': scheduler.state_dict(),         # LR schedule state\n}\n```\nSaving only `model.state_dict()` is fine for inference, but for resuming training you need the optimizer state too â€” otherwise Adam's momentum and variance accumulators reset to zero."
  },
  {
   "cell_type": "markdown",
   "id": "e7c75e1c",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Spoon vs Shovel)\n",
    "\n",
    "Why do we use Batches?\n",
    "\n",
    "Imagine you need to eat a mountain of rice (The Dataset).\n",
    "- **Batch Size = 1 (SGD)**: Eating one grain at a time. You will starve before you finish.\n",
    "- **Batch Size = All (Batch GD)**: Trying to shove the entire mountain into your mouth at once. You will choke (Out of Memory).\n",
    "- **Batch Size = 64 (Mini-Batch GD)**: Eating with a spoon. Efficient and manageable.\n",
    "\n",
    "The `DataLoader` is your spoon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4723e8",
   "metadata": {},
   "source": [
    "## Part 3: Custom Dataset (The Practice)\n",
    "\n",
    "Let's build a fake dataset of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Create a dataset with 1000 items\n",
    "dataset = RandomDataset(size=5, length=1000)\n",
    "\n",
    "# Test the Librarian\n",
    "print(f\"Item 0: {dataset[0]}\")\n",
    "print(f\"Length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730c42d",
   "metadata": {},
   "source": [
    "## Part 4: The DataLoader (The Driver)\n",
    "\n",
    "Now let's put the Driver to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0caed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "loader = DataLoader(dataset=dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test the Driver\n",
    "for batch in loader:\n",
    "    print(f\"Batch Shape: {batch.shape}\")\n",
    "    break # Just show the first one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50237a3c",
   "metadata": {},
   "source": "## Part 5: Saving and Loading (The Save Game)\n\nTraining takes hours. You don't want to lose progress if your computer crashes.\n\nWe use `torch.save` and `torch.load`.\n\n### `state_dict()` vs Saving the Entire Model\n\n**Option A: Save `state_dict()` (Recommended)**\n```python\ntorch.save(model.state_dict(), 'weights.pth')\n```\n- Saves only the learned parameters (weights and biases)\n- Portable: works even if you change the class definition slightly\n- Smaller file size\n- You must define the model class before loading\n\n**Option B: Save the entire model (Not recommended)**\n```python\ntorch.save(model, 'model.pth')\n```\n- Saves everything including the class definition via Python's `pickle`\n- Breaks if you move files, rename classes, or change imports\n- Brittle and hard to debug\n\n**Rule of thumb**: Always use `state_dict()`. The only time you'd save the full model is for quick prototyping when you don't care about long-term reproducibility.\n\n### Loading with `weights_only=True` (Security)\n\nStarting in PyTorch 2.0+, you should use `weights_only=True` when loading:\n```python\nmodel.load_state_dict(torch.load('weights.pth', weights_only=True))\n```\nThis prevents arbitrary code execution from malicious checkpoint files (pickle deserialization attacks)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd309120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "model = nn.Linear(5, 1)\n",
    "\n",
    "# Save the state dictionary (The Weights)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "print(\"Model saved to model_weights.pth\")\n",
    "\n",
    "# Load it back\n",
    "new_model = nn.Linear(5, 1)\n",
    "new_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Verify they are the same\n",
    "print(f\"Original Weight: {model.weight}\")\n",
    "print(f\"Loaded Weight:   {new_model.weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c81da6",
   "metadata": {},
   "source": "## ML System Design: Summary Checklist\n\n### The Interview Template (Print This!)\n\n```\nSTEP 1: CLARIFY (5 min)\n  â–¡ Scale (users, items, QPS)\n  â–¡ Latency requirements\n  â–¡ Online vs Offline\n  â–¡ Cold start problem?\n\nSTEP 2: OBJECTIVE (5 min)\n  â–¡ What to predict? (click, watch time, etc)\n  â–¡ Offline metrics (AUC, NDCG)\n  â–¡ Online metrics (revenue, engagement)\n\nSTEP 3: FEATURES (10 min)\n  â–¡ User features\n  â–¡ Item features\n  â–¡ Context features\n  â–¡ Interaction features\n\nSTEP 4: MODEL (10 min)\n  â–¡ Two-stage (retrieval + ranking)\n  â–¡ Candidate generation method\n  â–¡ Ranking model architecture\n\nSTEP 5: TRAINING (10 min)\n  â–¡ Data collection strategy\n  â–¡ Loss function\n  â–¡ Negative sampling\n  â–¡ Evaluation strategy\n\nSTEP 6: SERVING (10 min)\n  â–¡ System architecture\n  â–¡ Latency optimization\n  â–¡ Caching strategy\n  â–¡ Model update frequency\n```\n\n### Red Flags to Avoid\n\nâŒ **Don't do this**:\n1. Jump straight to model architecture (skip requirements)\n2. Ignore latency constraints (real-time matters!)\n3. Use accuracy for imbalanced data\n4. Forget about cold start\n5. Not mention A/B testing\n6. Ignore fairness/ethics\n\nâœ… **Do this**:\n1. Ask clarifying questions\n2. Discuss trade-offs (accuracy vs latency)\n3. Mention production challenges (drift, scaling)\n4. Show ML + Systems knowledge\n5. Use concrete numbers (not \"a lot of users\")\n6. Draw diagrams!\n\n### Key Takeaways\n\n1. **Two-stage is standard**: Retrieval (fast, broad) â†’ Ranking (slow, precise)\n2. **Features matter more than models**: 50 good features > fancy architecture\n3. **Offline â‰  Online**: AUC â†‘ doesn't guarantee revenue â†‘\n4. **Scale is everything**: Can't run BERT on 1B items in 100ms\n5. **Always A/B test**: Never deploy without validation\n\n### Additional Resources (What FAANG Engineers Read)\n\n**Papers**:\n- YouTube: \"Deep Neural Networks for YouTube Recommendations\" (2016)\n- Facebook: \"Practical Lessons from Predicting Clicks on Ads at Facebook\" (2014)\n- Netflix: \"Netflix Recommendations: Beyond the 5 stars\" (2012)\n- Google: \"Wide & Deep Learning for Recommender Systems\" (2016)\n\n**Books**:\n- \"Designing Machine Learning Systems\" by Chip Huyen\n- \"Machine Learning System Design Interview\" by Ali Aminian & Alex Xu\n\n**Practice Platforms**:\n- InterviewQuery.com (ML system design mock interviews)\n- ByteByteGo (System design fundamentals)\n\n---\n\n## Final Thoughts\n\n**You now have the complete toolkit for FAANG ML interviews:**\n\n1. âœ… **PyTorch fundamentals** (tensors, autograd, models)\n2. âœ… **Deep learning architectures** (CNNs, RNNs, Transformers, ResNet, ViT)\n3. âœ… **Optimization techniques** (AdamW, AMP, LR warmup, quantization)\n4. âœ… **Production deployment** (TorchServe, FastAPI, Docker)\n5. âœ… **Real-world challenges** (imbalanced data, focal loss, metrics)\n6. âœ… **ML System Design** (recommendations, ranking, search, ads)\n\n**The difference between you and other candidates**: You understand both the **code** AND the **systems**.\n\n**Next steps**:\n1. Practice coding 1 architecture from scratch daily\n2. Mock ML system design interviews weekly\n3. Read 1 FAANG ML paper per week\n4. Build 1 end-to-end project (training â†’ deployment)\n\n**You are ready for FAANG 2025.** ðŸš€",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}