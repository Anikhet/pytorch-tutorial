{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Optimization and Tuning\n",
    "\n",
    "Building a model is just the start. Making it train fast and generalize well requires optimization and tuning. This notebook covers essential techniques for improving model performance.\n",
    "\n",
    "## Learning Objectives\n",
    "- Use Learning Rate Schedulers\n",
    "- Apply Regularization (Dropout, Weight Decay)\n",
    "- Implement Batch Normalization\n",
    "- Understand Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Rate Schedulers\n",
    "\n",
    "A constant learning rate is rarely optimal. We often want to start high (to learn fast) and decrease it (to fine-tune).\n",
    "\n",
    "Common schedulers:\n",
    "- `StepLR`: Decays LR by gamma every step_size epochs\n",
    "- `ReduceLROnPlateau`: Decays LR when validation loss stops improving\n",
    "- `CosineAnnealingLR`: Follows a cosine curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model and optimizer\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Setup scheduler: Multiply LR by 0.1 every 5 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(20):\n",
    "    optimizer.step()  # Simulate training step\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()  # Update LR\n",
    "\n",
    "plt.plot(lrs, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('StepLR Scheduler')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xdslso2g9p",
   "source": "---\n\n## 1.5. Learning Rate Warmup (CRITICAL for Transformers!)\n\n### The Problem\nStarting with a high learning rate can cause:\n- Unstable training in early epochs\n- Divergence (NaN losses)\n- Poor final performance\n\n**Especially problematic for:**\n- Large models (Transformers, LLMs)\n- Large batch sizes\n- Adam/AdamW optimizers\n\n### The Solution: Warmup\nStart with a very small LR and gradually increase it over the first N steps.\n\n```\nWarmup phase (0-1000 steps): LR goes from 0 ‚Üí target_lr\nMain phase (1000+ steps): Use normal schedule (cosine, constant, etc.)\n```\n\n### Why It Works\n1. **Prevents instability**: Model starts with small updates\n2. **Adaptive optimizers need warmup**: Adam's momentum estimates are unreliable initially\n3. **Standard in Transformer training**: BERT, GPT, all modern LLMs use warmup\n\n### FAANG Interview Question\n**\"Why do we need learning rate warmup for Transformers?\"** ‚Üê Asked at Google, OpenAI\n\n**Answer:**\n1. Adam/AdamW have poor estimates of gradient statistics early in training\n2. Large learning rates can cause exploding gradients in early stages\n3. Warmup stabilizes training, especially for large models/batches\n4. Standard practice: warmup for 5-10% of total training steps",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kh3b5eajfv",
   "source": "# Learning Rate Warmup Implementation\n\n# Method 1: Manual Warmup Function\ndef get_lr_with_warmup(step, warmup_steps, base_lr, max_lr):\n    \"\"\"\n    Linear warmup from base_lr to max_lr over warmup_steps.\n    This is the STANDARD approach in Transformer training!\n    \"\"\"\n    if step < warmup_steps:\n        # Linear warmup\n        return base_lr + (max_lr - base_lr) * step / warmup_steps\n    else:\n        # After warmup, use max_lr (or apply decay)\n        return max_lr\n\n# Visualize warmup schedule\nwarmup_steps = 1000\ntotal_steps = 10000\nbase_lr = 0.0\nmax_lr = 1e-3\n\nlrs = [get_lr_with_warmup(step, warmup_steps, base_lr, max_lr) \n       for step in range(total_steps)]\n\nplt.figure(figsize=(10, 4))\nplt.plot(lrs)\nplt.axvline(x=warmup_steps, color='r', linestyle='--', label='End of warmup')\nplt.xlabel('Training Step')\nplt.ylabel('Learning Rate')\nplt.title('Linear Warmup Schedule')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"‚úì LR starts at {base_lr} and warms up to {max_lr} over {warmup_steps} steps\")\nprint(f\"Then stays constant at {max_lr}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cbammsako15",
   "source": "# Method 2: Warmup + Cosine Decay (The FAANG Standard!)\n\nimport math\n\ndef get_cosine_schedule_with_warmup(step, warmup_steps, total_steps, max_lr, min_lr=0):\n    \"\"\"\n    Warmup + Cosine Annealing.\n    This is used to train GPT-3, BERT, Llama, and most modern LLMs!\n    \"\"\"\n    if step < warmup_steps:\n        # Linear warmup\n        return max_lr * step / warmup_steps\n    else:\n        # Cosine decay\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return min_lr + (max_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n\n# Visualize warmup + cosine\nwarmup_steps = 1000\ntotal_steps = 10000\nmax_lr = 1e-3\nmin_lr = 1e-5\n\nlrs = [get_cosine_schedule_with_warmup(step, warmup_steps, total_steps, max_lr, min_lr) \n       for step in range(total_steps)]\n\nplt.figure(figsize=(12, 4))\nplt.plot(lrs, linewidth=2)\nplt.axvline(x=warmup_steps, color='r', linestyle='--', alpha=0.7, label='End of warmup')\nplt.xlabel('Training Step')\nplt.ylabel('Learning Rate')\nplt.title('Warmup + Cosine Annealing (GPT-3, BERT, Llama)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"‚úì This is the STANDARD LR schedule for LLM training!\")\nprint(f\"‚Ä¢ Warmup: {warmup_steps} steps (10% of training)\")\nprint(f\"‚Ä¢ Peak LR: {max_lr}\")\nprint(f\"‚Ä¢ Final LR: {min_lr}\")\nprint(f\"‚Ä¢ Total steps: {total_steps}\")\nprint(\"\\\\nUsed in: GPT-3, BERT, RoBERTa, Llama, Mistral, etc.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization: Dropout and Weight Decay\n",
    "\n",
    "**Overfitting** happens when a model memorizes training data but fails on new data. Regularization prevents this.\n",
    "\n",
    "### Dropout\n",
    "Randomly zeros out neurons during training. This forces the network to learn robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% probability\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = RegularizedNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay (L2 Regularization)\n",
    "Penalizes large weights. In PyTorch, this is part of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weight_decay parameter to optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization\n",
    "\n",
    "Normalizes layer inputs to have mean 0 and variance 1. This stabilizes training and allows higher learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # Batch Norm for 1D data\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Apply BN before activation\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = BatchNormNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stopping\n",
    "\n",
    "Stop training when validation loss stops improving. This saves time and prevents overfitting.\n",
    "\n",
    "*(Concept only - usually implemented as a loop check)*\n",
    "\n",
    "```python\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model, 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary of Part 1\n\n1. **Schedulers**: Adjust learning rate dynamically.\n2. **Dropout**: Randomly disable neurons to improve robustness.\n3. **Weight Decay**: Penalize large weights to prevent overfitting.\n4. **Batch Norm**: Normalize inputs for stable, faster training.\n5. **Early Stopping**: Stop when you stop improving.\n\n---\n\n# PART 2: ADVANCED OPTIMIZATION (FAANG-Level)\n\nThese techniques are used in production at all major AI companies."
  },
  {
   "cell_type": "markdown",
   "id": "70nc09pnlie",
   "source": "## 5. Advanced Optimizers: Beyond Adam\n\n### The Optimizer Hierarchy\n\n**Basic (1980s-1990s):**\n- SGD: Slow but reliable\n- Momentum: Accelerates SGD\n\n**Adaptive (2010s):**\n- Adam: Adaptive learning rates per parameter (most popular)\n- RMSprop: Similar to Adam, used by DeepMind\n\n**Modern (2020s):**\n- **AdamW**: Adam with decoupled weight decay (used in BERT, GPT)\n- **LAMB**: Large batch training (used for BERT pretraining)\n- **Lion**: New optimizer from Google (2023)\n\n### AdamW vs Adam: The Critical Difference\n\n**Adam**: Weight decay applied BEFORE gradient update (incorrect!)  \n**AdamW**: Weight decay applied AFTER gradient update (correct!)\n\n**Impact:** AdamW generalizes better, especially for Transformers.\n\n### FAANG Interview Question\n**\"What's the difference between Adam and AdamW?\"** ‚Üê Asked at Google, OpenAI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jkwje07jsk",
   "source": "# Comparing Optimizers\n\nmodel = nn.Linear(10, 1)\n\n# 1. Classic Adam (PyTorch default)\noptimizer_adam = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)\n\n# 2. AdamW (Decoupled weight decay - BETTER for deep learning!)\noptimizer_adamw = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n\n# 3. SGD with Momentum (Still used for CNNs like ResNet)\noptimizer_sgd = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n\nprint(\"Optimizer Comparison:\")\nprint(f\"Adam: {optimizer_adam}\")\nprint(f\"AdamW: {optimizer_adamw}\")\nprint(f\"SGD+Momentum: {optimizer_sgd}\")\n\nprint(\"\\\\nüìä When to use each:\")\nprint(\"‚Ä¢ AdamW: Transformers, LLMs (GPT, BERT, Llama)\")\nprint(\"‚Ä¢ SGD+Momentum: CNNs (ResNet, EfficientNet)\")\nprint(\"‚Ä¢ Adam: General purpose (but prefer AdamW for research)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cw3dpe4mlvd",
   "source": "---\n\n## 6. Mixed Precision Training (CRITICAL for Nvidia/FAANG)\n\n### The Problem\n- Training in Float32 (32-bit) is slow and memory-intensive\n- Large models (LLMs) don't fit in GPU memory\n\n### The Solution: Mixed Precision (FP16 + FP32)\nUse 16-bit floating point (FP16) for most operations, 32-bit (FP32) for critical parts.\n\n**Benefits:**\n- **2x faster training** (on modern GPUs with Tensor Cores)\n- **2x less memory** (can train bigger models)\n- **Minimal accuracy loss** (with proper techniques)\n\n### How It Works\n1. Store weights in FP32 (master copy)\n2. Convert to FP16 for forward/backward pass\n3. Use FP32 for weight updates (precision matters here!)\n4. Apply **loss scaling** to prevent gradient underflow\n\n### Automatic Mixed Precision (AMP)\nPyTorch provides `torch.cuda.amp` that handles everything automatically!\n\n### FAANG Interview Question\n**\"How does mixed precision training work?\"** ‚Üê Asked at Nvidia, Google, Meta\n\n**Answer:**\n1. Model weights stored in FP32\n2. Forward/backward in FP16 (faster)\n3. Gradient scaling to prevent underflow\n4. Weight updates in FP32 (accuracy)\n5. Result: 2x speedup, same accuracy",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3xj43svlh1d",
   "source": "# Mixed Precision Training Example (Production Code!)\n\n# Setup\nmodel = nn.Linear(1000, 1000).cuda() if torch.cuda.is_available() else nn.Linear(1000, 1000)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\n# Create GradScaler for automatic mixed precision\nscaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n\n# Training loop with AMP\ndef train_with_amp(model, data, labels, optimizer, scaler):\n    \"\"\"\n    This is the STANDARD training loop at FAANG companies!\n    \"\"\"\n    optimizer.zero_grad()\n    \n    # Autocast: automatically uses FP16 where safe\n    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n        outputs = model(data)\n        loss = nn.functional.mse_loss(outputs, labels)\n    \n    # Backward pass with gradient scaling\n    if scaler:\n        scaler.scale(loss).backward()  # Scale loss to prevent underflow\n        scaler.step(optimizer)         # Unscale gradients and update weights\n        scaler.update()                # Update scale for next iteration\n    else:\n        loss.backward()\n        optimizer.step()\n    \n    return loss.item()\n\n# Simulate training\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndata = torch.randn(32, 1000).to(device)\nlabels = torch.randn(32, 1000).to(device)\n\nloss = train_with_amp(model, data, labels, optimizer, scaler)\n\nprint(f\"‚úì Mixed precision training completed! Loss: {loss:.4f}\")\nprint(\"\\\\nüí° Key benefits:\")\nprint(\"‚Ä¢ 2x faster on GPUs with Tensor Cores (V100, A100, H100)\")\nprint(\"‚Ä¢ 50% less memory usage\")\nprint(\"‚Ä¢ Enable with just 3 lines: autocast + GradScaler\")\nprint(\"\\\\n‚ö†Ô∏è Must-know for Nvidia interviews!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yusi4uu4d0q",
   "source": "---\n\n## 7. Gradient Accumulation (Train Huge Models on Small GPUs)\n\n### The Problem\n- You want batch_size=256, but your GPU only fits batch_size=32\n- Large batches ‚Üí better gradients, faster convergence\n\n### The Solution: Gradient Accumulation\nAccumulate gradients over multiple forward/backward passes before updating weights.\n\n```\nReal batch size = micro_batch_size √ó accumulation_steps\nExample: 32 √ó 8 = 256\n```\n\n### How It Works\n1. Forward pass (batch 1) ‚Üí compute gradients (don't update!)\n2. Forward pass (batch 2) ‚Üí add gradients (don't update!)\n3. ...\n4. Forward pass (batch 8) ‚Üí add gradients ‚Üí **NOW update!**\n\n### FAANG Interview Question\n**\"How do you train with a large batch size on limited GPU memory?\"** ‚Üê Asked at Meta, Google",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "or63h0t2wyi",
   "source": "# Gradient Accumulation Example (Used in GPT training!)\n\nmodel = nn.Linear(100, 10)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\n# Configuration\naccumulation_steps = 4  # Simulate 4x larger batch\nmicro_batch_size = 8\neffective_batch_size = micro_batch_size * accumulation_steps\n\nprint(f\"Training with effective batch size: {effective_batch_size}\")\nprint(f\"(Micro-batch: {micro_batch_size} √ó Accumulation: {accumulation_steps})\")\n\n# Training loop with gradient accumulation\noptimizer.zero_grad()\n\nfor step in range(accumulation_steps):\n    # Get micro-batch\n    data = torch.randn(micro_batch_size, 100)\n    labels = torch.randn(micro_batch_size, 10)\n    \n    # Forward\n    outputs = model(data)\n    loss = nn.functional.mse_loss(outputs, labels)\n    \n    # Scale loss by accumulation steps (important!)\n    loss = loss / accumulation_steps\n    \n    # Backward (gradients accumulate)\n    loss.backward()\n    \n    print(f\"  Step {step+1}/{accumulation_steps}: loss={loss.item():.4f}\")\n\n# Now update weights (after all accumulation steps)\noptimizer.step()\noptimizer.zero_grad()\n\nprint(\"\\\\n‚úì Weights updated after accumulating gradients from all steps!\")\nprint(\"This technique powers training of GPT-3, Llama, etc.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p1dlh6b3qg",
   "source": "---\n\n## 8. Gradient Clipping (Preventing Exploding Gradients)\n\n### The Problem\nGradients can explode (become very large) in:\n- RNNs/LSTMs (long sequences)\n- Very deep networks\n- Transformers (sometimes)\n\n**Result:** NaN losses, training divergence\n\n### The Solution: Gradient Clipping\nCap gradients to a maximum value.\n\n**Two methods:**\n1. **Clip by value**: `grad = min(max(grad, -threshold), threshold)`\n2. **Clip by norm**: If ||grad|| > threshold, scale down: `grad = grad * (threshold / ||grad||)`\n\n**Clip by norm is standard!**\n\n### FAANG Interview Question\n**\"What causes NaN losses and how do you fix it?\"** ‚Üê Asked at all FAANG\n\n**Answer:**\n1. Exploding gradients ‚Üí Gradient clipping\n2. Learning rate too high ‚Üí Lower LR or use warmup\n3. Numerical instability ‚Üí Mixed precision with loss scaling\n4. Bad initialization ‚Üí Use proper init (Kaiming, Xavier)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ox39p1zluvc",
   "source": "# Gradient Clipping Example (Standard in Transformer training)\n\nmodel = nn.Linear(100, 10)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\n# Simulate training\ndata = torch.randn(32, 100)\nlabels = torch.randn(32, 10)\n\noptimizer.zero_grad()\noutputs = model(data)\nloss = nn.functional.mse_loss(outputs, labels)\nloss.backward()\n\n# Check gradient norm BEFORE clipping\ntotal_norm_before = 0.0\nfor p in model.parameters():\n    if p.grad is not None:\n        param_norm = p.grad.data.norm(2)\n        total_norm_before += param_norm.item() ** 2\ntotal_norm_before = total_norm_before ** 0.5\n\n# Gradient Clipping (THIS IS THE KEY LINE!)\nmax_norm = 1.0  # Common values: 0.5, 1.0, 5.0\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n# Check gradient norm AFTER clipping\ntotal_norm_after = 0.0\nfor p in model.parameters():\n    if p.grad is not None:\n        param_norm = p.grad.data.norm(2)\n        total_norm_after += param_norm.item() ** 2\ntotal_norm_after = total_norm_after ** 0.5\n\nprint(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\nprint(f\"Gradient norm after clipping:  {total_norm_after:.4f}\")\nprint(f\"Max allowed norm: {max_norm}\")\n\noptimizer.step()\n\nprint(\"\\\\n‚úì Gradients clipped successfully!\")\nprint(\"\\\\nüí° Best practices:\")\nprint(\"‚Ä¢ Always clip gradients for RNNs/Transformers\")\nprint(\"‚Ä¢ Typical max_norm: 0.5-5.0\")\nprint(\"‚Ä¢ Monitor gradient norms during training\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8plcdof8hsv",
   "source": "---\n\n## Final Summary: Production ML Training Stack\n\n### The Complete Training Recipe (FAANG Standard)\n\n```python\n# 1. Model\nmodel = YourModel().cuda()\n\n# 2. Optimizer (AdamW for Transformers, SGD+Momentum for CNNs)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n\n# 3. Learning Rate Schedule (Warmup + Cosine Decay)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n# 4. Mixed Precision\nscaler = torch.cuda.amp.GradScaler()\n\n# 5. Training Loop\naccumulation_steps = 4\nfor epoch in range(epochs):\n    for i, (data, labels) in enumerate(dataloader):\n        # Mixed precision forward pass\n        with torch.cuda.amp.autocast():\n            outputs = model(data)\n            loss = criterion(outputs, labels) / accumulation_steps\n        \n        # Backward\n        scaler.scale(loss).backward()\n        \n        # Update every N steps (gradient accumulation)\n        if (i + 1) % accumulation_steps == 0:\n            # Gradient clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # Optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n    \n    # LR schedule\n    scheduler.step()\n```\n\n---\n\n### FAANG Interview Cheat Sheet\n\n| Topic | Key Point | When Asked |\n|-------|-----------|------------|\n| **AdamW vs Adam** | Decoupled weight decay | Google, OpenAI |\n| **Mixed Precision** | FP16 compute + FP32 weights | Nvidia, Meta |\n| **Gradient Accumulation** | Simulate large batches | Meta, Google |\n| **Gradient Clipping** | Prevent exploding gradients | All FAANG |\n| **Learning Rate Warmup** | Start small, ramp up | OpenAI, Google |\n| **Batch Normalization** | Normalize layer inputs | Basic question |\n\n---\n\n### Optimizer Decision Tree\n\n```\nTraining Transformers/LLMs?\n‚îú‚îÄ YES ‚Üí Use AdamW (lr=1e-4 to 1e-3)\n‚îî‚îÄ NO ‚Üí Training CNNs?\n    ‚îú‚îÄ YES ‚Üí Use SGD + Momentum (lr=0.1)\n    ‚îî‚îÄ NO ‚Üí Use AdamW (safe default)\n```\n\n---\n\n### Common Mistakes to Avoid\n\n1. ‚ùå **Using Adam instead of AdamW for Transformers**\n   - ‚úÖ Always use AdamW for modern deep learning\n\n2. ‚ùå **Not using mixed precision on modern GPUs**\n   - ‚úÖ Always enable AMP on V100/A100/H100\n\n3. ‚ùå **Forgetting to clip gradients for RNNs/Transformers**\n   - ‚úÖ Always clip with `max_norm=1.0`\n\n4. ‚ùå **Not scaling loss with gradient accumulation**\n   - ‚úÖ `loss = loss / accumulation_steps`\n\n5. ‚ùå **Constant learning rate**\n   - ‚úÖ Use warmup + cosine decay\n\n---\n\n### What We Covered (Enhanced)\n\n**Part 1: Basics**\n- ‚úÖ Learning rate schedulers\n- ‚úÖ Dropout, weight decay\n- ‚úÖ Batch normalization\n- ‚úÖ Early stopping\n\n**Part 2: Advanced (FAANG-Level)**\n- ‚úÖ AdamW vs Adam\n- ‚úÖ Mixed precision training (AMP)\n- ‚úÖ Gradient accumulation\n- ‚úÖ Gradient clipping\n\n---\n\n## Next Steps for FAANG Prep\n\n1. **Practice:** Implement training loop with all techniques\n2. **Understand:** Why each technique works (not just how)\n3. **Memorize:** Common hyperparameters (lr, weight_decay, max_norm)\n4. **Debug:** Practice fixing NaN losses, slow convergence\n\n---\n\n**You are now ready for Optimization questions at FAANG/Nvidia! üöÄ**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}