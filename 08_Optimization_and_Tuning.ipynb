{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Optimization and Tuning\n",
    "\n",
    "Building a model is just the start. Making it train fast and generalize well requires optimization and tuning. This notebook covers essential techniques for improving model performance.\n",
    "\n",
    "## Learning Objectives\n",
    "- Use Learning Rate Schedulers\n",
    "- Apply Regularization (Dropout, Weight Decay)\n",
    "- Implement Batch Normalization\n",
    "- Understand Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Rate Schedulers\n",
    "\n",
    "A constant learning rate is rarely optimal. We often want to start high (to learn fast) and decrease it (to fine-tune).\n",
    "\n",
    "Common schedulers:\n",
    "- `StepLR`: Decays LR by gamma every step_size epochs\n",
    "- `ReduceLROnPlateau`: Decays LR when validation loss stops improving\n",
    "- `CosineAnnealingLR`: Follows a cosine curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model and optimizer\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Setup scheduler: Multiply LR by 0.1 every 5 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(20):\n",
    "    optimizer.step()  # Simulate training step\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()  # Update LR\n",
    "\n",
    "plt.plot(lrs, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('StepLR Scheduler')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization: Dropout and Weight Decay\n",
    "\n",
    "**Overfitting** happens when a model memorizes training data but fails on new data. Regularization prevents this.\n",
    "\n",
    "### Dropout\n",
    "Randomly zeros out neurons during training. This forces the network to learn robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% probability\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = RegularizedNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay (L2 Regularization)\n",
    "Penalizes large weights. In PyTorch, this is part of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weight_decay parameter to optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization\n",
    "\n",
    "Normalizes layer inputs to have mean 0 and variance 1. This stabilizes training and allows higher learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # Batch Norm for 1D data\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Apply BN before activation\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = BatchNormNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stopping\n",
    "\n",
    "Stop training when validation loss stops improving. This saves time and prevents overfitting.\n",
    "\n",
    "*(Concept only - usually implemented as a loop check)*\n",
    "\n",
    "```python\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model, 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Schedulers**: Adjust learning rate dynamically.\n",
    "2. **Dropout**: Randomly disable neurons to improve robustness.\n",
    "3. **Weight Decay**: Penalize large weights to prevent overfitting.\n",
    "4. **Batch Norm**: Normalize inputs for stable, faster training.\n",
    "5. **Early Stopping**: Stop when you stop improving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
