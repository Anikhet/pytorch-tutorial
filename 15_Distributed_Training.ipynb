{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Distributed Training (DDP & FSDP)\n",
    "\n",
    "When your model is too big for one GPU, or your data is so large that training takes weeks, you need **Distributed Training**. This is a standard requirement for FAANG AI roles working on Foundation Models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **Data Parallelism** vs **Model Parallelism**\n",
    "- Learn the structure of **DDP (Distributed Data Parallel)**\n",
    "- Introduction to **FSDP (Fully Sharded Data Parallel)** for massive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "- **Node**: A physical machine (server). A node can have multiple GPUs.\n",
    "- **Rank**: The ID of a process. If you have 4 GPUs, ranks are 0, 1, 2, 3.\n",
    "- **World Size**: Total number of processes (GPUs) in the training job.\n",
    "- **Master Address/Port**: Where the processes coordinate (usually Rank 0).\n",
    "- **Scatter/Gather**: Sending data to GPUs / Collecting results back.\n",
    "- **All-Reduce**: A synchronization step where all GPUs share their gradients and calculate the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Data Parallel (DDP)\n",
    "\n",
    "**How it works:**\n",
    "1. Copy the model to every GPU.\n",
    "2. Split the dataset (each GPU gets a different chunk).\n",
    "3. Forward pass runs independently on each GPU.\n",
    "4. Backward pass computes gradients.\n",
    "5. **All-Reduce**: Gradients are averaged across all GPUs.\n",
    "6. Optimizer updates weights (identical on all GPUs).\n",
    "\n",
    "### The Code Structure\n",
    "DDP requires a script, not a notebook, because it spawns multiple processes. Here is the template you would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddp_script.py (Template)\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 1. Create Model and move to GPU (Rank)\n",
    "    model = torch.nn.Linear(10, 10).to(rank)\n",
    "    \n",
    "    # 2. Wrap with DDP\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 3. Loss and Optimizer\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 4. Training Loop\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(rank))\n",
    "    loss = loss_fn(outputs, torch.randn(20, 10).to(rank))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = 2 # Number of GPUs\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fully Sharded Data Parallel (FSDP)\n",
    "\n",
    "DDP replicates the *entire model* on every GPU. If your model is 100GB and your GPU has 80GB, DDP fails.\n",
    "\n",
    "**FSDP** solves this by **sharding** (splitting) the model parameters, gradients, and optimizer states across GPUs. Each GPU only holds a piece of the model.\n",
    "\n",
    "### When to use FSDP?\n",
    "- Training LLMs (Llama 3, GPT-4 class models).\n",
    "- When model size > GPU memory.\n",
    "\n",
    "```python\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "# Wrap your model\n",
    "fsdp_model = FSDP(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **DDP** is fast and standard for most models.\n",
    "2. **FSDP** is necessary for giant models (LLMs).\n",
    "3. **`dist.init_process_group`** is the handshake that starts distributed training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
