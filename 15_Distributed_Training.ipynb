{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Distributed Training (DDP & FSDP)\n",
    "\n",
    "When your model is too big for one GPU, or your data is so large that training takes weeks, you need **Distributed Training**. This is a standard requirement for FAANG AI roles working on Foundation Models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **Data Parallelism** vs **Model Parallelism**\n",
    "- Learn the structure of **DDP (Distributed Data Parallel)**\n",
    "- Introduction to **FSDP (Fully Sharded Data Parallel)** for massive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Vocabulary First\n\n- **Node**: A physical machine (server). A node can have multiple GPUs.\n- **Rank**: The ID of a process. If you have 4 GPUs, ranks are 0, 1, 2, 3.\n- **World Size**: Total number of processes (GPUs) in the training job.\n- **Master Address/Port**: Where the processes coordinate (usually Rank 0).\n- **Scatter/Gather**: Sending data to GPUs / Collecting results back.\n- **All-Reduce**: A synchronization step where all GPUs share their gradients and calculate the average.\n\n### Why Distributed Training Matters\n\nSingle-GPU training simply cannot keep up with modern model sizes. Here is the math:\n\n| Model | Parameters | FP32 Memory (Weights Only) | Training Memory (with gradients + optimizer) |\n|-------|-----------|---------------------------|----------------------------------------------|\n| ResNet-50 | 25M | 100 MB | ~400 MB |\n| BERT-Large | 340M | 1.3 GB | ~5 GB |\n| GPT-3 | 175B | 700 GB | ~2.8 TB |\n| Llama-3 70B | 70B | 280 GB | ~1.1 TB |\n\n**Training memory is roughly 4x the model size** because you need to store:\n1. **Weights** (the model parameters)\n2. **Gradients** (same size as weights)\n3. **Optimizer states** (Adam stores 2 extra copies: momentum + variance)\n\nSo a 70B model needs: `70B x 4 bytes x 4 = 1.1 TB` — no single GPU holds that.\n\n### The Two Fundamental Strategies\n\n**Data Parallelism** — Same model on every GPU, different data.\n- Each GPU has a full copy of the model\n- The dataset is split across GPUs\n- Gradients are averaged after each step\n- Works when the model fits on one GPU but training is slow\n\n**Model Parallelism** — Different parts of the model on different GPUs.\n- The model is split across GPUs (each GPU holds a piece)\n- Data flows between GPUs during forward/backward pass\n- Necessary when the model is too large for one GPU\n- Adds communication overhead between GPUs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Distributed Data Parallel (DDP)\n\n**How it works:**\n1. Copy the model to every GPU.\n2. Split the dataset (each GPU gets a different chunk).\n3. Forward pass runs independently on each GPU.\n4. Backward pass computes gradients.\n5. **All-Reduce**: Gradients are averaged across all GPUs.\n6. Optimizer updates weights (identical on all GPUs).\n\n### The All-Reduce Operation (Core Concept)\n\nAll-Reduce is the heart of distributed training. It ensures every GPU ends up with the **same averaged gradients**.\n\n**Naive Approach (Don't do this)**:\n```\nGPU 0 sends gradients → GPU Master\nGPU 1 sends gradients → GPU Master\nGPU 2 sends gradients → GPU Master\nGPU Master computes average → sends back to all\n```\nThis creates a bottleneck at the master and doesn't scale.\n\n**Ring All-Reduce (What actually happens)**:\n```\nGPUs arranged in a ring: 0 → 1 → 2 → 3 → 0\nStep 1: Each GPU sends a chunk to its neighbor\nStep 2: Each GPU adds received chunk to its own\nStep 3: Repeat until all GPUs have the full sum\nStep 4: Divide by world_size to get average\n```\n\nRing All-Reduce is efficient because **every GPU sends and receives simultaneously** — no single bottleneck. The communication cost is `O(N)` regardless of the number of GPUs (where N is the gradient size).\n\n### Effective Batch Size and Learning Rate\n\nWhen you use 4 GPUs with batch_size=32 per GPU, your **effective batch size is 128** (4 × 32).\n\nThis matters because larger effective batches need adjusted learning rates:\n\n**Linear Scaling Rule** (from Facebook's paper on large-batch training):\n```\nnew_lr = base_lr × (effective_batch_size / base_batch_size)\n```\n\n**Example**: If `lr=0.001` works with batch_size=32, then with 4 GPUs:\n```\nnew_lr = 0.001 × (128 / 32) = 0.004\n```\n\n**Learning Rate Warmup**: When scaling up, the model can diverge if you jump straight to a high LR. Warmup gradually increases LR over the first few hundred steps to stabilize training.\n\n### The Code Structure\nDDP requires a script, not a notebook, because it spawns multiple processes. Here is the template you would use:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddp_script.py (Template)\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 1. Create Model and move to GPU (Rank)\n",
    "    model = torch.nn.Linear(10, 10).to(rank)\n",
    "    \n",
    "    # 2. Wrap with DDP\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 3. Loss and Optimizer\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 4. Training Loop\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(rank))\n",
    "    loss = loss_fn(outputs, torch.randn(20, 10).to(rank))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = 2 # Number of GPUs\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Fully Sharded Data Parallel (FSDP)\n\nDDP replicates the *entire model* on every GPU. If your model is 100GB and your GPU has 80GB, DDP fails.\n\n**FSDP** solves this by **sharding** (splitting) the model parameters, gradients, and optimizer states across GPUs. Each GPU only holds a piece of the model.\n\n### DDP vs FSDP: Memory Comparison\n\nConsider training a **7B parameter model** with Adam optimizer on 4x A100 (80GB each):\n\n**DDP (each GPU stores everything)**:\n```\nWeights:          7B × 4 bytes = 28 GB\nGradients:        7B × 4 bytes = 28 GB\nOptimizer (Adam): 7B × 8 bytes = 56 GB  (momentum + variance)\n─────────────────────────────────────────\nTotal per GPU:                   112 GB  ← Doesn't fit on 80GB A100!\n```\n\n**FSDP (sharded across 4 GPUs)**:\n```\nWeights:          7B × 4 bytes / 4 GPUs =  7 GB\nGradients:        7B × 4 bytes / 4 GPUs =  7 GB\nOptimizer (Adam): 7B × 8 bytes / 4 GPUs = 14 GB\n─────────────────────────────────────────\nTotal per GPU:                             28 GB  ← Fits easily!\n```\n\n### FSDP Sharding Strategies\n\nFSDP offers three levels of sharding — the more you shard, the more memory you save, but the more communication overhead you pay:\n\n| Strategy | What's Sharded | Memory Savings | Communication Cost |\n|----------|---------------|----------------|--------------------|\n| `FULL_SHARD` | Weights + Gradients + Optimizer | Maximum (3x savings) | Highest (all-gather before each forward) |\n| `SHARD_GRAD_OP` | Gradients + Optimizer only | Moderate (2x savings) | Moderate |\n| `NO_SHARD` | Nothing (same as DDP) | None | Lowest |\n\n### When to use FSDP?\n- Training LLMs (Llama 3, GPT-4 class models).\n- When model size > GPU memory.\n- When you need to train (not just inference) large models.\n\n```python\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n# Wrap your model\nfsdp_model = FSDP(model)\n```\n\n### Common Failure Modes in Distributed Training\n\nThese are the bugs that waste hours of expensive GPU time:\n\n1. **Deadlocks**: One GPU crashes, others hang forever waiting for All-Reduce. Fix: Set `timeout` in `init_process_group` and use `NCCL_ASYNC_ERROR_HANDLING=1`.\n\n2. **OOM on one GPU**: Uneven data distribution causes one GPU to get a larger batch. Fix: Use `DistributedSampler` to ensure equal splits.\n\n3. **Inconsistent models**: Forgetting to set the same random seed or not using `DistributedSampler` correctly. Fix: Always set seeds and use `sampler.set_epoch(epoch)` for shuffling.\n\n4. **Slow communication**: Using `gloo` backend instead of `nccl` for GPU training. Fix: Always use `nccl` for GPU-to-GPU communication.\n\n5. **Gradient accumulation bugs**: Not dividing loss by the number of accumulation steps. Fix: `loss = loss / accumulation_steps` before `backward()`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **DDP** is fast and standard for models that fit on one GPU — it replicates the model and averages gradients via Ring All-Reduce.\n2. **FSDP** is necessary for giant models (LLMs) — it shards weights, gradients, and optimizer states across GPUs.\n3. **`dist.init_process_group`** is the handshake that starts distributed training — always use `nccl` backend for GPUs.\n4. **Effective batch size = per_GPU_batch × num_GPUs** — scale learning rate proportionally and use warmup.\n5. **Ring All-Reduce** is the communication primitive that makes distributed training scale — no central bottleneck.\n\n### Quick Decision Guide\n\n```\nCan your model fit on 1 GPU?\n  ├─ Yes → Is training too slow?\n  │         ├─ Yes → Use DDP (multiple GPUs, same model)\n  │         └─ No  → Single GPU is fine\n  └─ No  → Use FSDP (shard the model across GPUs)\n              └─ Still doesn't fit? → Combine FSDP with model parallelism (tensor/pipeline parallel)\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}