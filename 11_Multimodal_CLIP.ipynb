{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Multimodal AI (CLIP)\n",
    "\n",
    "The future of AI is **Multimodal**: models that can understand text, images, audio, and video simultaneously. The breakthrough model that started this era is **CLIP (Contrastive Language-Image Pre-training)** by OpenAI.\n",
    "\n",
    "In this notebook, we will build the core components of CLIP from scratch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Contrastive Learning\n",
    "- Implement Dual Encoders (Image & Text)\n",
    "- Implement the Contrastive Loss function\n",
    "- Understand Zero-Shot Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Contrastive Learning\n",
    "\n",
    "CLIP doesn't predict labels (like \"cat\" or \"dog\"). Instead, it learns to match **images** to their **captions**.\n",
    "\n",
    "- **Positive pairs**: (Image of dog, \"A photo of a dog\") -> Maximize similarity\n",
    "- **Negative pairs**: (Image of dog, \"A photo of a car\") -> Minimize similarity\n",
    "\n",
    "We project both images and text into a shared **embedding space**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Dual Encoder\n",
    "\n",
    "We need two separate neural networks:\n",
    "1. **Image Encoder**: Turns pixels into a vector.\n",
    "2. **Text Encoder**: Turns text into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Simple CNN for demonstration\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, embedding_dim) # Assuming 32x32 input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 128)\n",
    "        self.rnn = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        x = hidden[-1]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "embed_dim = 64\n",
    "image_model = ImageEncoder(embed_dim)\n",
    "text_model = TextEncoder(vocab_size=1000, embedding_dim=embed_dim)\n",
    "print(\"Encoders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The CLIP Model\n",
    "\n",
    "The CLIP model puts them together. Crucially, it normalizes the embeddings so we can compute Cosine Similarity easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # 1. Get Embeddings\n",
    "        image_features = self.image_encoder(images)\n",
    "        text_features = self.text_encoder(text)\n",
    "\n",
    "        # 2. Normalize (L2 norm)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 3. Compute Similarity Matrix (Batch x Batch)\n",
    "        # logits[i][j] = similarity between image i and text j\n",
    "        logits = (image_features @ text_features.t()) / self.temperature\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = CLIP(image_model, text_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contrastive Loss\n",
    "\n",
    "We want the diagonal of the similarity matrix to be high (image matches its own text) and off-diagonal to be low.\n",
    "\n",
    "We use **Cross Entropy Loss** on both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(logits):\n",
    "    batch_size = logits.size(0)\n",
    "    targets = torch.arange(batch_size) # [0, 1, 2, ...]\n",
    "    \n",
    "    # Loss for (Image -> Text)\n",
    "    loss_i = F.cross_entropy(logits, targets)\n",
    "    # Loss for (Text -> Image)\n",
    "    loss_t = F.cross_entropy(logits.t(), targets)\n",
    "    \n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Dummy Forward Pass\n",
    "images = torch.randn(4, 3, 32, 32)\n",
    "text = torch.randint(0, 1000, (4, 10))\n",
    "\n",
    "logits = model(images, text)\n",
    "loss = contrastive_loss(logits)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zero-Shot Classification\n",
    "\n",
    "Once trained, how do we classify an image?\n",
    "1. Give the image to the Image Encoder.\n",
    "2. Give a list of possible class names (e.g., \"dog\", \"cat\", \"bird\") to the Text Encoder.\n",
    "3. The class with the highest similarity score wins!\n",
    "\n",
    "This is why it's called \"Zero-Shot\" â€” it can classify categories it has never seen during training, as long as it understands the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Multimodal**: Combining vision and text in one shared space.\n",
    "2. **Contrastive Loss**: Learning by comparing positive and negative pairs.\n",
    "3. **Dual Encoders**: Two towers (Image & Text) that meet at the end.\n",
    "4. **Zero-Shot**: Classifying without explicit training on those classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
