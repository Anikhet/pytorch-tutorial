{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Multimodal AI (CLIP)\n",
    "\n",
    "The future of AI is **Multimodal**: models that can understand text, images, audio, and video simultaneously. The breakthrough model that started this era is **CLIP (Contrastive Language-Image Pre-training)** by OpenAI.\n",
    "\n",
    "In this notebook, we will build the core components of CLIP from scratch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Contrastive Learning\n",
    "- Implement Dual Encoders (Image & Text)\n",
    "- Implement the Contrastive Loss function\n",
    "- Understand Zero-Shot Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Contrastive Learning\n",
    "\n",
    "CLIP doesn't predict labels (like \"cat\" or \"dog\"). Instead, it learns to match **images** to their **captions**.\n",
    "\n",
    "- **Positive pairs**: (Image of dog, \"A photo of a dog\") -> Maximize similarity\n",
    "- **Negative pairs**: (Image of dog, \"A photo of a car\") -> Minimize similarity\n",
    "\n",
    "We project both images and text into a shared **embedding space**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Dual Encoder\n",
    "\n",
    "We need two separate neural networks:\n",
    "1. **Image Encoder**: Turns pixels into a vector.\n",
    "2. **Text Encoder**: Turns text into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Simple CNN for demonstration\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, embedding_dim) # Assuming 32x32 input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 128)\n",
    "        self.rnn = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        x = hidden[-1]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "embed_dim = 64\n",
    "image_model = ImageEncoder(embed_dim)\n",
    "text_model = TextEncoder(vocab_size=1000, embedding_dim=embed_dim)\n",
    "print(\"Encoders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The CLIP Model\n",
    "\n",
    "The CLIP model puts them together. Crucially, it normalizes the embeddings so we can compute Cosine Similarity easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # 1. Get Embeddings\n",
    "        image_features = self.image_encoder(images)\n",
    "        text_features = self.text_encoder(text)\n",
    "\n",
    "        # 2. Normalize (L2 norm)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 3. Compute Similarity Matrix (Batch x Batch)\n",
    "        # logits[i][j] = similarity between image i and text j\n",
    "        logits = (image_features @ text_features.t()) / self.temperature\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = CLIP(image_model, text_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contrastive Loss\n",
    "\n",
    "We want the diagonal of the similarity matrix to be high (image matches its own text) and off-diagonal to be low.\n",
    "\n",
    "We use **Cross Entropy Loss** on both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(logits):\n",
    "    batch_size = logits.size(0)\n",
    "    targets = torch.arange(batch_size) # [0, 1, 2, ...]\n",
    "    \n",
    "    # Loss for (Image -> Text)\n",
    "    loss_i = F.cross_entropy(logits, targets)\n",
    "    # Loss for (Text -> Image)\n",
    "    loss_t = F.cross_entropy(logits.t(), targets)\n",
    "    \n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Dummy Forward Pass\n",
    "images = torch.randn(4, 3, 32, 32)\n",
    "text = torch.randint(0, 1000, (4, 10))\n",
    "\n",
    "logits = model(images, text)\n",
    "loss = contrastive_loss(logits)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zero-Shot Classification\n",
    "\n",
    "Once trained, how do we classify an image?\n",
    "1. Give the image to the Image Encoder.\n",
    "2. Give a list of possible class names (e.g., \"dog\", \"cat\", \"bird\") to the Text Encoder.\n",
    "3. The class with the highest similarity score wins!\n",
    "\n",
    "This is why it's called \"Zero-Shot\" — it can classify categories it has never seen during training, as long as it understands the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Understanding Temperature in Contrastive Learning\n\nThe **temperature** parameter is crucial for contrastive learning. Let's understand why."
  },
  {
   "cell_type": "code",
   "id": "4v45r2tl94u",
   "source": "# Temperature controls the \"sharpness\" of the probability distribution\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax_with_temp(logits, temperature):\n    logits = logits / temperature\n    exp_logits = np.exp(logits - np.max(logits))\n    return exp_logits / exp_logits.sum()\n\n# Example: similarities between one image and 5 text candidates\nsimilarities = np.array([0.8, 0.6, 0.5, 0.3, 0.1])  # Image matches text 0\n\ntemperatures = [0.01, 0.07, 0.5, 1.0]\nfig, axes = plt.subplots(1, 4, figsize=(14, 3))\n\nfor ax, temp in zip(axes, temperatures):\n    probs = softmax_with_temp(similarities, temp)\n    ax.bar(range(5), probs)\n    ax.set_title(f'τ = {temp}')\n    ax.set_xlabel('Text Index')\n    ax.set_ylabel('Probability')\n    ax.set_ylim(0, 1.1)\n\nplt.suptitle('Effect of Temperature on Probability Distribution', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Low temperature (τ=0.01): Almost one-hot, very confident\")\nprint(\"CLIP default (τ=0.07): Sharp but not extreme\")\nprint(\"High temperature (τ=1.0): Softer distribution, more exploration\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rw1d6fgwvb",
   "source": "## 7. InfoNCE Loss (The Formal Math)\n\nCLIP uses **InfoNCE (Noise Contrastive Estimation)** loss, which is a generalization of cross-entropy:\n\n$$\\mathcal{L}_{i \\to t} = -\\log \\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j) / \\tau)}$$\n\nThis is equivalent to:\n1. For each image, classify which of the N texts is its match\n2. Cross-entropy loss where the \"correct class\" is the diagonal\n\n**Why it works**: Maximizes mutual information between image and text representations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ylls0a2u08",
   "source": "## 8. Hard Negative Mining\n\nNot all negatives are equal. **Hard negatives** are samples that are similar but not matches - they provide the strongest learning signal.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aes0fno1wce",
   "source": "def contrastive_loss_with_hard_negatives(image_features, text_features, temperature=0.07):\n    \"\"\"\n    Enhanced contrastive loss with hard negative mining.\n    \n    Hard negatives: samples with high similarity but wrong label\n    - Easy negative: \"dog\" vs \"airplane\" (very different)\n    - Hard negative: \"golden retriever\" vs \"labrador\" (similar but different)\n    \"\"\"\n    batch_size = image_features.shape[0]\n    \n    # Normalize features\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n    \n    # Compute similarity matrix\n    logits = (image_features @ text_features.t()) / temperature\n    \n    # Labels are the diagonal (i matches i)\n    labels = torch.arange(batch_size, device=logits.device)\n    \n    # Standard InfoNCE loss\n    loss_i2t = F.cross_entropy(logits, labels)\n    loss_t2i = F.cross_entropy(logits.t(), labels)\n    \n    # Hard negative analysis (for monitoring)\n    with torch.no_grad():\n        # Find hardest negative for each image\n        logits_masked = logits.clone()\n        logits_masked[torch.arange(batch_size), torch.arange(batch_size)] = float('-inf')\n        hardest_neg_sim = logits_masked.max(dim=1).values.mean()\n        \n        # Positive similarity (diagonal)\n        pos_sim = logits.diag().mean()\n        \n        # Margin between positive and hardest negative\n        margin = pos_sim - hardest_neg_sim\n    \n    return (loss_i2t + loss_t2i) / 2, {\n        'positive_sim': pos_sim.item(),\n        'hardest_neg_sim': hardest_neg_sim.item(),\n        'margin': margin.item()\n    }\n\n# Demo\nbatch_size = 8\nembed_dim = 64\nimg_feats = torch.randn(batch_size, embed_dim)\ntxt_feats = torch.randn(batch_size, embed_dim)\n\nloss, metrics = contrastive_loss_with_hard_negatives(img_feats, txt_feats)\nprint(f\"Loss: {loss.item():.4f}\")\nprint(f\"Positive similarity: {metrics['positive_sim']:.4f}\")\nprint(f\"Hardest negative similarity: {metrics['hardest_neg_sim']:.4f}\")\nprint(f\"Margin (higher is better): {metrics['margin']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7n5o7xifyjc",
   "source": "## 9. Vision Transformer (ViT) Image Encoder\n\nModern CLIP uses **Vision Transformers** instead of CNNs for the image encoder.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5bfrc4p1nej",
   "source": "class PatchEmbedding(nn.Module):\n    \"\"\"Split image into patches and project to embedding dimension.\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.n_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, \n                              kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        # (B, C, H, W) -> (B, embed_dim, n_patches_h, n_patches_w)\n        x = self.proj(x)\n        # Flatten spatial dimensions\n        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, embed_dim)\n        return x\n\nclass ViTImageEncoder(nn.Module):\n    \"\"\"Simplified Vision Transformer for CLIP.\"\"\"\n    def __init__(self, img_size=224, patch_size=16, embed_dim=768, \n                 depth=12, num_heads=12, output_dim=512):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        n_patches = (img_size // patch_size) ** 2\n        \n        # CLS token and position embeddings\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n        \n        # Transformer blocks\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, \n            dim_feedforward=embed_dim * 4, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        \n        # Project to output dimension\n        self.proj = nn.Linear(embed_dim, output_dim)\n        self.ln = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n        \n        # Add CLS token\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)  # (B, n_patches+1, embed_dim)\n        \n        # Add position embeddings\n        x = x + self.pos_embed\n        \n        # Transformer\n        x = self.transformer(x)\n        \n        # Use CLS token as image representation\n        x = self.ln(x[:, 0])\n        x = self.proj(x)\n        \n        return x\n\n# Demo\nvit_encoder = ViTImageEncoder(img_size=32, patch_size=4, embed_dim=128, \n                               depth=4, num_heads=4, output_dim=64)\ndummy_image = torch.randn(2, 3, 32, 32)\noutput = vit_encoder(dummy_image)\nprint(f\"ViT output shape: {output.shape}\")  # (2, 64)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5ibxevu30fr",
   "source": "## 10. FAANG Interview Questions\n\n### Q1: Explain how CLIP achieves zero-shot classification without being trained on specific classes.\n\n**Answer**: CLIP learns a shared embedding space where both images and text are mapped to vectors. During training:\n1. It sees millions of (image, caption) pairs from the internet\n2. It learns to maximize similarity between matching pairs (InfoNCE loss)\n3. The text encoder learns semantic representations of natural language\n\nFor zero-shot classification:\n1. Create text prompts like \"a photo of a {class}\" for each class\n2. Encode the image and all prompts\n3. The class with highest cosine similarity wins\n\nThis works because:\n- The model understands language semantically, not just memorizes class names\n- It can generalize to any concept describable in natural language\n- No retraining or fine-tuning needed for new classes\n\n---\n\n### Q2: Why does CLIP use a learnable temperature parameter instead of a fixed value?\n\n**Answer**: Temperature controls the sharpness of the softmax distribution.\n\n**Fixed temperature issues**:\n- Too low: Loss becomes numerically unstable, gradients explode\n- Too high: All pairs become equally likely, slow learning\n\n**Learnable temperature**:\n- Starts at a reasonable value (e.g., 0.07)\n- Model learns optimal sharpness during training\n- Adapts to the difficulty of the task\n- Typically decreases during training as embeddings become more separable\n\nOpenAI found that learnable temperature consistently outperforms fixed values.\n\n---\n\n### Q3: What are the computational challenges of training CLIP and how are they addressed?\n\n**Answer**:\n\n**Challenge 1: Large batch sizes needed**\n- Contrastive learning requires many negatives (batch = negatives)\n- CLIP used batch size of 32,768!\n- **Solution**: Distributed training across many GPUs, gradient accumulation\n\n**Challenge 2: Memory for similarity matrix**\n- Similarity matrix is $N \\times N$ where N = batch size\n- For N=32768: ~4GB just for the matrix\n- **Solution**: Gradient checkpointing, mixed precision, distributed computation\n\n**Challenge 3: Expensive text encoding**\n- Must encode all text in batch for every forward pass\n- **Solution**: Cache text embeddings during evaluation, use efficient transformers\n\n**Challenge 4: Data quality**\n- Need high-quality (image, text) pairs at massive scale\n- **Solution**: Filter web data, use CLIP itself to filter (bootstrapping)\n\n---\n\n### Q4: How would you extend CLIP to handle more than 2 modalities (e.g., add audio)?\n\n**Answer**: Multi-modal extensions like ImageBind use:\n\n1. **Anchor modality**: Use images as the \"hub\" that connects all modalities\n2. **Shared embedding space**: All modalities map to same dimensional space\n3. **Pairwise contrastive learning**:\n   - Image-Text pairs (existing)\n   - Image-Audio pairs (new)\n   - Image-Video pairs (new)\n4. **Emergent alignments**: Audio-Text alignment emerges through shared image space\n\nKey insight: You don't need paired data for all modality combinations - alignment transfers through the anchor modality.\n\n---\n\n### Q5: What's the difference between early fusion and late fusion in multimodal models?\n\n**Answer**:\n\n| Aspect | Early Fusion | Late Fusion (CLIP) |\n|--------|-------------|-------------------|\n| **Architecture** | Combine inputs before encoder | Separate encoders, combine embeddings |\n| **Interaction** | Deep cross-modal attention | Dot product / shallow interaction |\n| **Flexibility** | Single model, fixed modalities | Modular, can swap encoders |\n| **Efficiency** | One forward pass | Can pre-compute embeddings |\n| **Use cases** | VQA, detailed understanding | Retrieval, classification |\n\nCLIP uses **late fusion** because:\n1. Enables efficient retrieval (pre-compute all embeddings)\n2. Modular design (upgrade image encoder without retraining text)\n3. Scales to massive datasets\n4. Simple to parallelize training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ewzb8dqkdj",
   "source": "## Key Takeaways\n\n1. **Multimodal**: Combining vision and text in one shared embedding space.\n2. **Contrastive Loss (InfoNCE)**: Learning by comparing positive and negative pairs.\n3. **Dual Encoders**: Two towers (Image & Text) that meet through similarity computation.\n4. **Zero-Shot**: Classifying without explicit training on those classes.\n5. **Temperature**: Controls softmax sharpness; learnable gives best results.\n6. **Hard Negatives**: The most informative training signal comes from challenging examples.\n7. **Vision Transformers**: Modern CLIP uses ViT, not CNNs, for image encoding.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}