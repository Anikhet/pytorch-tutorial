{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Model Deployment and Production\n",
    "\n",
    "Training a model is only half the battle. To use it in the real world (in a mobile app, a web server, or an embedded device), you need to **deploy** it. This often means optimizing it for speed and portability.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **TorchScript** (Tracing vs Scripting)\n",
    "- Export models to **ONNX** (Open Neural Network Exchange)\n",
    "- **Quantization**: Making models smaller and faster\n",
    "- Measure Inference Speed (Benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "- **Inference**: Using a trained model to make predictions (no backprop).\n",
    "- **Latency**: Time taken to process one input (lower is better).\n",
    "- **Throughput**: Number of inputs processed per second (higher is better).\n",
    "- **Serialization**: Saving a model to a file so it can be loaded anywhere (even in C++).\n",
    "- **Quantization**: Reducing the precision of numbers (e.g., 32-bit float -> 8-bit integer) to save memory and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Let's use a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "model = SimpleModel()\n",
    "model.eval() # Important! Set to eval mode for deployment\n",
    "example_input = torch.randn(1, 10)\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TorchScript: Tracing vs Scripting\n",
    "\n",
    "TorchScript allows you to serialize your models and run them in a C++ runtime (no Python needed!).\n",
    "\n",
    "### Method A: Tracing\n",
    "Runs the model with a dummy input and records the operations. Fast and easy, but fails with dynamic control flow (if/else).\n",
    "\n",
    "```python\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"traced_model.pt\")\n",
    "```\n",
    "\n",
    "### Method B: Scripting\n",
    "Analyzes the Python source code to compile it. Handles if/else loops correctly.\n",
    "\n",
    "```python\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"scripted_model.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Tracing\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "print(traced_model.code) # You can see the internal representation\n",
    "\n",
    "# Verify it works\n",
    "output_orig = model(example_input)\n",
    "output_traced = traced_model(example_input)\n",
    "assert torch.allclose(output_orig, output_traced)\n",
    "print(\"Tracing successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONNX Export\n",
    "\n",
    "ONNX is a standard format supported by many frameworks (TensorFlow, PyTorch, MATLAB, etc.). It's great for deploying to edge devices or using specialized runtimes like ONNX Runtime.\n",
    "\n",
    "```python\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    example_input, \n",
    "    \"model.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model, \n",
    "    example_input, \n",
    "    \"simple_model.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "print(\"Exported to simple_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization (Making it smaller)\n",
    "\n",
    "We can convert weights from Float32 to Int8. This reduces model size by 4x and speeds up inference on supported hardware.\n",
    "\n",
    "### Dynamic Quantization\n",
    "Quantizes weights ahead of time, but activations are quantized dynamically at runtime. Good for LSTMs/Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {nn.Linear}, # Layers to quantize\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(f\"Original Size: {os.path.getsize('traced_model.pt')/1024:.2f} KB\")\n",
    "# Note: For this tiny model, overhead might make it larger, but for large models it helps.\n",
    "torch.jit.save(torch.jit.trace(quantized_model, example_input), \"quantized_model.pt\")\n",
    "print(f\"Quantized Size: {os.path.getsize('quantized_model.pt')/1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tjwpo385en",
   "source": "## 5. Batch vs Online Inference (Critical Decision!)\n\nBefore deploying, you must choose between **batch** and **online** inference. This decision affects your entire architecture.\n\n### Batch Inference (Offline)\n- Generate predictions on a schedule (hourly, daily)\n- Store results in a database for later retrieval\n- **When to use**: Non-real-time applications, expensive models, large datasets\n- **Examples**: Daily product recommendations, overnight credit scoring, monthly churn predictions\n\n### Online Inference (Real-time)\n- Generate predictions on-demand when requested\n- Return results immediately (< 100ms typically)\n- **When to use**: Real-time applications, personalization, user-facing features\n- **Examples**: Fraud detection, chatbots, search ranking, content moderation\n\n### Key Trade-offs\n\n| Aspect | Batch Inference | Online Inference |\n|--------|----------------|------------------|\n| Latency | Hours to days | < 100ms |\n| Cost | Lower (scheduled compute) | Higher (always-on servers) |\n| Complexity | Simpler infrastructure | Requires load balancing, caching |\n| Use Case | Periodic predictions | Real-time responses |\n| Scalability | Easy (process in parallel) | Harder (handle traffic spikes) |\n\n**Important**: Inference costs account for **up to 90% of production ML costs** in 2025!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1a50fj6wckv",
   "source": "# Example: Batch Inference Pattern\nimport torch\n\n# Simulate a batch of data\nbatch_data = torch.randn(1000, 10)  # 1000 samples\n\n# Process in batches for efficiency\nbatch_size = 32\npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i in range(0, len(batch_data), batch_size):\n        batch = batch_data[i:i+batch_size]\n        preds = model(batch)\n        predictions.append(preds)\n\n# Combine all predictions\nall_predictions = torch.cat(predictions, dim=0)\nprint(f\"Batch inference: Processed {len(all_predictions)} samples\")\nprint(f\"Predictions shape: {all_predictions.shape}\")\n\n# In production, you would save these to a database:\n# db.save_predictions(all_predictions)  # Store for later lookup",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gjfv8r3fp3m",
   "source": "## 6. Modern Serving Frameworks for LLMs (2025 Update)\n\n**IMPORTANT UPDATE**: TorchServe was archived in August 2025 and is now in \"Limited Maintenance\" mode. For production LLM deployment in 2025, use these modern alternatives:\n\n### vLLM - The Current Standard\n**Best for**: Flexibility, rapid deployment, Hugging Face integration\n\n```python\n# Installation: pip install vllm\n\nfrom vllm import LLM, SamplingParams\n\n# Initialize with advanced optimizations\nllm = LLM(\n    model=\"meta-llama/Llama-3-8B\",\n    tensor_parallel_size=2,           # Multi-GPU\n    gpu_memory_utilization=0.95,      # Maximize GPU usage\n    enable_prefix_caching=True,       # KV cache optimization\n    max_model_len=8192               # Context length\n)\n\n# Automatic continuous batching happens under the hood\nprompts = [\"What is PyTorch?\", \"Explain neural networks\"]\nsampling_params = SamplingParams(temperature=0.7, max_tokens=100)\n\noutputs = llm.generate(prompts, sampling_params)\nfor output in outputs:\n    print(output.outputs[0].text)\n```\n\n**Key Features**:\n- **PagedAttention**: Treats KV cache like OS virtual memory (huge memory savings!)\n- **Continuous Batching**: No waiting for entire batch to finish\n- **120-160 req/sec** typical throughput\n- Hugging Face compatible out-of-the-box\n\n### TensorRT-LLM - Maximum Performance\n**Best for**: NVIDIA hardware, production systems requiring maximum throughput\n\n**Key Features**:\n- Custom NVIDIA kernels (fastest on H100/A100)\n- **180-220 req/sec** throughput, **35-50ms** time-to-first-token\n- FP8/INT4/INT8 quantization support\n- **10,000+ tokens/sec** on H100 with FP8\n\n**Trade-off**: Less flexible, NVIDIA-specific, harder setup\n\n### When to Use Which?\n\n| Framework | Use When | Pros | Cons |\n|-----------|----------|------|------|\n| **vLLM** | Standard LLM deployment | Easy, flexible, HF-compatible | Slightly slower than TensorRT |\n| **TensorRT-LLM** | Max performance needed | Fastest, NVIDIA-optimized | Complex setup, vendor lock-in |\n| **FastAPI + PyTorch** | Custom models, small scale | Full control, simple | Manual optimization needed |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4s504crnpkk",
   "source": "## 7. Practical Example: FastAPI + PyTorch Serving\n\nFor custom models or when you need full control, FastAPI is the simplest production-ready option.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2fjk1bnd4ws",
   "source": "# Save this as: serve_model.py\n# Run with: uvicorn serve_model:app --reload\n\ncode_example = '''\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport torch\nimport torch.nn as nn\n\napp = FastAPI()\n\n# Load model once at startup\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 50)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(50, 2)\n    \n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\nmodel = SimpleModel()\nmodel.load_state_dict(torch.load(\"model_weights.pth\"))\nmodel.eval()\n\n# Request/Response schema\nclass PredictionRequest(BaseModel):\n    features: list[float]\n\nclass PredictionResponse(BaseModel):\n    prediction: list[float]\n    latency_ms: float\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(request: PredictionRequest):\n    import time\n    start = time.time()\n    \n    # Convert to tensor\n    x = torch.tensor([request.features])\n    \n    # Inference\n    with torch.no_grad():\n        output = model(x)\n    \n    latency = (time.time() - start) * 1000  # Convert to ms\n    \n    return PredictionResponse(\n        prediction=output[0].tolist(),\n        latency_ms=latency\n    )\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n'''\n\nprint(\"FastAPI Serving Example:\")\nprint(code_example)\nprint(\"\\nTo deploy:\")\nprint(\"1. Save the code above as 'serve_model.py'\")\nprint(\"2. Install: pip install fastapi uvicorn\")\nprint(\"3. Run: uvicorn serve_model:app --host 0.0.0.0 --port 8000\")\nprint(\"4. Test: curl -X POST http://localhost:8000/predict -H 'Content-Type: application/json' -d '{\\\"features\\\": [1,2,3,4,5,6,7,8,9,10]}'\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8db236f",
   "metadata": {},
   "source": "## Summary: 2025 Deployment Decision Tree\n\n```\nStart\n  â†“\nAre you deploying an LLM (Large Language Model)?\n  â”œâ”€ Yes â†’ Are you using NVIDIA GPUs (A100/H100)?\n  â”‚         â”œâ”€ Yes, need MAX performance â†’ TensorRT-LLM\n  â”‚         â””â”€ No, or need flexibility â†’ vLLM\n  â”‚\n  â””â”€ No (Custom Model) â†’ Do you need real-time predictions?\n            â”œâ”€ Yes (< 100ms latency) â†’ FastAPI + Docker + GPU\n            â””â”€ No (periodic predictions) â†’ Batch Inference\n                  â†“\n            Do you need to deploy to production?\n              â”œâ”€ Yes â†’ Docker + Kubernetes + Monitoring\n              â””â”€ No â†’ Local FastAPI server\n```\n\n## 2025 Production Checklist\n\n**Export & Optimization:**\n- âœ… TorchScript/ONNX for model export\n- âœ… Quantization (INT8/INT4 for 4x smaller models)\n- âœ… ONNX Runtime for edge deployment\n\n**LLM Serving (if applicable):**\n- âœ… vLLM for standard deployments (PagedAttention, continuous batching)\n- âœ… TensorRT-LLM for maximum NVIDIA performance\n- âš ï¸  **AVOID**: TorchServe (archived August 2025, limited maintenance)\n\n**Custom Model Serving:**\n- âœ… FastAPI for RESTful APIs (lightweight, flexible)\n- âœ… Docker for containerization (reproducibility)\n- âœ… Kubernetes for orchestration (auto-scaling)\n\n**Inference Pattern:**\n- âœ… Online inference for real-time (< 100ms latency)\n- âœ… Batch inference for periodic (lower cost, higher throughput)\n- âš ï¸  **Remember**: Inference = 90% of production ML costs!\n\n**Monitoring (Critical!):**\n- âœ… Latency metrics (P50, P95, P99)\n- âœ… Throughput tracking (requests/sec)\n- âœ… Model drift detection\n- âœ… Error rate monitoring\n- âœ… Resource utilization (GPU/CPU/Memory)\n\n**Deployment Strategies:**\n- âœ… Canary deployments (gradual rollout)\n- âœ… Blue-Green deployments (zero downtime)\n- âœ… A/B testing (compare model versions)\n\n## What FAANG/Top Tech Companies Look For in 2025\n\n- âœ… Can you choose the right inference pattern? (Batch vs Online)\n- âœ… Can you optimize LLM inference? (vLLM, KV cache, quantization)\n- âœ… Can you serve models efficiently? (FastAPI, modern frameworks)\n- âœ… Can you containerize and orchestrate? (Docker, Kubernetes)\n- âœ… Can you monitor production systems? (Latency, drift, errors)\n- âœ… Can you handle deployments safely? (Canary, rollback, A/B testing)\n- âœ… Do you understand cost optimization? (Inference is 90% of costs!)\n\nYou now know the **2025 state-of-the-art** for model deployment! ðŸš€",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}