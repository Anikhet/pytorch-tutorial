{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Model Deployment and Production\n",
    "\n",
    "Training a model is only half the battle. To use it in the real world (in a mobile app, a web server, or an embedded device), you need to **deploy** it. This often means optimizing it for speed and portability.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **TorchScript** (Tracing vs Scripting)\n",
    "- Export models to **ONNX** (Open Neural Network Exchange)\n",
    "- **Quantization**: Making models smaller and faster\n",
    "- Measure Inference Speed (Benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "- **Inference**: Using a trained model to make predictions (no backprop).\n",
    "- **Latency**: Time taken to process one input (lower is better).\n",
    "- **Throughput**: Number of inputs processed per second (higher is better).\n",
    "- **Serialization**: Saving a model to a file so it can be loaded anywhere (even in C++).\n",
    "- **Quantization**: Reducing the precision of numbers (e.g., 32-bit float -> 8-bit integer) to save memory and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Let's use a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "model = SimpleModel()\n",
    "model.eval() # Important! Set to eval mode for deployment\n",
    "example_input = torch.randn(1, 10)\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TorchScript: Tracing vs Scripting\n",
    "\n",
    "TorchScript allows you to serialize your models and run them in a C++ runtime (no Python needed!).\n",
    "\n",
    "### Method A: Tracing\n",
    "Runs the model with a dummy input and records the operations. Fast and easy, but fails with dynamic control flow (if/else).\n",
    "\n",
    "```python\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"traced_model.pt\")\n",
    "```\n",
    "\n",
    "### Method B: Scripting\n",
    "Analyzes the Python source code to compile it. Handles if/else loops correctly.\n",
    "\n",
    "```python\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"scripted_model.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Tracing\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "print(traced_model.code) # You can see the internal representation\n",
    "\n",
    "# Verify it works\n",
    "output_orig = model(example_input)\n",
    "output_traced = traced_model(example_input)\n",
    "assert torch.allclose(output_orig, output_traced)\n",
    "print(\"Tracing successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONNX Export\n",
    "\n",
    "ONNX is a standard format supported by many frameworks (TensorFlow, PyTorch, MATLAB, etc.). It's great for deploying to edge devices or using specialized runtimes like ONNX Runtime.\n",
    "\n",
    "```python\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    example_input, \n",
    "    \"model.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model, \n",
    "    example_input, \n",
    "    \"simple_model.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "print(\"Exported to simple_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization (Making it smaller)\n",
    "\n",
    "We can convert weights from Float32 to Int8. This reduces model size by 4x and speeds up inference on supported hardware.\n",
    "\n",
    "### Dynamic Quantization\n",
    "Quantizes weights ahead of time, but activations are quantized dynamically at runtime. Good for LSTMs/Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {nn.Linear}, # Layers to quantize\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(f\"Original Size: {os.path.getsize('traced_model.pt')/1024:.2f} KB\")\n",
    "# Note: For this tiny model, overhead might make it larger, but for large models it helps.\n",
    "torch.jit.save(torch.jit.trace(quantized_model, example_input), \"quantized_model.pt\")\n",
    "print(f\"Quantized Size: {os.path.getsize('quantized_model.pt')/1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Always `model.eval()`** before exporting.\n",
    "2. Use **TorchScript** for C++ production environments.\n",
    "3. Use **ONNX** for cross-platform compatibility.\n",
    "4. Use **Quantization** to reduce size and latency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
