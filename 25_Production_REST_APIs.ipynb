{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production REST APIs for ML Services\n",
    "\n",
    "## Overview\n",
    "Building production-grade REST APIs for ML inference with:\n",
    "- **FastAPI**: Modern, async Python framework\n",
    "- **Authentication & Authorization**: OAuth2, JWT, API keys\n",
    "- **Rate Limiting**: Protect against abuse\n",
    "- **Versioning**: Breaking changes without downtime\n",
    "- **Monitoring**: Prometheus metrics, structured logging\n",
    "- **Performance**: Caching, batch processing, load balancing\n",
    "\n",
    "## Why Production APIs Matter\n",
    "- 90% of ML costs are inference (not training)\n",
    "- Availability = revenue (99.9% uptime = 8.76 hours downtime/year)\n",
    "- Security breaches are expensive (avg $4.35M per breach)\n",
    "- Performance impacts UX (100ms delay = 7% conversion drop)\n",
    "\n",
    "## Interview Focus\n",
    "- RESTful design principles\n",
    "- Async programming patterns\n",
    "- Security best practices\n",
    "- API versioning strategies\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation\n",
    "# pip install fastapi uvicorn pydantic python-jose python-multipart prometheus-client redis slowapi\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, status, Request, Header\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm, HTTPBearer, HTTPAuthorizationCredentials\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "from jose import JWTError, jwt\n",
    "import torch\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from functools import wraps\n",
    "import hashlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Core API Structure\n",
    "\n",
    "### Production-Ready FastAPI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration with environment variables.\"\"\"\n",
    "    API_TITLE = \"ML Inference API\"\n",
    "    API_VERSION = \"v1\"\n",
    "    SECRET_KEY = \"your-secret-key-change-in-production\"  # Use env vars in production\n",
    "    ALGORITHM = \"HS256\"\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
    "    RATE_LIMIT = \"100/minute\"\n",
    "    \n",
    "    # Model settings\n",
    "    MODEL_PATH = \"./models/\"\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# Initialize FastAPI with metadata\n",
    "app = FastAPI(\n",
    "    title=Config.API_TITLE,\n",
    "    description=\"Production ML inference service with authentication and monitoring\",\n",
    "    version=Config.API_VERSION,\n",
    "    docs_url=f\"/{Config.API_VERSION}/docs\",\n",
    "    redoc_url=f\"/{Config.API_VERSION}/redoc\"\n",
    ")\n",
    "\n",
    "# CORS middleware for cross-origin requests\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure properly in production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "print(f\"✅ FastAPI app initialized: {Config.API_TITLE} {Config.API_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Request/Response Models\n",
    "\n",
    "### Type-Safe Schemas with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class InferenceRequest(BaseModel):\n",
    "    \"\"\"Input schema with validation.\"\"\"\n",
    "    text: str = Field(..., min_length=1, max_length=5000, description=\"Input text for inference\")\n",
    "    model_version: Optional[str] = Field(\"latest\", description=\"Model version to use\")\n",
    "    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0, description=\"Sampling temperature\")\n",
    "    max_tokens: Optional[int] = Field(100, ge=1, le=2048, description=\"Max output tokens\")\n",
    "    \n",
    "    @validator('text')\n",
    "    def validate_text(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError('Text cannot be empty')\n",
    "        return v.strip()\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"text\": \"What is machine learning?\",\n",
    "                \"model_version\": \"v2.1\",\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 150\n",
    "            }\n",
    "        }\n",
    "\n",
    "class InferenceResponse(BaseModel):\n",
    "    \"\"\"Output schema with metadata.\"\"\"\n",
    "    request_id: str = Field(..., description=\"Unique request identifier\")\n",
    "    result: str = Field(..., description=\"Model prediction\")\n",
    "    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description=\"Prediction confidence\")\n",
    "    latency_ms: float = Field(..., description=\"Inference latency in milliseconds\")\n",
    "    model_version: str = Field(..., description=\"Model version used\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.utcnow)\n",
    "\n",
    "class BatchInferenceRequest(BaseModel):\n",
    "    \"\"\"Batch processing for efficiency.\"\"\"\n",
    "    inputs: List[str] = Field(..., min_items=1, max_items=100)\n",
    "    model_version: Optional[str] = \"latest\"\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    version: str\n",
    "    uptime_seconds: float\n",
    "    gpu_available: bool\n",
    "    models_loaded: List[str]\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    \"\"\"Standardized error response.\"\"\"\n",
    "    error: str\n",
    "    detail: Optional[str] = None\n",
    "    request_id: Optional[str] = None\n",
    "    timestamp: datetime = Field(default_factory=datetime.utcnow)\n",
    "\n",
    "print(\"✅ Request/Response models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Authentication & Authorization\n",
    "\n",
    "### JWT-Based Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OAuth2 scheme\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f\"/{Config.API_VERSION}/token\")\n",
    "security = HTTPBearer()\n",
    "\n",
    "class User(BaseModel):\n",
    "    username: str\n",
    "    email: Optional[str] = None\n",
    "    full_name: Optional[str] = None\n",
    "    disabled: Optional[bool] = None\n",
    "    tier: str = \"free\"  # free, pro, enterprise\n",
    "\n",
    "class UserInDB(User):\n",
    "    hashed_password: str\n",
    "\n",
    "# Mock database (use real DB in production)\n",
    "fake_users_db = {\n",
    "    \"demo_user\": {\n",
    "        \"username\": \"demo_user\",\n",
    "        \"full_name\": \"Demo User\",\n",
    "        \"email\": \"demo@example.com\",\n",
    "        \"hashed_password\": hashlib.sha256(\"demo_password\".encode()).hexdigest(),\n",
    "        \"disabled\": False,\n",
    "        \"tier\": \"pro\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def verify_password(plain_password: str, hashed_password: str) -> bool:\n",
    "    \"\"\"Verify password (use bcrypt in production).\"\"\"\n",
    "    return hashlib.sha256(plain_password.encode()).hexdigest() == hashed_password\n",
    "\n",
    "def get_user(username: str) -> Optional[UserInDB]:\n",
    "    \"\"\"Retrieve user from database.\"\"\"\n",
    "    if username in fake_users_db:\n",
    "        user_dict = fake_users_db[username]\n",
    "        return UserInDB(**user_dict)\n",
    "    return None\n",
    "\n",
    "def authenticate_user(username: str, password: str) -> Optional[UserInDB]:\n",
    "    \"\"\"Authenticate user credentials.\"\"\"\n",
    "    user = get_user(username)\n",
    "    if not user or not verify_password(password, user.hashed_password):\n",
    "        return None\n",
    "    return user\n",
    "\n",
    "def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:\n",
    "    \"\"\"Generate JWT access token.\"\"\"\n",
    "    to_encode = data.copy()\n",
    "    if expires_delta:\n",
    "        expire = datetime.utcnow() + expires_delta\n",
    "    else:\n",
    "        expire = datetime.utcnow() + timedelta(minutes=15)\n",
    "    \n",
    "    to_encode.update({\"exp\": expire})\n",
    "    encoded_jwt = jwt.encode(to_encode, Config.SECRET_KEY, algorithm=Config.ALGORITHM)\n",
    "    return encoded_jwt\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme)) -> User:\n",
    "    \"\"\"Validate token and return current user.\"\"\"\n",
    "    credentials_exception = HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Could not validate credentials\",\n",
    "        headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        payload = jwt.decode(token, Config.SECRET_KEY, algorithms=[Config.ALGORITHM])\n",
    "        username: str = payload.get(\"sub\")\n",
    "        if username is None:\n",
    "            raise credentials_exception\n",
    "    except JWTError:\n",
    "        raise credentials_exception\n",
    "    \n",
    "    user = get_user(username)\n",
    "    if user is None:\n",
    "        raise credentials_exception\n",
    "    return user\n",
    "\n",
    "async def get_current_active_user(current_user: User = Depends(get_current_user)) -> User:\n",
    "    \"\"\"Check if user is active.\"\"\"\n",
    "    if current_user.disabled:\n",
    "        raise HTTPException(status_code=400, detail=\"Inactive user\")\n",
    "    return current_user\n",
    "\n",
    "print(\"✅ Authentication system configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Rate Limiting\n",
    "\n",
    "### Protect Against Abuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "from threading import Lock\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter.\"\"\"\n",
    "    \n",
    "    def __init__(self, rate: int, per: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rate: Number of requests allowed\n",
    "            per: Time window in seconds\n",
    "        \"\"\"\n",
    "        self.rate = rate\n",
    "        self.per = per\n",
    "        self.allowance = defaultdict(lambda: rate)\n",
    "        self.last_check = defaultdict(lambda: time.time())\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def is_allowed(self, key: str) -> bool:\n",
    "        \"\"\"Check if request is allowed.\"\"\"\n",
    "        with self.lock:\n",
    "            current = time.time()\n",
    "            time_passed = current - self.last_check[key]\n",
    "            self.last_check[key] = current\n",
    "            \n",
    "            # Add tokens based on time passed\n",
    "            self.allowance[key] += time_passed * (self.rate / self.per)\n",
    "            \n",
    "            if self.allowance[key] > self.rate:\n",
    "                self.allowance[key] = self.rate\n",
    "            \n",
    "            if self.allowance[key] < 1.0:\n",
    "                return False\n",
    "            \n",
    "            self.allowance[key] -= 1.0\n",
    "            return True\n",
    "\n",
    "# Create rate limiters for different tiers\n",
    "rate_limiters = {\n",
    "    \"free\": RateLimiter(rate=10, per=60),      # 10 req/min\n",
    "    \"pro\": RateLimiter(rate=100, per=60),      # 100 req/min\n",
    "    \"enterprise\": RateLimiter(rate=1000, per=60)  # 1000 req/min\n",
    "}\n",
    "\n",
    "async def check_rate_limit(request: Request, user: User = Depends(get_current_active_user)):\n",
    "    \"\"\"Dependency for rate limiting.\"\"\"\n",
    "    limiter = rate_limiters.get(user.tier, rate_limiters[\"free\"])\n",
    "    \n",
    "    if not limiter.is_allowed(user.username):\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n",
    "            detail=f\"Rate limit exceeded. Tier: {user.tier}\"\n",
    "        )\n",
    "    \n",
    "    return user\n",
    "\n",
    "print(\"✅ Rate limiter configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ML Model Service\n",
    "\n",
    "### Model Loading and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ModelService:\n",
    "    \"\"\"Singleton model service with lazy loading.\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    _lock = asyncio.Lock()\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(ModelService, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    async def initialize(self):\n",
    "        \"\"\"Load models asynchronously.\"\"\"\n",
    "        if self._initialized:\n",
    "            return\n",
    "        \n",
    "        async with self._lock:\n",
    "            if self._initialized:\n",
    "                return\n",
    "            \n",
    "            logger.info(\"Loading ML models...\")\n",
    "            \n",
    "            # Simulate model loading\n",
    "            await asyncio.sleep(1)\n",
    "            \n",
    "            self.models = {\n",
    "                \"latest\": \"v2.1\",\n",
    "                \"v2.1\": self._create_dummy_model(),\n",
    "                \"v2.0\": self._create_dummy_model()\n",
    "            }\n",
    "            \n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.start_time = time.time()\n",
    "            self._initialized = True\n",
    "            \n",
    "            logger.info(f\"Models loaded on {self.device}\")\n",
    "    \n",
    "    def _create_dummy_model(self):\n",
    "        \"\"\"Create a simple model for demonstration.\"\"\"\n",
    "        return torch.nn.Linear(10, 2)\n",
    "    \n",
    "    async def predict(self, text: str, model_version: str = \"latest\", **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Run inference.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Resolve version\n",
    "        if model_version == \"latest\":\n",
    "            model_version = self.models[\"latest\"]\n",
    "        \n",
    "        if model_version not in self.models:\n",
    "            raise ValueError(f\"Model version {model_version} not found\")\n",
    "        \n",
    "        # Simulate inference\n",
    "        await asyncio.sleep(0.1)  # Simulate model processing\n",
    "        \n",
    "        # Mock prediction\n",
    "        result = f\"Processed: {text[:50]}...\"\n",
    "        confidence = np.random.uniform(0.8, 0.99)\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"result\": result,\n",
    "            \"confidence\": confidence,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"model_version\": model_version\n",
    "        }\n",
    "    \n",
    "    async def batch_predict(self, texts: List[str], model_version: str = \"latest\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Batch inference for efficiency.\"\"\"\n",
    "        # Process in parallel\n",
    "        tasks = [self.predict(text, model_version) for text in texts]\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def get_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return service health.\"\"\"\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"uptime_seconds\": time.time() - self.start_time,\n",
    "            \"gpu_available\": torch.cuda.is_available(),\n",
    "            \"models_loaded\": [k for k in self.models.keys() if k != \"latest\"]\n",
    "        }\n",
    "\n",
    "# Global model service\n",
    "model_service = ModelService()\n",
    "\n",
    "print(\"✅ Model service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: API Endpoints\n",
    "\n",
    "### RESTful Routes with Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import uuid\n",
    "\n",
    "# Startup event\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize services on startup.\"\"\"\n",
    "    await model_service.initialize()\n",
    "    logger.info(\"API server started\")\n",
    "\n",
    "# Health check (no auth required)\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"Monitoring\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for load balancers.\"\"\"\n",
    "    health = model_service.get_health()\n",
    "    return HealthResponse(\n",
    "        version=Config.API_VERSION,\n",
    "        **health\n",
    "    )\n",
    "\n",
    "# Authentication endpoint\n",
    "@app.post(f\"/{Config.API_VERSION}/token\", tags=[\"Authentication\"])\n",
    "async def login(form_data: OAuth2PasswordRequestForm = Depends()):\n",
    "    \"\"\"Obtain access token.\"\"\"\n",
    "    user = authenticate_user(form_data.username, form_data.password)\n",
    "    if not user:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Incorrect username or password\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )\n",
    "    \n",
    "    access_token_expires = timedelta(minutes=Config.ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    access_token = create_access_token(\n",
    "        data={\"sub\": user.username, \"tier\": user.tier},\n",
    "        expires_delta=access_token_expires\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"access_token\": access_token,\n",
    "        \"token_type\": \"bearer\",\n",
    "        \"expires_in\": Config.ACCESS_TOKEN_EXPIRE_MINUTES * 60\n",
    "    }\n",
    "\n",
    "# User info endpoint\n",
    "@app.get(f\"/{Config.API_VERSION}/users/me\", response_model=User, tags=[\"Users\"])\n",
    "async def read_users_me(current_user: User = Depends(get_current_active_user)):\n",
    "    \"\"\"Get current user information.\"\"\"\n",
    "    return current_user\n",
    "\n",
    "# Single inference endpoint\n",
    "@app.post(\n",
    "    f\"/{Config.API_VERSION}/predict\",\n",
    "    response_model=InferenceResponse,\n",
    "    tags=[\"Inference\"],\n",
    "    status_code=status.HTTP_200_OK\n",
    ")\n",
    "async def predict(\n",
    "    request: InferenceRequest,\n",
    "    user: User = Depends(check_rate_limit)\n",
    "):\n",
    "    \"\"\"Run ML inference on input text.\"\"\"\n",
    "    request_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Request {request_id} from user {user.username}\")\n",
    "        \n",
    "        prediction = await model_service.predict(\n",
    "            request.text,\n",
    "            model_version=request.model_version,\n",
    "            temperature=request.temperature,\n",
    "            max_tokens=request.max_tokens\n",
    "        )\n",
    "        \n",
    "        return InferenceResponse(\n",
    "            request_id=request_id,\n",
    "            **prediction\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in request {request_id}: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "# Batch inference endpoint\n",
    "@app.post(\n",
    "    f\"/{Config.API_VERSION}/predict/batch\",\n",
    "    tags=[\"Inference\"],\n",
    "    status_code=status.HTTP_200_OK\n",
    ")\n",
    "async def batch_predict(\n",
    "    request: BatchInferenceRequest,\n",
    "    user: User = Depends(check_rate_limit)\n",
    "):\n",
    "    \"\"\"Batch inference for multiple inputs.\"\"\"\n",
    "    request_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Batch request {request_id} with {len(request.inputs)} items\")\n",
    "        \n",
    "        predictions = await model_service.batch_predict(\n",
    "            request.inputs,\n",
    "            model_version=request.model_version\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"predictions\": predictions,\n",
    "            \"count\": len(predictions)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in batch request {request_id}: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "# Model info endpoint\n",
    "@app.get(f\"/{Config.API_VERSION}/models\", tags=[\"Models\"])\n",
    "async def list_models(user: User = Depends(get_current_active_user)):\n",
    "    \"\"\"List available models.\"\"\"\n",
    "    return {\n",
    "        \"models\": model_service.get_health()[\"models_loaded\"],\n",
    "        \"default\": model_service.models[\"latest\"]\n",
    "    }\n",
    "\n",
    "# Error handlers\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request: Request, exc: HTTPException):\n",
    "    \"\"\"Custom error response format.\"\"\"\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\n",
    "            \"error\": exc.detail,\n",
    "            \"status_code\": exc.status_code,\n",
    "            \"timestamp\": datetime.utcnow().isoformat()\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"✅ API endpoints configured\")\n",
    "print(f\"\\nAPI Documentation: http://localhost:8000/{Config.API_VERSION}/docs\")\n",
    "print(f\"\\nTest credentials:\")\n",
    "print(f\"  Username: demo_user\")\n",
    "print(f\"  Password: demo_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Monitoring & Metrics\n",
    "\n",
    "### Prometheus Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST\n",
    "from fastapi.responses import Response\n",
    "\n",
    "# Define metrics\n",
    "REQUEST_COUNT = Counter(\n",
    "    'api_requests_total',\n",
    "    'Total API requests',\n",
    "    ['method', 'endpoint', 'status']\n",
    ")\n",
    "\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'api_request_latency_seconds',\n",
    "    'API request latency',\n",
    "    ['method', 'endpoint']\n",
    ")\n",
    "\n",
    "INFERENCE_LATENCY = Histogram(\n",
    "    'inference_latency_seconds',\n",
    "    'Model inference latency',\n",
    "    ['model_version']\n",
    ")\n",
    "\n",
    "ACTIVE_REQUESTS = Gauge(\n",
    "    'api_active_requests',\n",
    "    'Number of active requests'\n",
    ")\n",
    "\n",
    "# Middleware for automatic metrics\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request: Request, call_next):\n",
    "    \"\"\"Track request metrics.\"\"\"\n",
    "    ACTIVE_REQUESTS.inc()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = await call_next(request)\n",
    "        status_code = response.status_code\n",
    "    except Exception as e:\n",
    "        status_code = 500\n",
    "        raise\n",
    "    finally:\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        REQUEST_COUNT.labels(\n",
    "            method=request.method,\n",
    "            endpoint=request.url.path,\n",
    "            status=status_code\n",
    "        ).inc()\n",
    "        \n",
    "        REQUEST_LATENCY.labels(\n",
    "            method=request.method,\n",
    "            endpoint=request.url.path\n",
    "        ).observe(latency)\n",
    "        \n",
    "        ACTIVE_REQUESTS.dec()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Metrics endpoint\n",
    "@app.get(\"/metrics\", tags=[\"Monitoring\"])\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint.\"\"\"\n",
    "    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "\n",
    "print(\"✅ Prometheus metrics configured\")\n",
    "print(\"Metrics available at: http://localhost:8000/metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Running the API\n",
    "\n",
    "### Start Server and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# To run the server:\n",
    "# uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4 --reload\n",
    "\n",
    "# Example client code\n",
    "import requests\n",
    "\n",
    "class APIClient:\n",
    "    \"\"\"Production API client with retries and error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, username: str, password: str):\n",
    "        self.base_url = base_url\n",
    "        self.token = self._login(username, password)\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "    \n",
    "    def _login(self, username: str, password: str) -> str:\n",
    "        \"\"\"Authenticate and get token.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/token\",\n",
    "            data={\"username\": username, \"password\": password}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"access_token\"]\n",
    "    \n",
    "    def predict(self, text: str, **kwargs) -> dict:\n",
    "        \"\"\"Single prediction.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/predict\",\n",
    "            json={\"text\": text, **kwargs},\n",
    "            headers=self.headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def batch_predict(self, texts: List[str], **kwargs) -> dict:\n",
    "        \"\"\"Batch prediction.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/predict/batch\",\n",
    "            json={\"inputs\": texts, **kwargs},\n",
    "            headers=self.headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n=== Example Client Usage ===\")\n",
    "print(\"\"\"\n",
    "# Initialize client\n",
    "client = APIClient(\n",
    "    base_url=\"http://localhost:8000\",\n",
    "    username=\"demo_user\",\n",
    "    password=\"demo_password\"\n",
    ")\n",
    "\n",
    "# Single prediction\n",
    "result = client.predict(\"What is machine learning?\")\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Latency: {result['latency_ms']:.2f}ms\")\n",
    "\n",
    "# Batch prediction\n",
    "batch_result = client.batch_predict([\n",
    "    \"Question 1\",\n",
    "    \"Question 2\",\n",
    "    \"Question 3\"\n",
    "])\n",
    "print(f\"Processed {batch_result['count']} items\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Production API Checklist:\n",
    "1. ✅ **Authentication**: JWT tokens, secure password hashing\n",
    "2. ✅ **Authorization**: Role-based access control\n",
    "3. ✅ **Rate Limiting**: Tiered limits to prevent abuse\n",
    "4. ✅ **Validation**: Pydantic models with type checking\n",
    "5. ✅ **Versioning**: URL-based versioning (v1, v2)\n",
    "6. ✅ **Monitoring**: Prometheus metrics, structured logging\n",
    "7. ✅ **Error Handling**: Consistent error responses\n",
    "8. ✅ **Performance**: Async endpoints, batch processing\n",
    "9. ✅ **Documentation**: Auto-generated OpenAPI docs\n",
    "10. ✅ **Health Checks**: For load balancers and orchestrators\n",
    "\n",
    "### Deployment Considerations:\n",
    "- **Load Balancing**: NGINX, HAProxy, AWS ALB\n",
    "- **Auto-Scaling**: Based on CPU/memory/request count\n",
    "- **SSL/TLS**: Let's Encrypt, AWS ACM\n",
    "- **Caching**: Redis for embeddings/predictions\n",
    "- **Database**: PostgreSQL for users, requests\n",
    "- **Monitoring**: Prometheus + Grafana, DataDog\n",
    "- **Logging**: ELK stack, CloudWatch\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "1. **How do you handle API versioning without breaking clients?**\n",
    "   - URL-based: /v1/predict, /v2/predict\n",
    "   - Header-based: Accept: application/vnd.api.v2+json\n",
    "   - Deprecation policy: 6-month sunset period\n",
    "\n",
    "2. **Explain JWT vs API Keys for authentication.**\n",
    "   - JWT: Stateless, contains user info, expires automatically\n",
    "   - API Keys: Stateful, requires DB lookup, manual revocation\n",
    "   - Use JWT for user sessions, API keys for service-to-service\n",
    "\n",
    "3. **How do you optimize API latency?**\n",
    "   - Caching: Redis for frequent requests\n",
    "   - Batching: Group requests for GPU efficiency\n",
    "   - Async: Non-blocking I/O with asyncio\n",
    "   - CDN: Cache responses at edge locations\n",
    "   - Connection pooling: Reuse DB/HTTP connections\n",
    "\n",
    "4. **What metrics do you track for ML APIs?**\n",
    "   - Throughput: Requests per second (QPS)\n",
    "   - Latency: P50, P95, P99 response times\n",
    "   - Error rate: 4xx and 5xx percentages\n",
    "   - Model metrics: Inference time, cache hit rate\n",
    "   - Business: API usage per tier, costs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
