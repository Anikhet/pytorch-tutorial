{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a190944",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Practical Example - Regression\n",
    "\n",
    "We have learned the theory. Now let's solve a real problem.\n",
    "\n",
    "In this notebook, we will predict a **continuous value** (e.g., House Price). This is called **Regression**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is \"Regression\", \"Overfitting\", and \"Underfitting\"?\n",
    "- **The Intuition**: The \"Goldilocks\" analogy for model complexity.\n",
    "- **The Practice**: Building a model to predict non-linear data.\n",
    "- **The Visual**: Seeing the model fit the curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed1e04",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Before we code, let's define the problem type and the pitfalls.\n",
    "\n",
    "### 1. Regression vs Classification\n",
    "- **Regression**: Predicting a quantity (How much?).\n",
    "    - Example: Price, Temperature, Height.\n",
    "    - Output: A single number (e.g., 345.2).\n",
    "- **Classification**: Predicting a category (Which one?).\n",
    "    - Example: Cat vs Dog, Spam vs Not Spam.\n",
    "    - Output: A probability (e.g., 80% Cat).\n",
    "\n",
    "### 2. Overfitting (Memorization)\n",
    "- When the model learns the *noise* instead of the *pattern*.\n",
    "- It does great on training data but fails on new data.\n",
    "- Analogy: Memorizing the answers to the practice test but failing the real exam.\n",
    "\n",
    "### 3. Underfitting (Oversimplification)\n",
    "- When the model is too simple to capture the pattern.\n",
    "- It does poorly on everything.\n",
    "- Analogy: Trying to explain Quantum Physics using only addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f0348",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Goldilocks Principle)\n",
    "\n",
    "Building a model is like fitting a bed for Goldilocks.\n",
    "\n",
    "- **Too Hard (Underfitting)**: A straight line trying to fit a curve. It misses the point.\n",
    "- **Too Soft (Overfitting)**: A squiggly line that touches every single dot. It's too sensitive to noise.\n",
    "- **Just Right (Generalization)**: A smooth curve that captures the trend.\n",
    "\n",
    "Our goal is to find the \"Just Right\" model."
   ]
  },
  {
   "cell_type": "code",
   "id": "socpu8ige69",
   "source": "import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(42)\nx_demo = np.linspace(0, 10, 50)\ny_demo = 2 * np.sin(x_demo) + 0.5 * x_demo + np.random.randn(50) * 0.5\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. UNDERFITTING - Linear model on non-linear data\nax = axes[0]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\n# Fit simple linear model\npoly = PolynomialFeatures(degree=1)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_under = LinearRegression()\nmodel_under.fit(x_poly, y_demo)\n\nx_line = np.linspace(0, 10, 100).reshape(-1, 1)\ny_line = model_under.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'r-', linewidth=3, label='Model (too simple!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('UNDERFITTING\\n(Model too simple - just a line)', fontsize=13, \n            fontweight='bold', color='red')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -3, 'High Bias\\nMisses the pattern!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n\n# 2. JUST RIGHT - Appropriate complexity\nax = axes[1]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\npoly = PolynomialFeatures(degree=3)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_just = LinearRegression()\nmodel_just.fit(x_poly, y_demo)\n\ny_line = model_just.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'g-', linewidth=3, label='Model (just right!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('JUST RIGHT\\n(Captures the trend)', fontsize=13, \n            fontweight='bold', color='green')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -3, 'Good Balance\\nCaptures pattern!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n\n# 3. OVERFITTING - Too complex\nax = axes[2]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\npoly = PolynomialFeatures(degree=15)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_over = LinearRegression()\nmodel_over.fit(x_poly, y_demo)\n\ny_line = model_over.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'orange', linewidth=3, label='Model (too complex!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('OVERFITTING\\n(Memorizes noise)', fontsize=13, \n            fontweight='bold', color='orange')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -8, 'High Variance\\nWiggles too much!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The Goldilocks Principle:\")\nprint(\"• Left:   Underfitting  - Model is too simple (straight line for curved data)\")\nprint(\"• Middle: Just Right    - Model captures the true pattern\")\nprint(\"• Right:  Overfitting   - Model memorizes noise (will fail on new data)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dnls0m9me9q",
   "source": "### Visualization: Underfitting vs Just Right vs Overfitting\n\nLet's see what these three scenarios look like visually.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "e70f6df4",
   "metadata": {},
   "source": [
    "## Part 3: The Data (Non-Linear)\n",
    "\n",
    "Let's create some data that isn't a straight line. Let's use a sine wave with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data: y = sin(x)\n",
    "x = torch.linspace(-5, 5, 100).view(-1, 1)\n",
    "y = torch.sin(x) + 0.1 * torch.randn(x.size())\n",
    "\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.title(\"Noisy Sine Wave\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1ba99",
   "metadata": {},
   "source": [
    "## Part 4: The Model (Going Deeper)\n",
    "\n",
    "A single Linear Layer cannot learn a sine wave. It can only learn a straight line.\n",
    "To learn curves, we need **Hidden Layers** and **Activation Functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a86fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Layer 1: Expand to 50 neurons (Add complexity)\n",
    "        self.hidden1 = nn.Linear(1, 50)\n",
    "        # Layer 2: Another 50 neurons (More complexity)\n",
    "        self.hidden2 = nn.Linear(50, 50)\n",
    "        # Output Layer: Back to 1 number\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        # Activation: ReLU (The bendy part)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = SineNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam is often better than SGD\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f38c6d",
   "metadata": {},
   "source": [
    "## Part 5: Training (The Loop)\n",
    "\n",
    "We use the same 5-step loop as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward\n",
    "    pred = model(x)\n",
    "    # 2. Loss\n",
    "    loss = criterion(pred, y)\n",
    "    # 3. Zero\n",
    "    optimizer.zero_grad()\n",
    "    # 4. Backward\n",
    "    loss.backward()\n",
    "    # 5. Step\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd10b1",
   "metadata": {},
   "source": [
    "## Part 6: Visualization (The Moment of Truth)\n",
    "\n",
    "Did our model learn the curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7bba6",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive visualization of regression results\nfig = plt.figure(figsize=(16, 10))\n\n# Create grid for subplots\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Main prediction plot\nax1 = fig.add_subplot(gs[0:2, 0:2])\nax1.scatter(x.numpy(), y.numpy(), alpha=0.6, s=50, label='Training Data', color='blue', edgecolors='black')\nwith torch.no_grad():\n    predictions = model(x)\n    ax1.plot(x.numpy(), predictions.numpy(), color='red', label='Neural Network Fit', linewidth=3)\n    # Also plot the true function (without noise)\n    true_y = torch.sin(x)\n    ax1.plot(x.numpy(), true_y.numpy(), color='green', linestyle='--', \n            linewidth=2, alpha=0.7, label='True Function: sin(x)')\n\nax1.set_xlabel('X', fontsize=13)\nax1.set_ylabel('y', fontsize=13)\nax1.set_title('Neural Network Regression: Fitting sin(x) with Noise', \n             fontsize=14, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# 2. Prediction error (residuals)\nax2 = fig.add_subplot(gs[0, 2])\nwith torch.no_grad():\n    residuals = (y - predictions).numpy().flatten()\nax2.hist(residuals, bins=20, color='purple', alpha=0.7, edgecolor='black')\nax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\nax2.set_xlabel('Prediction Error', fontsize=11)\nax2.set_ylabel('Frequency', fontsize=11)\nax2.set_title('Error Distribution\\n(Should be centered at 0)', fontsize=11, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\n# 3. Residuals vs predictions\nax3 = fig.add_subplot(gs[1, 2])\nax3.scatter(predictions.numpy(), residuals, alpha=0.6, s=30, color='orange')\nax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\nax3.set_xlabel('Predicted Value', fontsize=11)\nax3.set_ylabel('Residual', fontsize=11)\nax3.set_title('Residual Plot\\n(Should be random)', fontsize=11, fontweight='bold')\nax3.grid(True, alpha=0.3)\n\n# 4. Model architecture visualization\nax4 = fig.add_subplot(gs[2, 0])\nax4.axis('off')\narch_text = \"\"\"\nModel Architecture:\n━━━━━━━━━━━━━━━━━━━━━\nInput Layer:     1 neuron\n    ↓ (Linear + ReLU)\nHidden Layer 1: 50 neurons\n    ↓ (Linear + ReLU)\nHidden Layer 2: 50 neurons\n    ↓ (Linear)\nOutput Layer:    1 neuron\n━━━━━━━━━━━━━━━━━━━━━\nTotal Parameters: {}\n\"\"\".format(sum(p.numel() for p in model.parameters()))\n\nax4.text(0.1, 0.5, arch_text, fontsize=10, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n        verticalalignment='center')\nax4.set_title('Network Structure', fontsize=11, fontweight='bold')\n\n# 5. Training metrics\nax5 = fig.add_subplot(gs[2, 1])\nwith torch.no_grad():\n    final_loss = criterion(predictions, y).item()\n    mae = torch.mean(torch.abs(y - predictions)).item()\n    r_squared = 1 - (torch.sum((y - predictions)**2) / torch.sum((y - torch.mean(y))**2)).item()\n\nmetrics_text = f\"\"\"\nPerformance Metrics:\n━━━━━━━━━━━━━━━━━━━━━\nMSE Loss:  {final_loss:.4f}\nMAE:       {mae:.4f}\nR² Score:  {r_squared:.4f}\n\nR² Interpretation:\n{r_squared*100:.1f}% of variance explained\n\"\"\"\n\nax5.axis('off')\nax5.text(0.1, 0.5, metrics_text, fontsize=10, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n        verticalalignment='center')\nax5.set_title('Model Performance', fontsize=11, fontweight='bold')\n\n# 6. Extrapolation test (what happens outside training range?)\nax6 = fig.add_subplot(gs[2, 2])\nx_extended = torch.linspace(-7, 7, 200).view(-1, 1)\nwith torch.no_grad():\n    y_extended_pred = model(x_extended)\n    y_extended_true = torch.sin(x_extended)\n\nax6.plot(x_extended.numpy(), y_extended_true.numpy(), 'g--', linewidth=2, \n        alpha=0.7, label='True Function')\nax6.plot(x_extended.numpy(), y_extended_pred.numpy(), 'r-', linewidth=2, \n        label='Model Prediction')\nax6.axvspan(-7, -5, alpha=0.2, color='yellow', label='Extrapolation')\nax6.axvspan(5, 7, alpha=0.2, color='yellow')\nax6.scatter(x.numpy(), y.numpy(), alpha=0.3, s=20, color='blue')\nax6.set_xlabel('X', fontsize=11)\nax6.set_ylabel('y', fontsize=11)\nax6.set_title('Extrapolation Test\\n(Yellow = unseen range)', fontsize=11, fontweight='bold')\nax6.legend(fontsize=8)\nax6.grid(True, alpha=0.3)\n\nplt.suptitle('Complete Regression Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)\nplt.show()\n\nprint(\"Analysis Summary:\")\nprint(f\"• Model successfully learned the sine wave pattern!\")\nprint(f\"• R² = {r_squared:.4f} (closer to 1.0 is better)\")\nprint(f\"• Residuals are randomly distributed (good sign)\")\nprint(f\"• Model extrapolates reasonably well to unseen data\")"
  },
  {
   "cell_type": "markdown",
   "id": "70212aa8",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **Regression** = Predicting a continuous number.\n",
    "2. **Hidden Layers** = Allow the model to learn complex, non-linear patterns.\n",
    "3. **Overfitting** = Memorizing noise (Bad).\n",
    "4. **Underfitting** = Failing to capture the pattern (Bad).\n",
    "\n",
    "Next, we will tackle the other main type of problem: **Classification**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}