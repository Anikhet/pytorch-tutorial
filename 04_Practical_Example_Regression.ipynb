{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Practical Example - Regression\n",
    "\n",
    "In this notebook, we will solve a real-world regression problem: predicting continuous values (like house prices) based on input features.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to prepare data for regression\n",
    "- Build a flexible neural network for regression tasks\n",
    "- Implement a complete training loop with validation\n",
    "- Visualize training progress and model predictions\n",
    "- Analyze model errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating a Synthetic Dataset\n",
    "\n",
    "To understand regression, let's create a synthetic dataset representing house prices.\n",
    "\n",
    "**Features:**\n",
    "1. **Size**: Square footage (500 - 2500 sq ft)\n",
    "2. **Bedrooms**: Number of bedrooms (1 - 5)\n",
    "3. **Age**: Age of the house (0 - 30 years)\n",
    "\n",
    "**Target:**\n",
    "- **Price**: Determined by a formula + some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate random features\n",
    "# Size: Random values between 500 and 2500\n",
    "size = torch.rand(n_samples, 1) * 2000 + 500\n",
    "\n",
    "# Bedrooms: Random integers between 1 and 5\n",
    "bedrooms = torch.randint(1, 6, (n_samples, 1)).float()\n",
    "\n",
    "# Age: Random values between 0 and 30\n",
    "age = torch.rand(n_samples, 1) * 30\n",
    "\n",
    "# Create the feature matrix X (concatenate columns)\n",
    "X = torch.cat([size, bedrooms, age], dim=1)\n",
    "\n",
    "# Define the \"true\" relationship (Price formula)\n",
    "# Price = 100 * Size + 50000 * Bedrooms - 2000 * Age + Noise\n",
    "true_weights = torch.tensor([100.0, 50000.0, -2000.0])\n",
    "bias = 50000.0  # Base price\n",
    "\n",
    "# Calculate target y\n",
    "noise = torch.randn(n_samples, 1) * 20000  # Add some random noise\n",
    "y = (X @ true_weights.unsqueeze(1)) + bias + noise\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(\"\\nSample data (first 3 rows):\")\n",
    "print(\"Size | Bedrooms | Age | Price\")\n",
    "for i in range(3):\n",
    "    print(f\"{X[i,0]:.0f} | {X[i,1]:.0f} | {X[i,2]:.1f} | ${y[i,0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "**Crucial Step:** Neural networks train much better when input features are on a similar scale (e.g., between 0 and 1, or mean 0 and std 1).\n",
    "\n",
    "- **Size**: ~2000\n",
    "- **Bedrooms**: ~3\n",
    "\n",
    "If we don't normalize, the \"Size\" feature will dominate the gradients because its values are huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets (80% train, 20% val)\n",
    "train_split = int(0.8 * n_samples)\n",
    "\n",
    "X_train_raw = X[:train_split]\n",
    "y_train_raw = y[:train_split]\n",
    "X_val_raw = X[train_split:]\n",
    "y_val_raw = y[train_split:]\n",
    "\n",
    "# Compute Mean and Std from TRAINING data only (to avoid data leakage)\n",
    "X_mean = X_train_raw.mean(dim=0)\n",
    "X_std = X_train_raw.std(dim=0)\n",
    "\n",
    "y_mean = y_train_raw.mean()\n",
    "y_std = y_train_raw.std()\n",
    "\n",
    "# Normalize function\n",
    "def normalize(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "def denormalize(data, mean, std):\n",
    "    return data * std + mean\n",
    "\n",
    "# Normalize inputs and targets\n",
    "X_train = normalize(X_train_raw, X_mean, X_std)\n",
    "X_val = normalize(X_val_raw, X_mean, X_std)\n",
    "\n",
    "y_train = normalize(y_train_raw, y_mean, y_std)\n",
    "y_val = normalize(y_val_raw, y_mean, y_std)\n",
    "\n",
    "print(\"Data normalized!\")\n",
    "print(f\"X_train mean: {X_train.mean(dim=0)} (should be close to 0)\")\n",
    "print(f\"X_train std: {X_train.std(dim=0)} (should be close to 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePriceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # A simple feed-forward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),  # Input layer -> Hidden layer 1\n",
    "            nn.ReLU(),                  # Activation\n",
    "            nn.Linear(64, 32),          # Hidden layer 1 -> Hidden layer 2\n",
    "            nn.ReLU(),                  # Activation\n",
    "            nn.Linear(32, 1)            # Hidden layer 2 -> Output (1 value)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = HousePriceModel(input_size=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: MSE (Mean Squared Error) is standard for regression\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam is usually better than SGD for this kind of task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    # 1. Forward pass\n",
    "    predictions = model(X_train)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = criterion(predictions, y_train)\n",
    "    \n",
    "    # 3. Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val)\n",
    "        val_loss = criterion(val_predictions, y_val)\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss (Normalized)\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions vs Actuals\n",
    "\n",
    "Let's see how well our model predicts prices on the validation set. We need to **denormalize** the predictions to get actual dollar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions on normalized validation data\n",
    "    pred_normalized = model(X_val)\n",
    "    \n",
    "    # Convert back to real dollars\n",
    "    pred_actual = denormalize(pred_normalized, y_mean, y_std)\n",
    "    y_val_actual = denormalize(y_val, y_mean, y_std)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val_actual.numpy(), pred_actual.numpy(), alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_val_actual.min(), y_val_actual.max()], [y_val_actual.min(), y_val_actual.max()], \"r--\", lw=2)\n",
    "plt.xlabel(\"Actual Price ($)\")\n",
    "plt.ylabel(\"Predicted Price ($)\")\n",
    "plt.title(\"Actual vs Predicted Prices\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Checking Individual Examples\n",
    "\n",
    "Let's pick a few houses from the validation set and see the specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<15} {:<15} {:<15}\".format(\"Actual\", \"Predicted\", \"Difference\"))\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(10):\n",
    "    actual = y_val_actual[i].item()\n",
    "    predicted = pred_actual[i].item()\n",
    "    diff = abs(actual - predicted)\n",
    "    print(f\"${actual:,.0f:<14} ${predicted:,.0f:<14} ${diff:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Normalization**: Always normalize input features when they have different scales (e.g., square footage vs number of bedrooms).\n",
    "2. **Regression Output**: The output layer usually has 1 neuron and **no activation function** (or linear activation) because we want to predict any continuous value.\n",
    "3. **Loss**: MSE (Mean Squared Error) is the standard loss for regression.\n",
    "4. **Evaluation**: Visualizing \"Predicted vs Actual\" is a great way to check regression performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}