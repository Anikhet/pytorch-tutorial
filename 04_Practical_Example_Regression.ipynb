{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a190944",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Practical Example - Regression\n",
    "\n",
    "We have learned the theory. Now let's solve a real problem.\n",
    "\n",
    "In this notebook, we will predict a **continuous value** (e.g., House Price). This is called **Regression**.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is \"Regression\", \"Overfitting\", and \"Underfitting\"?\n",
    "- **The Intuition**: The \"Goldilocks\" analogy for model complexity.\n",
    "- **The Practice**: Building a model to predict non-linear data.\n",
    "- **The Visual**: Seeing the model fit the curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed1e04",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "Before we code, let's define the problem type and the pitfalls.\n",
    "\n",
    "### 1. Regression vs Classification\n",
    "- **Regression**: Predicting a quantity (How much?).\n",
    "    - Example: Price, Temperature, Height.\n",
    "    - Output: A single number (e.g., 345.2).\n",
    "- **Classification**: Predicting a category (Which one?).\n",
    "    - Example: Cat vs Dog, Spam vs Not Spam.\n",
    "    - Output: A probability (e.g., 80% Cat).\n",
    "\n",
    "### 2. Overfitting (Memorization)\n",
    "- When the model learns the *noise* instead of the *pattern*.\n",
    "- It does great on training data but fails on new data.\n",
    "- Analogy: Memorizing the answers to the practice test but failing the real exam.\n",
    "\n",
    "### 3. Underfitting (Oversimplification)\n",
    "- When the model is too simple to capture the pattern.\n",
    "- It does poorly on everything.\n",
    "- Analogy: Trying to explain Quantum Physics using only addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f0348",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Goldilocks Principle)\n",
    "\n",
    "Building a model is like fitting a bed for Goldilocks.\n",
    "\n",
    "- **Too Hard (Underfitting)**: A straight line trying to fit a curve. It misses the point.\n",
    "- **Too Soft (Overfitting)**: A squiggly line that touches every single dot. It's too sensitive to noise.\n",
    "- **Just Right (Generalization)**: A smooth curve that captures the trend.\n",
    "\n",
    "Our goal is to find the \"Just Right\" model."
   ]
  },
  {
   "cell_type": "code",
   "id": "socpu8ige69",
   "source": "import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(42)\nx_demo = np.linspace(0, 10, 50)\ny_demo = 2 * np.sin(x_demo) + 0.5 * x_demo + np.random.randn(50) * 0.5\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. UNDERFITTING - Linear model on non-linear data\nax = axes[0]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\n# Fit simple linear model\npoly = PolynomialFeatures(degree=1)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_under = LinearRegression()\nmodel_under.fit(x_poly, y_demo)\n\nx_line = np.linspace(0, 10, 100).reshape(-1, 1)\ny_line = model_under.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'r-', linewidth=3, label='Model (too simple!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('UNDERFITTING\\n(Model too simple - just a line)', fontsize=13, \n            fontweight='bold', color='red')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -3, 'High Bias\\nMisses the pattern!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n\n# 2. JUST RIGHT - Appropriate complexity\nax = axes[1]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\npoly = PolynomialFeatures(degree=3)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_just = LinearRegression()\nmodel_just.fit(x_poly, y_demo)\n\ny_line = model_just.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'g-', linewidth=3, label='Model (just right!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('JUST RIGHT\\n(Captures the trend)', fontsize=13, \n            fontweight='bold', color='green')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -3, 'Good Balance\\nCaptures pattern!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n\n# 3. OVERFITTING - Too complex\nax = axes[2]\nax.scatter(x_demo, y_demo, alpha=0.6, s=50, label='Data', color='blue')\n\npoly = PolynomialFeatures(degree=15)\nx_poly = poly.fit_transform(x_demo.reshape(-1, 1))\nmodel_over = LinearRegression()\nmodel_over.fit(x_poly, y_demo)\n\ny_line = model_over.predict(poly.transform(x_line))\nax.plot(x_line, y_line, 'orange', linewidth=3, label='Model (too complex!)')\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('OVERFITTING\\n(Memorizes noise)', fontsize=13, \n            fontweight='bold', color='orange')\nax.legend()\nax.grid(True, alpha=0.3)\nax.text(5, -8, 'High Variance\\nWiggles too much!', ha='center', fontsize=11,\n       bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The Goldilocks Principle:\")\nprint(\"‚Ä¢ Left:   Underfitting  - Model is too simple (straight line for curved data)\")\nprint(\"‚Ä¢ Middle: Just Right    - Model captures the true pattern\")\nprint(\"‚Ä¢ Right:  Overfitting   - Model memorizes noise (will fail on new data)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dnls0m9me9q",
   "source": "### Visualization: Underfitting vs Just Right vs Overfitting\n\nLet's see what these three scenarios look like visually.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "e70f6df4",
   "metadata": {},
   "source": [
    "## Part 3: The Data (Non-Linear)\n",
    "\n",
    "Let's create some data that isn't a straight line. Let's use a sine wave with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data: y = sin(x)\n",
    "x = torch.linspace(-5, 5, 100).view(-1, 1)\n",
    "y = torch.sin(x) + 0.1 * torch.randn(x.size())\n",
    "\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.title(\"Noisy Sine Wave\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1ba99",
   "metadata": {},
   "source": [
    "## Part 4: The Model (Going Deeper)\n",
    "\n",
    "A single Linear Layer cannot learn a sine wave. It can only learn a straight line.\n",
    "To learn curves, we need **Hidden Layers** and **Activation Functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a86fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Layer 1: Expand to 50 neurons (Add complexity)\n",
    "        self.hidden1 = nn.Linear(1, 50)\n",
    "        # Layer 2: Another 50 neurons (More complexity)\n",
    "        self.hidden2 = nn.Linear(50, 50)\n",
    "        # Output Layer: Back to 1 number\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        # Activation: ReLU (The bendy part)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = SineNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam is often better than SGD\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f38c6d",
   "metadata": {},
   "source": [
    "## Part 5: Training (The Loop)\n",
    "\n",
    "We use the same 5-step loop as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward\n",
    "    pred = model(x)\n",
    "    # 2. Loss\n",
    "    loss = criterion(pred, y)\n",
    "    # 3. Zero\n",
    "    optimizer.zero_grad()\n",
    "    # 4. Backward\n",
    "    loss.backward()\n",
    "    # 5. Step\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd10b1",
   "metadata": {},
   "source": [
    "## Part 6: Visualization (The Moment of Truth)\n",
    "\n",
    "Did our model learn the curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7bba6",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive visualization of regression results\nfig = plt.figure(figsize=(16, 10))\n\n# Create grid for subplots\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Main prediction plot\nax1 = fig.add_subplot(gs[0:2, 0:2])\nax1.scatter(x.numpy(), y.numpy(), alpha=0.6, s=50, label='Training Data', color='blue', edgecolors='black')\nwith torch.no_grad():\n    predictions = model(x)\n    ax1.plot(x.numpy(), predictions.numpy(), color='red', label='Neural Network Fit', linewidth=3)\n    # Also plot the true function (without noise)\n    true_y = torch.sin(x)\n    ax1.plot(x.numpy(), true_y.numpy(), color='green', linestyle='--', \n            linewidth=2, alpha=0.7, label='True Function: sin(x)')\n\nax1.set_xlabel('X', fontsize=13)\nax1.set_ylabel('y', fontsize=13)\nax1.set_title('Neural Network Regression: Fitting sin(x) with Noise', \n             fontsize=14, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# 2. Prediction error (residuals)\nax2 = fig.add_subplot(gs[0, 2])\nwith torch.no_grad():\n    residuals = (y - predictions).numpy().flatten()\nax2.hist(residuals, bins=20, color='purple', alpha=0.7, edgecolor='black')\nax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\nax2.set_xlabel('Prediction Error', fontsize=11)\nax2.set_ylabel('Frequency', fontsize=11)\nax2.set_title('Error Distribution\\n(Should be centered at 0)', fontsize=11, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\n# 3. Residuals vs predictions\nax3 = fig.add_subplot(gs[1, 2])\nax3.scatter(predictions.numpy(), residuals, alpha=0.6, s=30, color='orange')\nax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\nax3.set_xlabel('Predicted Value', fontsize=11)\nax3.set_ylabel('Residual', fontsize=11)\nax3.set_title('Residual Plot\\n(Should be random)', fontsize=11, fontweight='bold')\nax3.grid(True, alpha=0.3)\n\n# 4. Model architecture visualization\nax4 = fig.add_subplot(gs[2, 0])\nax4.axis('off')\narch_text = \"\"\"\nModel Architecture:\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nInput Layer:     1 neuron\n    ‚Üì (Linear + ReLU)\nHidden Layer 1: 50 neurons\n    ‚Üì (Linear + ReLU)\nHidden Layer 2: 50 neurons\n    ‚Üì (Linear)\nOutput Layer:    1 neuron\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nTotal Parameters: {}\n\"\"\".format(sum(p.numel() for p in model.parameters()))\n\nax4.text(0.1, 0.5, arch_text, fontsize=10, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n        verticalalignment='center')\nax4.set_title('Network Structure', fontsize=11, fontweight='bold')\n\n# 5. Training metrics\nax5 = fig.add_subplot(gs[2, 1])\nwith torch.no_grad():\n    final_loss = criterion(predictions, y).item()\n    mae = torch.mean(torch.abs(y - predictions)).item()\n    r_squared = 1 - (torch.sum((y - predictions)**2) / torch.sum((y - torch.mean(y))**2)).item()\n\nmetrics_text = f\"\"\"\nPerformance Metrics:\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nMSE Loss:  {final_loss:.4f}\nMAE:       {mae:.4f}\nR¬≤ Score:  {r_squared:.4f}\n\nR¬≤ Interpretation:\n{r_squared*100:.1f}% of variance explained\n\"\"\"\n\nax5.axis('off')\nax5.text(0.1, 0.5, metrics_text, fontsize=10, family='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n        verticalalignment='center')\nax5.set_title('Model Performance', fontsize=11, fontweight='bold')\n\n# 6. Extrapolation test (what happens outside training range?)\nax6 = fig.add_subplot(gs[2, 2])\nx_extended = torch.linspace(-7, 7, 200).view(-1, 1)\nwith torch.no_grad():\n    y_extended_pred = model(x_extended)\n    y_extended_true = torch.sin(x_extended)\n\nax6.plot(x_extended.numpy(), y_extended_true.numpy(), 'g--', linewidth=2, \n        alpha=0.7, label='True Function')\nax6.plot(x_extended.numpy(), y_extended_pred.numpy(), 'r-', linewidth=2, \n        label='Model Prediction')\nax6.axvspan(-7, -5, alpha=0.2, color='yellow', label='Extrapolation')\nax6.axvspan(5, 7, alpha=0.2, color='yellow')\nax6.scatter(x.numpy(), y.numpy(), alpha=0.3, s=20, color='blue')\nax6.set_xlabel('X', fontsize=11)\nax6.set_ylabel('y', fontsize=11)\nax6.set_title('Extrapolation Test\\n(Yellow = unseen range)', fontsize=11, fontweight='bold')\nax6.legend(fontsize=8)\nax6.grid(True, alpha=0.3)\n\nplt.suptitle('Complete Regression Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)\nplt.show()\n\nprint(\"Analysis Summary:\")\nprint(f\"‚Ä¢ Model successfully learned the sine wave pattern!\")\nprint(f\"‚Ä¢ R¬≤ = {r_squared:.4f} (closer to 1.0 is better)\")\nprint(f\"‚Ä¢ Residuals are randomly distributed (good sign)\")\nprint(f\"‚Ä¢ Model extrapolates reasonably well to unseen data\")"
  },
  {
   "cell_type": "markdown",
   "id": "70212aa8",
   "metadata": {},
   "source": "## Part 7: Regression Loss Functions (FAANG Interview Essentials)\n\nDifferent loss functions for different problems. Know when to use each."
  },
  {
   "cell_type": "code",
   "id": "5vzdt7dfs1d",
   "source": "# Regression Loss Functions Comparison\n\n# Sample predictions and targets\ny_true = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\ny_pred = torch.tensor([1.2, 2.5, 2.8, 4.2, 4.8])\n\n# MSE - Mean Squared Error\nmse_loss = nn.MSELoss()\nmse = mse_loss(y_pred, y_true)\nprint(f\"MSE:  {mse.item():.4f}\")\nprint(f\"      Formula: mean((pred - true)^2)\")\nprint(f\"      Use: Standard regression, penalizes large errors heavily\")\n\n# MAE / L1 Loss - Mean Absolute Error\nmae_loss = nn.L1Loss()\nmae = mae_loss(y_pred, y_true)\nprint(f\"\\nMAE:  {mae.item():.4f}\")\nprint(f\"      Formula: mean(|pred - true|)\")\nprint(f\"      Use: When outliers are common, more robust\")\n\n# Huber Loss (Smooth L1) - Best of both worlds\nhuber_loss = nn.SmoothL1Loss()\nhuber = huber_loss(y_pred, y_true)\nprint(f\"\\nHuber: {huber.item():.4f}\")\nprint(f\"      Formula: L2 for small errors, L1 for large errors\")\nprint(f\"      Use: Object detection (bounding box regression)\")\n\n# Visualize loss functions\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Loss curves\nerrors = torch.linspace(-3, 3, 100)\nmse_curve = errors ** 2\nmae_curve = torch.abs(errors)\n\n# Huber loss (delta=1)\ndelta = 1.0\nhuber_curve = torch.where(\n    torch.abs(errors) < delta,\n    0.5 * errors ** 2,\n    delta * (torch.abs(errors) - 0.5 * delta)\n)\n\nax1.plot(errors.numpy(), mse_curve.numpy(), label='MSE (L2)', linewidth=2)\nax1.plot(errors.numpy(), mae_curve.numpy(), label='MAE (L1)', linewidth=2)\nax1.plot(errors.numpy(), huber_curve.numpy(), label='Huber', linewidth=2)\nax1.set_xlabel('Error (pred - true)', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Regression Loss Functions', fontsize=13, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-3, 3)\nax1.set_ylim(0, 5)\n\n# Right: Comparison table\nax2.axis('off')\ntable_data = [\n    ['Loss', 'Gradient', 'Outlier\\nSensitivity', 'Best For'],\n    ['MSE (L2)', '2√óerror', 'High', 'Most regression'],\n    ['MAE (L1)', 'sign(error)', 'Low', 'Noisy data'],\n    ['Huber', 'Mixed', 'Medium', 'Object detection'],\n]\n\ntable = ax2.table(cellText=table_data, loc='center', cellLoc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(11)\ntable.scale(1.3, 2.0)\n\nfor j in range(4):\n    table[(0, j)].set_facecolor('#4472C4')\n    table[(0, j)].set_text_props(color='white', fontweight='bold')\n\nax2.set_title('Loss Function Comparison', fontsize=13, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüîë FAANG Tip: MSE is default, use Huber for object detection, MAE for noisy data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ryxmxj7ksm",
   "source": "### Regularization Techniques - Fighting Overfitting\n\nRegularization = adding constraints to prevent overfitting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "suopps4sni",
   "source": "# Regularization Techniques Demo\n\n# 1. L2 Regularization (Weight Decay)\n# Built into optimizer - adds penalty for large weights\noptimizer_l2 = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\nprint(\"1. L2 Regularization (Weight Decay):\")\nprint(\"   - Adds Œª * sum(weights¬≤) to loss\")\nprint(\"   - Keeps weights small\")\nprint(\"   - Standard in AdamW\")\n\n# 2. L1 Regularization (Lasso)\n# Must add manually to loss\ndef l1_loss(model, lambda_l1=0.01):\n    l1_penalty = sum(p.abs().sum() for p in model.parameters())\n    return lambda_l1 * l1_penalty\n\nprint(\"\\n2. L1 Regularization (Lasso):\")\nprint(\"   - Adds Œª * sum(|weights|) to loss\")\nprint(\"   - Pushes weights to exactly zero (sparsity)\")\nprint(\"   - Good for feature selection\")\n\n# 3. Dropout\nclass RegularizedNet(nn.Module):\n    def __init__(self, dropout_rate=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(1, 50)\n        self.dropout = nn.Dropout(dropout_rate)  # Randomly zero 50% of neurons\n        self.fc2 = nn.Linear(50, 50)\n        self.fc3 = nn.Linear(50, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)  # Only active in training mode!\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nprint(\"\\n3. Dropout:\")\nprint(\"   - Randomly zeros neurons during training\")\nprint(\"   - Forces network to not rely on specific neurons\")\nprint(\"   - Automatically disabled in eval() mode\")\n\n# 4. Early Stopping (we covered this before)\nprint(\"\\n4. Early Stopping:\")\nprint(\"   - Stop training when validation loss stops improving\")\nprint(\"   - Simplest and most effective regularization\")\n\n# Visualize regularization effect\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Demo: Train with and without regularization on noisy data\ntorch.manual_seed(42)\nx_reg = torch.linspace(-3, 3, 30).view(-1, 1)\ny_reg = torch.sin(x_reg) + 0.2 * torch.randn(x_reg.size())\n\nx_test = torch.linspace(-4, 4, 100).view(-1, 1)\ny_test_true = torch.sin(x_test)\n\nfor ax, (name, weight_decay) in zip(axes, [('No Regularization', 0), ('Light L2', 0.001), ('Strong L2', 0.1)]):\n    # Train model\n    model_reg = nn.Sequential(\n        nn.Linear(1, 100),\n        nn.ReLU(),\n        nn.Linear(100, 100),\n        nn.ReLU(),\n        nn.Linear(100, 1)\n    )\n    opt = optim.Adam(model_reg.parameters(), lr=0.01, weight_decay=weight_decay)\n    \n    for _ in range(500):\n        pred = model_reg(x_reg)\n        loss = nn.MSELoss()(pred, y_reg)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    \n    # Plot\n    with torch.no_grad():\n        y_pred = model_reg(x_test)\n    \n    ax.scatter(x_reg.numpy(), y_reg.numpy(), alpha=0.7, s=50, label='Data')\n    ax.plot(x_test.numpy(), y_test_true.numpy(), 'g--', linewidth=2, label='True')\n    ax.plot(x_test.numpy(), y_pred.numpy(), 'r-', linewidth=2, label='Model')\n    ax.set_title(f'{name}\\n(weight_decay={weight_decay})', fontsize=11, fontweight='bold')\n    ax.legend(fontsize=8)\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim(-2, 2)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüîë FAANG Tip: Start with weight_decay=0.01, adjust based on val loss\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "05rd1f8ed4vw",
   "source": "## Part 8: FAANG Interview Questions - Regression\n\n### Question 1: \"Explain the Bias-Variance Tradeoff\"\n\n**Answer**:\n\n**Bias**: Error from oversimplified model (underfitting)\n- High bias = Model too simple, misses patterns\n- Example: Using linear model for curved data\n\n**Variance**: Error from model being too sensitive to training data (overfitting)\n- High variance = Model memorizes noise\n- Example: Polynomial degree 20 for 10 data points\n\n**Tradeoff**:\n```\nTotal Error = Bias¬≤ + Variance + Irreducible Noise\n```\n\nAs model complexity increases:\n- Bias decreases (fits training data better)\n- Variance increases (fits noise, fails on test data)\n\n**Sweet spot**: Find complexity where bias + variance is minimized.\n\n### Question 2: \"When would you use MAE vs MSE?\"\n\n**Answer**:\n\n| Criteria | Use MSE | Use MAE |\n|----------|---------|---------|\n| Outliers | Rare/cleaned | Common/expected |\n| Large errors | Should be penalized heavily | All errors equal weight |\n| Gradient | Smooth (2√óerror) | Constant (¬±1) |\n| Example | House prices | Sensor readings |\n\n**MSE pros**: Smooth gradient, penalizes outliers\n**MAE pros**: Robust to outliers, interpretable\n\n### Question 3: \"How do you prevent overfitting in neural networks?\"\n\n**Answer** (in order of effectiveness):\n1. **More data**: Best solution, always try first\n2. **Data augmentation**: Creates \"free\" data\n3. **Early stopping**: Stop when val loss increases\n4. **Dropout**: Randomly zero neurons (p=0.1-0.5)\n5. **Weight decay (L2)**: Penalize large weights\n6. **Reduce model size**: Fewer parameters\n7. **Batch normalization**: Has regularization effect\n\n### Question 4: \"What is feature scaling and why is it important?\"\n\n**Answer**:\nFeature scaling normalizes features to similar ranges.\n\n**Common methods**:\n```python\n# StandardScaler: mean=0, std=1\nx_scaled = (x - x.mean()) / x.std()\n\n# MinMaxScaler: [0, 1]\nx_scaled = (x - x.min()) / (x.max() - x.min())\n```\n\n**Why it matters**:\n1. Gradient descent converges faster (spherical loss landscape)\n2. Prevents features with large values from dominating\n3. Required for distance-based algorithms\n4. Helps with numerical stability\n\n### Question 5: \"Explain cross-validation and when to use it\"\n\n**Answer**:\nCross-validation = rotating train/val splits to estimate generalization.\n\n**K-Fold CV**:\n```\nFold 1: [VAL][TRAIN][TRAIN][TRAIN][TRAIN]\nFold 2: [TRAIN][VAL][TRAIN][TRAIN][TRAIN]\nFold 3: [TRAIN][TRAIN][VAL][TRAIN][TRAIN]\n...\nFinal metric = average across all folds\n```\n\n**When to use**:\n- Small datasets (< 10,000 samples)\n- Hyperparameter tuning\n- Model selection\n\n**When NOT to use**:\n- Very large datasets (simple holdout is fine)\n- Time series (use time-based splits)\n- Expensive training (K√ó slower)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1h25llcwaqd",
   "source": "## Summary: Regression Mastery Checklist\n\n### Fundamentals (Know These Cold)\n- [ ] Regression = Predicting continuous values\n- [ ] Classification = Predicting categories\n- [ ] Overfitting = Memorizing noise (high variance)\n- [ ] Underfitting = Missing patterns (high bias)\n\n### Loss Functions\n- [ ] MSE - Default for regression\n- [ ] MAE - Robust to outliers\n- [ ] Huber - Object detection, best of both\n\n### Regularization (Fight Overfitting)\n- [ ] Weight decay (L2) - Keep weights small\n- [ ] Dropout - Random neuron zeroing\n- [ ] Early stopping - Stop when val loss increases\n- [ ] Data augmentation - More diverse training data\n\n### Evaluation Metrics\n- [ ] MSE/RMSE - Most common\n- [ ] MAE - Interpretable in original units\n- [ ] R¬≤ Score - Explained variance (0-1)\n- [ ] Residual plots - Check for patterns\n\n### Best Practices\n- [ ] Always split data: train/val/test\n- [ ] Scale features to similar ranges\n- [ ] Monitor both train and val loss\n- [ ] Use cross-validation for small datasets\n\n---\n**Next**: Notebook 05 - Image Classification (CNNs, Transfer Learning)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}