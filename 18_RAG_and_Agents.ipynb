{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce51090d",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 18: RAG and Agents from Scratch\n",
    "\n",
    "**Author:** [Your Name/Organization]  \n",
    "**Date:** 2025  \n",
    "\n",
    "The modern AI stack has evolved beyond just training models. The frontier is now about **Systems of Intelligence**—combining Large Language Models (LLMs) with external data (**RAG**) and the ability to take action (**Agents**).\n",
    "\n",
    "In this tutorial, we will demystify these buzzwords by building them **from scratch** using PyTorch. We won't use high-level frameworks like LangChain or LlamaIndex here; instead, we will build the raw components to understand exactly how they work.\n",
    "\n",
    "## Learning Objectives\n",
    "1.  **RAG (Retrieval Augmented Generation)**: Build a vector search engine using PyTorch tensors.\n",
    "2.  **Embeddings**: Understand how to map text to semantic vector spaces.\n",
    "3.  **Agents (ReAct)**: Implement a \"Reasoning + Acting\" loop that allows an LLM to use tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625aabe",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "Before we write code, let's define the core concepts.\n",
    "\n",
    "### 1.1 Embeddings\n",
    "An **embedding** is a vector (a list of floating-point numbers) that represents the *meaning* of a piece of text. \n",
    "-   **Key Property**: Text with similar meanings will have vectors that are mathematically close (e.g., high Cosine Similarity).\n",
    "-   **Example**: `vec(\"dog\")` is closer to `vec(\"puppy\")` than to `vec(\"car\")`.\n",
    "\n",
    "### 1.2 RAG (Retrieval Augmented Generation)\n",
    "LLMs are frozen in time (trained on data up to a cutoff date). RAG is a technique to give them fresh or private data.\n",
    "-   **Retrieval**: Find documents relevant to the user's query.\n",
    "-   **Augmented**: Paste those documents into the LLM's prompt.\n",
    "-   **Generation**: The LLM answers the question using the pasted context.\n",
    "\n",
    "### 1.3 Agents\n",
    "An Agent is an LLM wrapper that can **do things**. It runs in a loop:\n",
    "1.  **Thought**: \"The user asked for the weather. I should check the weather tool.\"\n",
    "2.  **Action**: Call `get_weather(\"New York\")`.\n",
    "3.  **Observation**: Receive `\"25°C, Sunny\"`.\n",
    "4.  **Response**: \"It is currently 25°C and sunny in New York.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d5a0a",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies\n",
    "\n",
    "We will use `torch` for vector operations and `sentence-transformers` to generate real embeddings. \n",
    "\n",
    "> **Note**: If you don't have `sentence-transformers` installed, uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cc3333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:29.169391Z",
     "iopub.status.busy": "2025-11-20T05:07:29.169246Z",
     "iopub.status.idle": "2025-11-20T05:07:33.856127Z",
     "shell.execute_reply": "2025-11-20T05:07:33.855764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40b502",
   "metadata": {},
   "source": [
    "## 3. Part 1: Building RAG from Scratch\n",
    "\n",
    "We will build a \"Vector Database\" using a simple Python dictionary and PyTorch tensors.\n",
    "\n",
    "### 3.1 The Knowledge Base\n",
    "Let's define a small dataset of private information that an LLM wouldn't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f39cef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.857805Z",
     "iopub.status.busy": "2025-11-20T05:07:33.857688Z",
     "iopub.status.idle": "2025-11-20T05:07:33.859561Z",
     "shell.execute_reply": "2025-11-20T05:07:33.859147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base size: 4 documents\n"
     ]
    }
   ],
   "source": [
    "knowledge_base = [\n",
    "    \"The company 'TechCorp' was founded in 2023 by Alice Johnson.\",\n",
    "    \"TechCorp's flagship product is the 'Quantum Toaster', released in 2024.\",\n",
    "    \"Employees at TechCorp get unlimited coffee but only on Tuesdays.\",\n",
    "    \"The CEO's favorite color is neon pink.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge Base size: {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c810e",
   "metadata": {},
   "source": [
    "### 3.2 Generating Embeddings\n",
    "\n",
    "We'll use a small, efficient model from Hugging Face (`all-MiniLM-L6-v2`) to convert these sentences into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940741ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.860656Z",
     "iopub.status.busy": "2025-11-20T05:07:33.860573Z",
     "iopub.status.idle": "2025-11-20T05:07:33.864971Z",
     "shell.execute_reply": "2025-11-20T05:07:33.864638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers not installed. Using random vectors for demonstration.\n",
      "Database Matrix Shape: torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Load a small pre-trained model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def get_embedding(text):\n",
    "        # Encode text to a numpy array, then convert to PyTorch tensor\n",
    "        vector = model.encode(text)\n",
    "        return torch.tensor(vector, device=device)\n",
    "\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"sentence-transformers not installed. Using random vectors for demonstration.\")\n",
    "    # Fallback for when the library isn't installed\n",
    "    def get_embedding(text):\n",
    "        torch.manual_seed(len(text)) # Deterministic random based on text length\n",
    "        return torch.randn(384, device=device) # 384 is standard dim for MiniLM\n",
    "\n",
    "# Create the Vector Database (List of Tensors)\n",
    "db_vectors = []\n",
    "for doc in knowledge_base:\n",
    "    vec = get_embedding(doc)\n",
    "    db_vectors.append(vec)\n",
    "\n",
    "# Stack them into a single matrix [N_docs, Dim]\n",
    "db_matrix = torch.stack(db_vectors)\n",
    "print(f\"Database Matrix Shape: {db_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c5b2b",
   "metadata": {},
   "source": [
    "### 3.3 The Retrieval Function\n",
    "\n",
    "Now we implement the search mechanism. We use **Cosine Similarity** to find the closest match.\n",
    "\n",
    "$$ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
    "\n",
    "Since our vectors might not be normalized, we'll use `F.cosine_similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d2e38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.866178Z",
     "iopub.status.busy": "2025-11-20T05:07:33.866103Z",
     "iopub.status.idle": "2025-11-20T05:07:33.870123Z",
     "shell.execute_reply": "2025-11-20T05:07:33.869699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the vacation policy for coffee?\n",
      "\n",
      "Match 1 (Score: 0.0167):\n",
      "  > Employees at TechCorp get unlimited coffee but only on Tuesdays.\n",
      "Match 2 (Score: -0.0055):\n",
      "  > The company 'TechCorp' was founded in 2023 by Alice Johnson.\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k=1):\n",
    "    # 1. Embed the query\n",
    "    query_vec = get_embedding(query)\n",
    "    \n",
    "    # 2. Calculate similarity with ALL documents at once (Vectorized operation)\n",
    "    # query_vec: [Dim], db_matrix: [N, Dim]\n",
    "    scores = F.cosine_similarity(query_vec.unsqueeze(0), db_matrix)\n",
    "    \n",
    "    # 3. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(scores, k=k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(top_k_scores, top_k_indices):\n",
    "        results.append({\n",
    "            \"score\": score.item(),\n",
    "            \"text\": knowledge_base[idx.item()]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it!\n",
    "user_query = \"What is the vacation policy for coffee?\"\n",
    "matches = retrieve(user_query, k=2)\n",
    "\n",
    "print(f\"Query: {user_query}\\n\")\n",
    "for i, match in enumerate(matches):\n",
    "    print(f\"Match {i+1} (Score: {match['score']:.4f}):\\n  > {match['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056faa16",
   "metadata": {},
   "source": [
    "### 3.4 The Generation Step (Mocked)\n",
    "\n",
    "In a real system, you would take the `matches[0]['text']` and paste it into a prompt like:\n",
    "\n",
    "```\n",
    "Context: Employees at TechCorp get unlimited coffee but only on Tuesdays.\n",
    "Question: What is the vacation policy for coffee?\n",
    "Answer: \n",
    "```\n",
    "\n",
    "The LLM would then answer based on that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b2cc5",
   "metadata": {},
   "source": [
    "## 4. Part 2: Building an Agent from Scratch\n",
    "\n",
    "Now let's build an **Agent**. We will implement the **ReAct (Reasoning + Acting)** pattern.\n",
    "\n",
    "### 4.1 Defining Tools\n",
    "Tools are just Python functions that the Agent can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8576bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.871239Z",
     "iopub.status.busy": "2025-11-20T05:07:33.871172Z",
     "iopub.status.idle": "2025-11-20T05:07:33.873092Z",
     "shell.execute_reply": "2025-11-20T05:07:33.872720Z"
    }
   },
   "outputs": [],
   "source": [
    "def tool_calculator(expression):\n",
    "    \"\"\"Evaluates a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Error in calculation\"\n",
    "\n",
    "def tool_database_search(query):\n",
    "    \"\"\"Searches the TechCorp knowledge base.\"\"\"\n",
    "    results = retrieve(query, k=1)\n",
    "    if results:\n",
    "        return results[0]['text']\n",
    "    return \"No info found.\"\n",
    "\n",
    "tools = {\n",
    "    \"calculator\": tool_calculator,\n",
    "    \"search\": tool_database_search\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10aed83",
   "metadata": {},
   "source": [
    "### 4.2 The Simulated LLM\n",
    "\n",
    "To make this tutorial runnable without an API key, we will create a `SimulatedLLM`. In production, you would replace this class with a call to GPT-4 or Llama 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b81353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.874290Z",
     "iopub.status.busy": "2025-11-20T05:07:33.874220Z",
     "iopub.status.idle": "2025-11-20T05:07:33.876354Z",
     "shell.execute_reply": "2025-11-20T05:07:33.875956Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimulatedLLM:\n",
    "    def generate(self, prompt):\n",
    "        \"\"\"\n",
    "        This is a HARDCODED logic to simulate an intelligent agent.\n",
    "        It looks at the prompt and decides what to output to demonstrate the ReAct loop.\n",
    "        \"\"\"\n",
    "        prompt = prompt.lower()\n",
    "        \n",
    "        # Scenario 1: User asks about the CEO's favorite color\n",
    "        if \"ceo\" in prompt and \"color\" in prompt and \"observation\" not in prompt:\n",
    "            return \"Thought: The user is asking about the CEO. I should search the database.\\nAction: search[CEO color]\"\n",
    "        \n",
    "        # Scenario 1b: We have the observation (search result)\n",
    "        if \"neon pink\" in prompt and \"final answer\" not in prompt:\n",
    "            return \"Thought: I found the answer in the text. The color is neon pink.\\nFinal Answer: The CEO's favorite color is neon pink.\"\n",
    "\n",
    "        # Scenario 2: Math question\n",
    "        if \"25 * 4\" in prompt and \"observation\" not in prompt:\n",
    "             return \"Thought: This is a math problem. I should use the calculator.\\nAction: calculator[25 * 4]\"\n",
    "        \n",
    "        if \"100\" in prompt and \"final answer\" not in prompt:\n",
    "             return \"Thought: The calculator returned 100.\\nFinal Answer: The result is 100.\"\n",
    "            \n",
    "        return \"Thought: I don't know what to do. \\nFinal Answer: I am confused.\"\n",
    "\n",
    "llm = SimulatedLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d07ae",
   "metadata": {},
   "source": [
    "### 4.3 The ReAct Loop\n",
    "\n",
    "The core of an agent is a `while` loop that:\n",
    "1.  Appends the history to the prompt.\n",
    "2.  Asks the LLM for the next step.\n",
    "3.  If the LLM says **Action**, we run the tool and append the **Observation**.\n",
    "4.  If the LLM says **Final Answer**, we stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a98b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.877678Z",
     "iopub.status.busy": "2025-11-20T05:07:33.877599Z",
     "iopub.status.idle": "2025-11-20T05:07:33.880212Z",
     "shell.execute_reply": "2025-11-20T05:07:33.879841Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.history = \"\"\n",
    "    \n",
    "    def run(self, question):\n",
    "        self.history = f\"Question: {question}\\n\"\n",
    "        print(f\"--- Starting Agent Task: {question} ---\")\n",
    "        \n",
    "        for i in range(5): # Max 5 steps to prevent infinite loops\n",
    "            # 1. Generate LLM response\n",
    "            response = self.llm.generate(self.history)\n",
    "            print(f\"\\n[Step {i+1}] LLM Output:\\n{response}\")\n",
    "            \n",
    "            self.history += response + \"\\n\"\n",
    "            \n",
    "            # 2. Check for Final Answer\n",
    "            if \"Final Answer:\" in response:\n",
    "                return response.split(\"Final Answer:\")[1].strip()\n",
    "            \n",
    "            # 3. Check for Action\n",
    "            # We look for pattern: Action: tool_name[input]\n",
    "            match = re.search(r\"Action: (\\w+)\\[(.*?)\\]\", response)\n",
    "            if match:\n",
    "                tool_name = match.group(1)\n",
    "                tool_input = match.group(2)\n",
    "                \n",
    "                print(f\"   -> Executing Tool: {tool_name} with input: {tool_input}\")\n",
    "                \n",
    "                if tool_name in self.tools:\n",
    "                    result = self.tools[tool_name](tool_input)\n",
    "                    observation = f\"Observation: {result}\"\n",
    "                    print(f\"   -> {observation}\")\n",
    "                    self.history += observation + \"\\n\"\n",
    "                else:\n",
    "                    self.history += f\"Observation: Tool {tool_name} not found.\\n\"\n",
    "        \n",
    "        return \"Timeout: Agent took too many steps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ecbc37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:07:33.881296Z",
     "iopub.status.busy": "2025-11-20T05:07:33.881228Z",
     "iopub.status.idle": "2025-11-20T05:07:33.883493Z",
     "shell.execute_reply": "2025-11-20T05:07:33.883184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Agent Task: What is the CEO's favorite color? ---\n",
      "\n",
      "[Step 1] LLM Output:\n",
      "Thought: The user is asking about the CEO. I should search the database.\n",
      "Action: search[CEO color]\n",
      "   -> Executing Tool: search with input: CEO color\n",
      "   -> Observation: The company 'TechCorp' was founded in 2023 by Alice Johnson.\n",
      "\n",
      "[Step 2] LLM Output:\n",
      "Thought: I don't know what to do. \n",
      "Final Answer: I am confused.\n",
      "\n",
      ">>> FINAL RESULT: I am confused.\n"
     ]
    }
   ],
   "source": [
    "# Run the Agent\n",
    "agent = Agent(llm, tools)\n",
    "\n",
    "final_answer = agent.run(\"What is the CEO's favorite color?\")\n",
    "print(f\"\\n>>> FINAL RESULT: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a357d",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "You have just built the two most important components of modern AI applications:\n",
    "\n",
    "1.  **RAG System**: We converted text to PyTorch tensors and used cosine similarity to find relevant data.\n",
    "2.  **Agentic Loop**: We built a system that can \"reason\" (via the LLM) and \"act\" (via tools) to solve multi-step problems.\n",
    "\n",
    "### Next Steps\n",
    "-   Replace `SimulatedLLM` with a real API call to OpenAI, Anthropic, or a local Llama 3 model.\n",
    "-   Add more tools (e.g., a web search tool).\n",
    "-   Scale the Vector DB using a library like FAISS or ChromaDB for millions of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
