{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Orchestration\n",
    "\n",
    "## Overview\n",
    "Building enterprise-grade ML pipelines with:\n",
    "- **Apache Airflow**: Workflow orchestration and scheduling\n",
    "- **Feature Stores**: Feast, Tecton for feature management\n",
    "- **Data Versioning**: DVC, MLflow for reproducibility\n",
    "- **MLOps Patterns**: CI/CD for ML, model registry, A/B testing\n",
    "\n",
    "## Why Pipeline Orchestration?\n",
    "- **Reproducibility**: Track every step from data to deployment\n",
    "- **Automation**: Schedule retraining, monitoring, rollbacks\n",
    "- **Collaboration**: Teams share features and models\n",
    "- **Compliance**: Audit trails for regulated industries\n",
    "\n",
    "## Interview Focus\n",
    "- DAG design patterns\n",
    "- Feature engineering at scale\n",
    "- Training-serving skew prevention\n",
    "- Model deployment strategies\n",
    "- Pipeline monitoring and alerting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installation\n",
    "# pip install apache-airflow feast dvc mlflow great-expectations prefect\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Apache Airflow DAG\n",
    "\n",
    "### Complete ML Pipeline from Data to Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.models import Variable\n",
    "\n",
    "# Pipeline configuration\n",
    "default_args = {\n",
    "    'owner': 'ml-team',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['alerts@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=2),\n",
    "}\n",
    "\n",
    "# Create DAG\n",
    "dag = DAG(\n",
    "    'ml_training_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='End-to-end ML training pipeline',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    tags=['ml', 'production'],\n",
    ")\n",
    "\n",
    "# Task functions\n",
    "def extract_data(**context):\n",
    "    \"\"\"Extract data from source systems.\"\"\"\n",
    "    logger.info(\"Extracting data from database...\")\n",
    "    \n",
    "    # Simulate data extraction\n",
    "    data = {\n",
    "        'records': 10000,\n",
    "        'features': ['age', 'income', 'tenure', 'usage'],\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Push to XCom for next task\n",
    "    context['task_instance'].xcom_push(key='extraction_info', value=data)\n",
    "    logger.info(f\"Extracted {data['records']} records\")\n",
    "    return data\n",
    "\n",
    "def validate_data(**context):\n",
    "    \"\"\"Validate data quality with Great Expectations.\"\"\"\n",
    "    logger.info(\"Validating data quality...\")\n",
    "    \n",
    "    extraction_info = context['task_instance'].xcom_pull(\n",
    "        task_ids='extract_data',\n",
    "        key='extraction_info'\n",
    "    )\n",
    "    \n",
    "    # Validation checks\n",
    "    validations = {\n",
    "        'null_check': True,\n",
    "        'schema_check': True,\n",
    "        'distribution_check': True,\n",
    "        'outlier_check': True\n",
    "    }\n",
    "    \n",
    "    if not all(validations.values()):\n",
    "        raise ValueError(\"Data validation failed\")\n",
    "    \n",
    "    logger.info(\"Data validation passed\")\n",
    "    return validations\n",
    "\n",
    "def feature_engineering(**context):\n",
    "    \"\"\"Create features and store in feature store.\"\"\"\n",
    "    logger.info(\"Engineering features...\")\n",
    "    \n",
    "    # Create features\n",
    "    features = {\n",
    "        'user_engagement_7d': 'avg(events) over 7 days',\n",
    "        'user_engagement_30d': 'avg(events) over 30 days',\n",
    "        'ltv_prediction': 'predicted lifetime value',\n",
    "        'churn_risk': 'churn probability score'\n",
    "    }\n",
    "    \n",
    "    # Push to feature store (simulated)\n",
    "    context['task_instance'].xcom_push(key='features', value=features)\n",
    "    logger.info(f\"Created {len(features)} features\")\n",
    "    return features\n",
    "\n",
    "def train_model(**context):\n",
    "    \"\"\"Train ML model with hyperparameter tuning.\"\"\"\n",
    "    logger.info(\"Training model...\")\n",
    "    \n",
    "    features = context['task_instance'].xcom_pull(\n",
    "        task_ids='feature_engineering',\n",
    "        key='features'\n",
    "    )\n",
    "    \n",
    "    # Simulate training\n",
    "    model_metrics = {\n",
    "        'accuracy': 0.92,\n",
    "        'precision': 0.89,\n",
    "        'recall': 0.91,\n",
    "        'f1_score': 0.90,\n",
    "        'auc_roc': 0.95,\n",
    "        'training_time_seconds': 3600\n",
    "    }\n",
    "    \n",
    "    context['task_instance'].xcom_push(key='model_metrics', value=model_metrics)\n",
    "    logger.info(f\"Model trained: AUC-ROC = {model_metrics['auc_roc']}\")\n",
    "    return model_metrics\n",
    "\n",
    "def evaluate_model(**context):\n",
    "    \"\"\"Evaluate model on held-out test set.\"\"\"\n",
    "    logger.info(\"Evaluating model...\")\n",
    "    \n",
    "    metrics = context['task_instance'].xcom_pull(\n",
    "        task_ids='train_model',\n",
    "        key='model_metrics'\n",
    "    )\n",
    "    \n",
    "    # Quality gates\n",
    "    min_auc_roc = 0.85\n",
    "    min_precision = 0.80\n",
    "    \n",
    "    if metrics['auc_roc'] < min_auc_roc:\n",
    "        raise ValueError(f\"Model AUC-ROC {metrics['auc_roc']} below threshold {min_auc_roc}\")\n",
    "    \n",
    "    if metrics['precision'] < min_precision:\n",
    "        raise ValueError(f\"Model precision {metrics['precision']} below threshold {min_precision}\")\n",
    "    \n",
    "    logger.info(\"Model evaluation passed quality gates\")\n",
    "    return True\n",
    "\n",
    "def register_model(**context):\n",
    "    \"\"\"Register model in MLflow registry.\"\"\"\n",
    "    logger.info(\"Registering model...\")\n",
    "    \n",
    "    model_info = {\n",
    "        'name': 'churn_predictor',\n",
    "        'version': '2.3.0',\n",
    "        'stage': 'staging',\n",
    "        'artifacts': ['model.pt', 'preprocessor.pkl', 'feature_config.json'],\n",
    "        'registered_at': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    context['task_instance'].xcom_push(key='model_info', value=model_info)\n",
    "    logger.info(f\"Model registered: {model_info['name']} v{model_info['version']}\")\n",
    "    return model_info\n",
    "\n",
    "def deploy_to_staging(**context):\n",
    "    \"\"\"Deploy model to staging environment.\"\"\"\n",
    "    logger.info(\"Deploying to staging...\")\n",
    "    \n",
    "    model_info = context['task_instance'].xcom_pull(\n",
    "        task_ids='register_model',\n",
    "        key='model_info'\n",
    "    )\n",
    "    \n",
    "    deployment = {\n",
    "        'environment': 'staging',\n",
    "        'endpoint': 'https://staging-api.company.com/predict',\n",
    "        'replicas': 2,\n",
    "        'deployed_at': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Deployed to {deployment['environment']}\")\n",
    "    return deployment\n",
    "\n",
    "def run_integration_tests(**context):\n",
    "    \"\"\"Run integration tests on staging.\"\"\"\n",
    "    logger.info(\"Running integration tests...\")\n",
    "    \n",
    "    tests = {\n",
    "        'latency_p95_ms': 45,  # Must be < 50ms\n",
    "        'throughput_qps': 120,  # Must be > 100 QPS\n",
    "        'error_rate': 0.001,    # Must be < 0.5%\n",
    "        'prediction_accuracy': 0.91  # Must match training\n",
    "    }\n",
    "    \n",
    "    # Quality gates\n",
    "    assert tests['latency_p95_ms'] < 50, \"Latency too high\"\n",
    "    assert tests['throughput_qps'] > 100, \"Throughput too low\"\n",
    "    assert tests['error_rate'] < 0.005, \"Error rate too high\"\n",
    "    \n",
    "    logger.info(\"All integration tests passed\")\n",
    "    return tests\n",
    "\n",
    "def notify_team(**context):\n",
    "    \"\"\"Send notification to team.\"\"\"\n",
    "    logger.info(\"Sending notification...\")\n",
    "    \n",
    "    message = {\n",
    "        'pipeline': 'ml_training_pipeline',\n",
    "        'status': 'SUCCESS',\n",
    "        'run_id': context['run_id'],\n",
    "        'execution_date': context['execution_date'].isoformat(),\n",
    "        'next_step': 'Manual approval for production deployment'\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Notification sent: {message}\")\n",
    "    return message\n",
    "\n",
    "# Define task dependencies\n",
    "extract = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "validate = PythonOperator(\n",
    "    task_id='validate_data',\n",
    "    python_callable=validate_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "engineer_features = PythonOperator(\n",
    "    task_id='feature_engineering',\n",
    "    python_callable=feature_engineering,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "train = PythonOperator(\n",
    "    task_id='train_model',\n",
    "    python_callable=train_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "evaluate = PythonOperator(\n",
    "    task_id='evaluate_model',\n",
    "    python_callable=evaluate_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "register = PythonOperator(\n",
    "    task_id='register_model',\n",
    "    python_callable=register_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "deploy_staging = PythonOperator(\n",
    "    task_id='deploy_to_staging',\n",
    "    python_callable=deploy_to_staging,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "test = PythonOperator(\n",
    "    task_id='run_integration_tests',\n",
    "    python_callable=run_integration_tests,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "notify = PythonOperator(\n",
    "    task_id='notify_team',\n",
    "    python_callable=notify_team,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define DAG structure\n",
    "extract >> validate >> engineer_features >> train >> evaluate >> register >> deploy_staging >> test >> notify\n",
    "\n",
    "print(\"✅ Airflow DAG defined\")\n",
    "print(\"\\nDAG Structure:\")\n",
    "print(\"extract_data → validate_data → feature_engineering → train_model\")\n",
    "print(\"→ evaluate_model → register_model → deploy_to_staging\")\n",
    "print(\"→ run_integration_tests → notify_team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Store with Feast\n",
    "\n",
    "### Centralized Feature Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from feast import Entity, Feature, FeatureView, FileSource, ValueType\n",
    "from feast.feature_store import FeatureStore\n",
    "from datetime import timedelta\n",
    "\n",
    "class FeatureStoreManager:\n",
    "    \"\"\"Production feature store with online and offline serving.\"\"\"\n",
    "    \n",
    "    def __init__(self, repo_path: str = \"./feature_repo\"):\n",
    "        self.repo_path = Path(repo_path)\n",
    "        self.repo_path.mkdir(exist_ok=True)\n",
    "        self._setup_feature_definitions()\n",
    "    \n",
    "    def _setup_feature_definitions(self):\n",
    "        \"\"\"Define feature entities and views.\"\"\"\n",
    "        \n",
    "        # Define entity (e.g., user_id)\n",
    "        user = Entity(\n",
    "            name=\"user_id\",\n",
    "            value_type=ValueType.INT64,\n",
    "            description=\"User identifier\"\n",
    "        )\n",
    "        \n",
    "        # Define data source\n",
    "        user_stats_source = FileSource(\n",
    "            path=\"data/user_stats.parquet\",\n",
    "            event_timestamp_column=\"event_timestamp\",\n",
    "        )\n",
    "        \n",
    "        # Define feature view\n",
    "        user_engagement_features = FeatureView(\n",
    "            name=\"user_engagement\",\n",
    "            entities=[\"user_id\"],\n",
    "            ttl=timedelta(days=30),\n",
    "            features=[\n",
    "                Feature(name=\"sessions_7d\", dtype=ValueType.INT64),\n",
    "                Feature(name=\"sessions_30d\", dtype=ValueType.INT64),\n",
    "                Feature(name=\"avg_session_duration\", dtype=ValueType.FLOAT),\n",
    "                Feature(name=\"total_revenue\", dtype=ValueType.FLOAT),\n",
    "            ],\n",
    "            online=True,\n",
    "            source=user_stats_source,\n",
    "            tags={\"team\": \"ml\", \"version\": \"v2\"},\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Feature definitions created\")\n",
    "        return user_engagement_features\n",
    "    \n",
    "    def materialize_features(self, start_date: datetime, end_date: datetime):\n",
    "        \"\"\"Materialize features for offline training.\"\"\"\n",
    "        logger.info(f\"Materializing features from {start_date} to {end_date}\")\n",
    "        \n",
    "        # Simulated feature materialization\n",
    "        features = pd.DataFrame({\n",
    "            'user_id': range(1000),\n",
    "            'sessions_7d': np.random.randint(0, 50, 1000),\n",
    "            'sessions_30d': np.random.randint(0, 200, 1000),\n",
    "            'avg_session_duration': np.random.uniform(5, 60, 1000),\n",
    "            'total_revenue': np.random.uniform(0, 1000, 1000),\n",
    "            'event_timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Materialized {len(features)} feature rows\")\n",
    "        return features\n",
    "    \n",
    "    def get_online_features(self, entity_ids: List[int]) -> pd.DataFrame:\n",
    "        \"\"\"Get features for online inference.\"\"\"\n",
    "        logger.info(f\"Fetching online features for {len(entity_ids)} entities\")\n",
    "        \n",
    "        # Simulated online feature retrieval (sub-10ms in production)\n",
    "        features = pd.DataFrame({\n",
    "            'user_id': entity_ids,\n",
    "            'sessions_7d': np.random.randint(0, 50, len(entity_ids)),\n",
    "            'sessions_30d': np.random.randint(0, 200, len(entity_ids)),\n",
    "            'avg_session_duration': np.random.uniform(5, 60, len(entity_ids)),\n",
    "            'total_revenue': np.random.uniform(0, 1000, len(entity_ids)),\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_historical_features(self, entity_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Point-in-time correct features for training.\"\"\"\n",
    "        logger.info(\"Fetching historical features with point-in-time correctness\")\n",
    "        \n",
    "        # This prevents training-serving skew by ensuring features\n",
    "        # are computed exactly as they were at prediction time\n",
    "        historical_features = self.get_online_features(entity_df['user_id'].tolist())\n",
    "        \n",
    "        return pd.merge(entity_df, historical_features, on='user_id')\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n=== Feature Store Example ===\")\n",
    "fs = FeatureStoreManager()\n",
    "\n",
    "# Materialize for training\n",
    "offline_features = fs.materialize_features(\n",
    "    start_date=datetime.now() - timedelta(days=30),\n",
    "    end_date=datetime.now()\n",
    ")\n",
    "print(f\"\\nOffline features shape: {offline_features.shape}\")\n",
    "print(offline_features.head())\n",
    "\n",
    "# Online serving for inference\n",
    "online_features = fs.get_online_features([1, 2, 3, 4, 5])\n",
    "print(f\"\\nOnline features shape: {online_features.shape}\")\n",
    "print(online_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Versioning with DVC\n",
    "\n",
    "### Git for Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataVersionControl:\n",
    "    \"\"\"Data versioning and experiment tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_path: str = \".\"):\n",
    "        self.project_path = Path(project_path)\n",
    "        self.dvc_dir = self.project_path / \".dvc\"\n",
    "        self.dvc_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def track_dataset(self, dataset_path: str, remote: str = \"s3://ml-data\"):\n",
    "        \"\"\"Version control a dataset.\"\"\"\n",
    "        logger.info(f\"Tracking dataset: {dataset_path}\")\n",
    "        \n",
    "        # DVC commands (simulated)\n",
    "        commands = [\n",
    "            f\"dvc add {dataset_path}\",\n",
    "            f\"git add {dataset_path}.dvc .gitignore\",\n",
    "            f\"git commit -m 'Add dataset {dataset_path}'\",\n",
    "            f\"dvc push\"\n",
    "        ]\n",
    "        \n",
    "        metadata = {\n",
    "            'path': dataset_path,\n",
    "            'size_mb': 1024,\n",
    "            'hash': 'abc123def456',\n",
    "            'remote': remote,\n",
    "            'tracked_at': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Dataset tracked: {metadata}\")\n",
    "        return metadata\n",
    "    \n",
    "    def create_pipeline(self, name: str, stages: List[Dict]):\n",
    "        \"\"\"Define reproducible DVC pipeline.\"\"\"\n",
    "        logger.info(f\"Creating pipeline: {name}\")\n",
    "        \n",
    "        pipeline = {\n",
    "            'stages': {\n",
    "                'prepare': {\n",
    "                    'cmd': 'python src/prepare.py',\n",
    "                    'deps': ['data/raw.csv', 'src/prepare.py'],\n",
    "                    'outs': ['data/prepared.csv']\n",
    "                },\n",
    "                'featurize': {\n",
    "                    'cmd': 'python src/featurize.py',\n",
    "                    'deps': ['data/prepared.csv', 'src/featurize.py'],\n",
    "                    'outs': ['data/features.csv']\n",
    "                },\n",
    "                'train': {\n",
    "                    'cmd': 'python src/train.py',\n",
    "                    'deps': ['data/features.csv', 'src/train.py'],\n",
    "                    'params': ['train.epochs', 'train.lr'],\n",
    "                    'outs': ['models/model.pt'],\n",
    "                    'metrics': ['metrics.json']\n",
    "                },\n",
    "                'evaluate': {\n",
    "                    'cmd': 'python src/evaluate.py',\n",
    "                    'deps': ['models/model.pt', 'data/test.csv'],\n",
    "                    'metrics': ['evaluation.json']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Pipeline created with {len(pipeline['stages'])} stages\")\n",
    "        return pipeline\n",
    "    \n",
    "    def compare_experiments(self, experiment_ids: List[str]):\n",
    "        \"\"\"Compare metrics across experiments.\"\"\"\n",
    "        logger.info(f\"Comparing {len(experiment_ids)} experiments\")\n",
    "        \n",
    "        # Simulated experiment comparison\n",
    "        experiments = []\n",
    "        for exp_id in experiment_ids:\n",
    "            experiments.append({\n",
    "                'experiment': exp_id,\n",
    "                'accuracy': np.random.uniform(0.85, 0.95),\n",
    "                'precision': np.random.uniform(0.80, 0.92),\n",
    "                'recall': np.random.uniform(0.82, 0.93),\n",
    "                'f1_score': np.random.uniform(0.81, 0.92),\n",
    "                'train_time_s': np.random.randint(1800, 7200)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(experiments)\n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n=== Data Version Control ===\")\n",
    "dvc = DataVersionControl()\n",
    "\n",
    "# Track dataset\n",
    "dataset_meta = dvc.track_dataset('data/training_set_v3.parquet')\n",
    "print(f\"\\nDataset metadata: {dataset_meta}\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dvc.create_pipeline('ml_pipeline', [])\n",
    "print(f\"\\nPipeline stages: {list(pipeline['stages'].keys())}\")\n",
    "\n",
    "# Compare experiments\n",
    "comparison = dvc.compare_experiments(['exp-001', 'exp-002', 'exp-003'])\n",
    "print(\"\\nExperiment comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: MLflow Model Registry\n",
    "\n",
    "### Centralized Model Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"MLflow-based model registry for lifecycle management.\"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_uri: str = \"http://localhost:5000\"):\n",
    "        self.tracking_uri = tracking_uri\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        self.client = MlflowClient()\n",
    "    \n",
    "    def log_experiment(self, experiment_name: str, params: Dict, metrics: Dict, artifacts: List[str]):\n",
    "        \"\"\"Log training experiment with all metadata.\"\"\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        with mlflow.start_run() as run:\n",
    "            # Log parameters\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metrics(metrics)\n",
    "            \n",
    "            # Log artifacts (models, plots, configs)\n",
    "            for artifact in artifacts:\n",
    "                mlflow.log_artifact(artifact)\n",
    "            \n",
    "            # Log model\n",
    "            # mlflow.pytorch.log_model(model, \"model\")\n",
    "            \n",
    "            logger.info(f\"Logged experiment: {run.info.run_id}\")\n",
    "            return run.info.run_id\n",
    "    \n",
    "    def register_model(self, model_name: str, run_id: str, stage: str = \"Staging\"):\n",
    "        \"\"\"Register model in registry.\"\"\"\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        \n",
    "        # Register model\n",
    "        model_details = mlflow.register_model(model_uri, model_name)\n",
    "        \n",
    "        # Transition to stage\n",
    "        self.client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=model_details.version,\n",
    "            stage=stage\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Registered {model_name} v{model_details.version} in {stage}\")\n",
    "        return model_details\n",
    "    \n",
    "    def promote_model(self, model_name: str, version: int, from_stage: str, to_stage: str):\n",
    "        \"\"\"Promote model to production.\"\"\"\n",
    "        self.client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=to_stage,\n",
    "            archive_existing_versions=True  # Archive old production models\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Promoted {model_name} v{version} from {from_stage} to {to_stage}\")\n",
    "        return True\n",
    "    \n",
    "    def get_model_lineage(self, model_name: str, version: int):\n",
    "        \"\"\"Get full lineage: data, code, params, metrics.\"\"\"\n",
    "        model_version = self.client.get_model_version(model_name, version)\n",
    "        run = self.client.get_run(model_version.run_id)\n",
    "        \n",
    "        lineage = {\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'run_id': run.info.run_id,\n",
    "            'parameters': run.data.params,\n",
    "            'metrics': run.data.metrics,\n",
    "            'artifacts': run.info.artifact_uri,\n",
    "            'tags': run.data.tags,\n",
    "            'created_at': run.info.start_time\n",
    "        }\n",
    "        \n",
    "        return lineage\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n=== Model Registry ===\")\n",
    "print(\"MLflow model registry provides:\")\n",
    "print(\"- Model versioning and lineage\")\n",
    "print(\"- Stage transitions (None → Staging → Production → Archived)\")\n",
    "print(\"- Experiment tracking with parameters and metrics\")\n",
    "print(\"- Artifact storage for models, configs, and plots\")\n",
    "print(\"\\nTypical workflow:\")\n",
    "print(\"1. Train model and log to MLflow\")\n",
    "print(\"2. Register model in registry (Staging stage)\")\n",
    "print(\"3. Run A/B test in staging\")\n",
    "print(\"4. Promote to Production if metrics improve\")\n",
    "print(\"5. Archive old production model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Deployment Strategies\n",
    "\n",
    "### Safe Production Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DeploymentStrategy:\n",
    "    \"\"\"Safe deployment patterns for ML models.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def canary_deployment(new_model_traffic: float = 0.1):\n",
    "        \"\"\"Gradually increase traffic to new model.\"\"\"\n",
    "        \n",
    "        stages = [\n",
    "            {'traffic_pct': 10, 'duration_hours': 2, 'rollback_threshold': 0.05},\n",
    "            {'traffic_pct': 25, 'duration_hours': 4, 'rollback_threshold': 0.03},\n",
    "            {'traffic_pct': 50, 'duration_hours': 8, 'rollback_threshold': 0.02},\n",
    "            {'traffic_pct': 100, 'duration_hours': 24, 'rollback_threshold': 0.01}\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'strategy': 'canary',\n",
    "            'stages': stages,\n",
    "            'monitoring': ['error_rate', 'latency_p95', 'prediction_drift']\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def blue_green_deployment():\n",
    "        \"\"\"Instant switchover with quick rollback.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'strategy': 'blue_green',\n",
    "            'blue_environment': 'production-v1',\n",
    "            'green_environment': 'production-v2',\n",
    "            'steps': [\n",
    "                '1. Deploy v2 to green environment',\n",
    "                '2. Run smoke tests on green',\n",
    "                '3. Switch load balancer to green',\n",
    "                '4. Monitor metrics for 1 hour',\n",
    "                '5. If stable, keep blue for 24h then decomission'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def ab_testing(treatment_pct: float = 0.5, duration_days: int = 7):\n",
    "        \"\"\"A/B test new model against baseline.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'strategy': 'ab_test',\n",
    "            'control_model': 'v1.0',\n",
    "            'treatment_model': 'v2.0',\n",
    "            'traffic_split': {'control': 50, 'treatment': 50},\n",
    "            'duration_days': duration_days,\n",
    "            'metrics': [\n",
    "                'prediction_accuracy',\n",
    "                'user_engagement',\n",
    "                'revenue_per_user',\n",
    "                'latency_p95'\n",
    "            ],\n",
    "            'success_criteria': {\n",
    "                'min_improvement': 0.02,  # 2% lift required\n",
    "                'significance_level': 0.05,  # 95% confidence\n",
    "                'min_sample_size': 10000\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def shadow_deployment():\n",
    "        \"\"\"Run new model in parallel without affecting users.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'strategy': 'shadow',\n",
    "            'production_model': 'v1.0',\n",
    "            'shadow_model': 'v2.0',\n",
    "            'traffic': '100% to production, duplicate to shadow',\n",
    "            'duration_days': 3,\n",
    "            'comparison_metrics': [\n",
    "                'prediction_difference',\n",
    "                'latency_comparison',\n",
    "                'resource_usage'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Example deployment plans\n",
    "print(\"\\n=== Deployment Strategies ===\")\n",
    "\n",
    "canary = DeploymentStrategy.canary_deployment()\n",
    "print(\"\\n1. Canary Deployment:\")\n",
    "for stage in canary['stages']:\n",
    "    print(f\"   - {stage['traffic_pct']}% traffic for {stage['duration_hours']}h\")\n",
    "\n",
    "ab_test = DeploymentStrategy.ab_testing()\n",
    "print(f\"\\n2. A/B Testing:\")\n",
    "print(f\"   - Control: {ab_test['control_model']}\")\n",
    "print(f\"   - Treatment: {ab_test['treatment_model']}\")\n",
    "print(f\"   - Duration: {ab_test['duration_days']} days\")\n",
    "print(f\"   - Min improvement: {ab_test['success_criteria']['min_improvement']*100}%\")\n",
    "\n",
    "shadow = DeploymentStrategy.shadow_deployment()\n",
    "print(f\"\\n3. Shadow Deployment:\")\n",
    "print(f\"   - Production: {shadow['production_model']} (serves users)\")\n",
    "print(f\"   - Shadow: {shadow['shadow_model']} (observes only)\")\n",
    "print(f\"   - Duration: {shadow['duration_days']} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Enterprise ML Pipeline Components:\n",
    "1. ✅ **Orchestration**: Airflow DAGs with retries, alerts, dependencies\n",
    "2. ✅ **Feature Store**: Centralized features, point-in-time correctness\n",
    "3. ✅ **Data Versioning**: DVC for reproducible data and model tracking\n",
    "4. ✅ **Model Registry**: MLflow for lifecycle management\n",
    "5. ✅ **Deployment**: Canary, blue-green, A/B testing, shadow\n",
    "6. ✅ **Quality Gates**: Automated validation at each stage\n",
    "7. ✅ **Monitoring**: Metrics, alerts, rollback triggers\n",
    "8. ✅ **Compliance**: Audit trails, lineage tracking\n",
    "\n",
    "### Production Best Practices:\n",
    "- **Separation of Concerns**: Data, features, training, serving\n",
    "- **Automation**: CI/CD for ML, automated retraining\n",
    "- **Observability**: Metrics at every pipeline stage\n",
    "- **Reproducibility**: Version everything (data, code, models, config)\n",
    "- **Safety**: Quality gates, gradual rollouts, quick rollbacks\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "1. **What is training-serving skew and how do you prevent it?**\n",
    "   - Skew: Features computed differently in training vs serving\n",
    "   - Prevention: Feature store with point-in-time correctness\n",
    "   - Example: Using 7-day average at prediction time, not future data\n",
    "\n",
    "2. **How do you design a retraining pipeline?**\n",
    "   - Trigger: Schedule (daily), drift detection, or manual\n",
    "   - Steps: Extract new data → validate → engineer features → train → evaluate\n",
    "   - Gates: Data quality, model performance, integration tests\n",
    "   - Deploy: Canary rollout with monitoring\n",
    "\n",
    "3. **Explain the trade-offs between deployment strategies.**\n",
    "   - Canary: Gradual, safe, but slow rollout\n",
    "   - Blue-Green: Fast switchover, but requires 2x infrastructure\n",
    "   - A/B: Rigorous testing, but needs statistical significance\n",
    "   - Shadow: Zero risk, but doesn't validate business impact\n",
    "\n",
    "4. **What should be in a feature store?**\n",
    "   - Online store: Low-latency serving (<10ms) with Redis/DynamoDB\n",
    "   - Offline store: Historical features for training (Parquet/S3)\n",
    "   - Feature definitions: Transformations, owners, SLAs\n",
    "   - Monitoring: Drift detection, freshness, correctness\n",
    "   - Point-in-time joins: Prevent data leakage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
