{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780dac6d",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 20: Quantization and Efficiency (The Systems Layer)\n",
    "\n",
    "**Author:** [Your Name/Organization]  \n",
    "**Date:** 2025  \n",
    "\n",
    "In the world of Large Language Models, **Memory is Money**. \n",
    "\n",
    "A 70 Billion parameter model in standard 32-bit precision requires:\n",
    "$$ 70 \\times 10^9 \\times 4 \\text{ bytes} \\approx 280 \\text{ GB VRAM} $$\n",
    "\n",
    "That requires 4x A100 GPUs ($80k+ hardware). But with **Quantization**, we can shrink this to 4-bits, fitting it on a single GPU. \n",
    "\n",
    "This tutorial dives into the \"Systems\" side of AI: making models run fast and cheap.\n",
    "\n",
    "## Learning Objectives\n",
    "1.  **Understand Precision**: FP32 vs FP16 vs INT8.\n",
    "2.  **Implement Quantization**: Write a function to compress a tensor from 32-bit float to 8-bit integer.\n",
    "3.  **Implement LoRA**: Build a Low-Rank Adapter layer from scratch to fine-tune massive models efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391359c",
   "metadata": {},
   "source": "## 1. Vocabulary First\n\n-   **FP32 (Single Precision)**: Standard float format. 1 sign bit, 8 exponent, 23 mantissa. (4 bytes)\n-   **BF16 (Bfloat16)**: Truncated FP32. 1 sign, 8 exponent, 7 mantissa. Same range as FP32, less precision. (2 bytes)\n-   **FP16 (Half Precision)**: 1 sign, 5 exponent, 10 mantissa. Smaller range than BF16 but more precision within that range. (2 bytes)\n-   **INT8**: 8-bit Integer. Values from -128 to 127. (1 byte)\n-   **INT4/NF4**: 4-bit formats. Only 16 possible values. (0.5 bytes)\n-   **Quantization**: The process of mapping a large continuous range (float) to a small discrete set (int).\n-   **LoRA (Low-Rank Adaptation)**: Instead of updating all weights $W$, we learn a small update $\\Delta W = A \\times B$, where $A$ and $B$ are tiny matrices.\n\n### The Precision Hierarchy\n\n```\nFP32 (4 bytes) → BF16/FP16 (2 bytes) → INT8 (1 byte) → INT4 (0.5 bytes)\n   100%              50%                    25%             12.5%\n   Full quality      Minimal loss           Small loss      Noticeable loss\n```\n\n### BF16 vs FP16: Why BF16 Won\n\nBoth are 16-bit, but they allocate bits differently:\n- **FP16**: 5 exponent bits, 10 mantissa bits → More precision, but smaller range (overflows easily during training)\n- **BF16**: 8 exponent bits, 7 mantissa bits → Less precision, but same range as FP32 (no overflow)\n\nBF16 is the default for modern LLM training because it handles the wide range of values in gradients without needing loss scaling. FP16 requires careful loss scaling (`torch.cuda.amp.GradScaler`) to avoid underflow/overflow.\n\n### Quantization Methods Compared\n\n| Method | How It Works | Bits | Quality Loss | Use Case |\n|--------|-------------|------|-------------|----------|\n| **Absmax** | Scale by max absolute value | INT8 | Low | Simple, symmetric distributions |\n| **Zero-point** | Shift + scale to map min/max to INT8 range | INT8 | Lower | Asymmetric distributions (ReLU outputs) |\n| **NF4** (NormalFloat4) | Uses quantile-based mapping optimized for normal distributions | INT4 | Moderate | QLoRA, 4-bit LLM inference |\n| **GPTQ** | Layer-wise quantization minimizing reconstruction error | INT4 | Low for INT4 | Offline model compression |\n| **AWQ** | Protects salient weights from quantization | INT4 | Very low | Production INT4 deployment |\n\n### The Efficiency Landscape\n\nQuantization is one of several efficiency techniques. Here's how they compare:\n\n| Technique | What It Does | Model Size Reduction | Speed Improvement | Quality Impact |\n|-----------|-------------|---------------------|-------------------|----------------|\n| **Quantization** | Reduce precision | 2-4x | 2-4x | Small |\n| **Pruning** | Remove unimportant weights | 2-10x | 1.5-3x (with sparse hardware) | Small-Medium |\n| **Distillation** | Train small model to mimic large one | 10-100x | 10-100x | Medium |\n| **LoRA** | Train only small adapter weights | N/A (same size) | N/A (training only) | Minimal |\n| **QLoRA** | LoRA + 4-bit quantization | 4x | N/A (training only) | Small |"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8c224b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:36.848341Z",
     "iopub.status.busy": "2025-11-20T06:03:36.848110Z",
     "iopub.status.idle": "2025-11-20T06:03:37.708262Z",
     "shell.execute_reply": "2025-11-20T06:03:37.707823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d21b14",
   "metadata": {},
   "source": [
    "## 2. Implementing Int8 Quantization\n",
    "\n",
    "We will implement **Absmax Quantization**. This is the simplest form of symmetric quantization.\n",
    "\n",
    "### The Formula\n",
    "To map a float tensor $X$ to int8:\n",
    "\n",
    "1.  Find the absolute maximum value: $S = \\max(|X|)$.\n",
    "2.  Calculate the scaling factor: $scale = 127 / S$.\n",
    "3.  Quantize: $X_{quant} = \\text{round}(X \\times scale)$.\n",
    "4.  Clamp: Ensure values stay within [-128, 127].\n",
    "\n",
    "To get back the original values (Dequantization):\n",
    "$$ X_{approx} = X_{quant} / scale $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb4e5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.709597Z",
     "iopub.status.busy": "2025-11-20T06:03:37.709494Z",
     "iopub.status.idle": "2025-11-20T06:03:37.711723Z",
     "shell.execute_reply": "2025-11-20T06:03:37.711405Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_int8(tensor):\n",
    "    \"\"\"\n",
    "    Quantizes a float32 tensor to int8 using Absmax Quantization.\n",
    "    Returns the quantized tensor (int8) and the scale factor (float).\n",
    "    \"\"\"\n",
    "    # 1. Find the absolute max value\n",
    "    absmax = torch.abs(tensor).max()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if absmax == 0:\n",
    "        scale = 1.0\n",
    "    else:\n",
    "        # 2. Calculate scale to map max value to 127\n",
    "        scale = 127.0 / absmax\n",
    "    \n",
    "    # 3. Quantize\n",
    "    # We multiply by scale and round to nearest integer\n",
    "    quantized = torch.round(tensor * scale)\n",
    "    \n",
    "    # 4. Clamp to int8 range [-128, 127]\n",
    "    quantized = torch.clamp(quantized, -128, 127)\n",
    "    \n",
    "    # Cast to actual int8 type to save memory\n",
    "    quantized = quantized.to(torch.int8)\n",
    "    \n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_int8(quantized, scale):\n",
    "    \"\"\"\n",
    "    Dequantizes an int8 tensor back to float32.\n",
    "    \"\"\"\n",
    "    # Convert back to float for calculation\n",
    "    quantized_float = quantized.to(torch.float32)\n",
    "    \n",
    "    # Reverse the scaling\n",
    "    return quantized_float / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f9859",
   "metadata": {},
   "source": [
    "### Testing Quantization\n",
    "\n",
    "Let's see how much error this introduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7255123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.713016Z",
     "iopub.status.busy": "2025-11-20T06:03:37.712941Z",
     "iopub.status.idle": "2025-11-20T06:03:37.738877Z",
     "shell.execute_reply": "2025-11-20T06:03:37.738597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (First Row): tensor([-10.7216,  22.7951,  -2.6229, -12.3593,   7.3067])\n",
      "\n",
      "Quantized (int8): tensor([-60, 127, -15, -69,  41], dtype=torch.int8)\n",
      "Scale Factor: 5.5714\n",
      "\n",
      "Reconstructed: tensor([-10.7693,  22.7951,  -2.6923, -12.3847,   7.3591])\n",
      "\n",
      "Mean Squared Error: 0.002523\n",
      "Memory: 100 bytes -> 25 bytes (4.0x compression)\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor\n",
    "original = torch.randn(5, 5) * 10.0 # Scale it up a bit\n",
    "print(\"Original (First Row):\", original[0])\n",
    "\n",
    "# Quantize\n",
    "q_tensor, scale = quantize_int8(original)\n",
    "print(\"\\nQuantized (int8):\", q_tensor[0])\n",
    "print(f\"Scale Factor: {scale.item():.4f}\")\n",
    "\n",
    "# Dequantize\n",
    "reconstructed = dequantize_int8(q_tensor, scale)\n",
    "print(\"\\nReconstructed:\", reconstructed[0])\n",
    "\n",
    "# Calculate Error (MSE)\n",
    "mse = F.mse_loss(original, reconstructed)\n",
    "print(f\"\\nMean Squared Error: {mse.item():.6f}\")\n",
    "\n",
    "# Memory Savings\n",
    "orig_mem = original.element_size() * original.numel()\n",
    "quant_mem = q_tensor.element_size() * q_tensor.numel()\n",
    "print(f\"Memory: {orig_mem} bytes -> {quant_mem} bytes ({orig_mem/quant_mem}x compression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aee583",
   "metadata": {},
   "source": "## 3. Implementing LoRA (Low-Rank Adaptation)\n\nFine-tuning a 70B model involves updating 70B weights. That's expensive.\nLoRA freezes the main weights $W$ and adds a parallel branch with two tiny matrices $A$ and $B$.\n\n$$ h = Wx + BAx $$\n\nWhere $W$ is $[d_{in}, d_{out}]$, $A$ is $[d_{in}, r]$, and $B$ is $[r, d_{out}]$.\nIf $r=8$, we save 99.9% of trainable parameters.\n\n### Why LoRA Works (The Intuition)\n\nThe key insight is that weight updates during fine-tuning have **low intrinsic rank**. When you fine-tune a model on a specific task, the actual \"change\" to the weight matrix lies in a low-dimensional subspace. LoRA exploits this by directly parameterizing the update as a low-rank matrix.\n\n**Analogy**: Imagine a 1000x1000 weight matrix. Full fine-tuning updates all 1,000,000 values. But the \"meaningful change\" for a specific task might only require adjusting along 8 directions (rank 8). LoRA directly learns those 8 directions.\n\n### Key Design Choices\n\n- **Rank (r)**: Higher rank = more parameters, more expressiveness. Typical values: 4, 8, 16, 32. Most tasks work well with r=8.\n- **Alpha**: Scaling factor. The output is multiplied by `alpha/rank`. Higher alpha = stronger adaptation.\n- **Which layers?** Usually applied to attention layers (Q, K, V, O projections). Some approaches also add LoRA to MLP layers.\n- **B initialization**: B is initialized to zero so that the LoRA branch starts as an identity (no change to the original model).\n\n### QLoRA: Combining LoRA with 4-bit Quantization\n\nQLoRA makes it possible to fine-tune a 70B model on a single GPU:\n1. Load the base model in 4-bit (NF4 quantization) — only ~35GB\n2. Add LoRA adapters in BF16 on top — only ~100MB extra\n3. Only the LoRA weights are trained; the base model stays frozen in 4-bit\n4. Gradients flow through the quantized weights via dequantization during forward pass\n\nThis lets you fine-tune Llama-3 70B on a single A100 (80GB) — previously impossible without 4+ GPUs."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe426ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.740071Z",
     "iopub.status.busy": "2025-11-20T06:03:37.740007Z",
     "iopub.status.idle": "2025-11-20T06:03:37.742261Z",
     "shell.execute_reply": "2025-11-20T06:03:37.741967Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. The Pre-trained Weight (Frozen)\n",
    "        # In a real library, this would wrap an existing layer\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.linear.weight.requires_grad = False # FREEZE IT!\n",
    "        self.linear.bias.requires_grad = False\n",
    "        \n",
    "        # 2. The LoRA Adapters (Trainable)\n",
    "        # A: [in, rank] -> Gaussian Init\n",
    "        # B: [rank, out] -> Zero Init (so training starts as identity)\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)\n",
    "        \n",
    "        # 3. Scaling\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original Path (Frozen)\n",
    "        original_output = self.linear(x)\n",
    "        \n",
    "        # LoRA Path (Trainable)\n",
    "        # x -> A -> B -> Scale\n",
    "        lora_output = self.lora_b(self.lora_a(x)) * self.scaling\n",
    "        \n",
    "        # Combine\n",
    "        return original_output + lora_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295531a",
   "metadata": {},
   "source": [
    "### Testing LoRA\n",
    "\n",
    "Let's verify that initially, LoRA does nothing (because B is zero), but it has way fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c43686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.743286Z",
     "iopub.status.busy": "2025-11-20T06:03:37.743218Z",
     "iopub.status.idle": "2025-11-20T06:03:37.753390Z",
     "shell.execute_reply": "2025-11-20T06:03:37.752995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,065,984\n",
      "Trainable Parameters (LoRA): 16,384\n",
      "Percentage Trainable: 1.54%\n",
      "Output Shape: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "d_in, d_out = 1024, 1024\n",
    "rank = 8\n",
    "\n",
    "# Create Layer\n",
    "layer = LoRALinear(d_in, d_out, rank=rank)\n",
    "\n",
    "# Count Parameters\n",
    "total_params = sum(p.numel() for p in layer.parameters())\n",
    "trainable_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"Percentage Trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Verify Output\n",
    "x = torch.randn(1, d_in)\n",
    "y = layer(x)\n",
    "print(f\"Output Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee88af",
   "metadata": {},
   "source": "## 4. Conclusion\n\nYou have now implemented the two pillars of efficient AI:\n1.  **Quantization**: Reducing memory usage by 4x with minimal error. In practice, INT8 quantization is nearly lossless for most models, while INT4 requires careful methods like GPTQ or AWQ to maintain quality.\n2.  **LoRA**: Reducing trainable parameters by 99% for fine-tuning. Combined with 4-bit quantization (QLoRA), this lets you fine-tune massive models on consumer hardware.\n\n### Quick Decision Guide\n\n```\nNeed to run a large model faster/cheaper?\n  └─ Quantize it (INT8 for minimal loss, INT4 for maximum savings)\n\nNeed to fine-tune a large model?\n  ├─ Have multiple GPUs? → Full fine-tuning or LoRA\n  └─ Single GPU? → QLoRA (4-bit base + LoRA adapters)\n\nNeed a much smaller model?\n  └─ Knowledge distillation (train a small model to mimic the large one)\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}