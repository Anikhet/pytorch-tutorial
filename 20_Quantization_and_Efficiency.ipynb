{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780dac6d",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 20: Quantization and Efficiency (The Systems Layer)\n",
    "\n",
    "**Author:** [Your Name/Organization]  \n",
    "**Date:** 2025  \n",
    "\n",
    "In the world of Large Language Models, **Memory is Money**. \n",
    "\n",
    "A 70 Billion parameter model in standard 32-bit precision requires:\n",
    "$$ 70 \\times 10^9 \\times 4 \\text{ bytes} \\approx 280 \\text{ GB VRAM} $$\n",
    "\n",
    "That requires 4x A100 GPUs ($80k+ hardware). But with **Quantization**, we can shrink this to 4-bits, fitting it on a single GPU. \n",
    "\n",
    "This tutorial dives into the \"Systems\" side of AI: making models run fast and cheap.\n",
    "\n",
    "## Learning Objectives\n",
    "1.  **Understand Precision**: FP32 vs FP16 vs INT8.\n",
    "2.  **Implement Quantization**: Write a function to compress a tensor from 32-bit float to 8-bit integer.\n",
    "3.  **Implement LoRA**: Build a Low-Rank Adapter layer from scratch to fine-tune massive models efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391359c",
   "metadata": {},
   "source": [
    "## 1. Vocabulary First\n",
    "\n",
    "-   **FP32 (Single Precision)**: Standard float format. 1 sign bit, 8 exponent, 23 mantissa. (4 bytes)\n",
    "-   **BF16 (Bfloat16)**: Truncated FP32. 1 sign, 8 exponent, 7 mantissa. Same range as FP32, less precision. (2 bytes)\n",
    "-   **INT8**: 8-bit Integer. Values from -128 to 127. (1 byte)\n",
    "-   **Quantization**: The process of mapping a large continuous range (float) to a small discrete set (int).\n",
    "-   **LoRA (Low-Rank Adaptation)**: Instead of updating all weights $W$, we learn a small update $\\Delta W = A \\times B$, where $A$ and $B$ are tiny matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8c224b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:36.848341Z",
     "iopub.status.busy": "2025-11-20T06:03:36.848110Z",
     "iopub.status.idle": "2025-11-20T06:03:37.708262Z",
     "shell.execute_reply": "2025-11-20T06:03:37.707823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d21b14",
   "metadata": {},
   "source": [
    "## 2. Implementing Int8 Quantization\n",
    "\n",
    "We will implement **Absmax Quantization**. This is the simplest form of symmetric quantization.\n",
    "\n",
    "### The Formula\n",
    "To map a float tensor $X$ to int8:\n",
    "\n",
    "1.  Find the absolute maximum value: $S = \\max(|X|)$.\n",
    "2.  Calculate the scaling factor: $scale = 127 / S$.\n",
    "3.  Quantize: $X_{quant} = \\text{round}(X \\times scale)$.\n",
    "4.  Clamp: Ensure values stay within [-128, 127].\n",
    "\n",
    "To get back the original values (Dequantization):\n",
    "$$ X_{approx} = X_{quant} / scale $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb4e5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.709597Z",
     "iopub.status.busy": "2025-11-20T06:03:37.709494Z",
     "iopub.status.idle": "2025-11-20T06:03:37.711723Z",
     "shell.execute_reply": "2025-11-20T06:03:37.711405Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_int8(tensor):\n",
    "    \"\"\"\n",
    "    Quantizes a float32 tensor to int8 using Absmax Quantization.\n",
    "    Returns the quantized tensor (int8) and the scale factor (float).\n",
    "    \"\"\"\n",
    "    # 1. Find the absolute max value\n",
    "    absmax = torch.abs(tensor).max()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if absmax == 0:\n",
    "        scale = 1.0\n",
    "    else:\n",
    "        # 2. Calculate scale to map max value to 127\n",
    "        scale = 127.0 / absmax\n",
    "    \n",
    "    # 3. Quantize\n",
    "    # We multiply by scale and round to nearest integer\n",
    "    quantized = torch.round(tensor * scale)\n",
    "    \n",
    "    # 4. Clamp to int8 range [-128, 127]\n",
    "    quantized = torch.clamp(quantized, -128, 127)\n",
    "    \n",
    "    # Cast to actual int8 type to save memory\n",
    "    quantized = quantized.to(torch.int8)\n",
    "    \n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_int8(quantized, scale):\n",
    "    \"\"\"\n",
    "    Dequantizes an int8 tensor back to float32.\n",
    "    \"\"\"\n",
    "    # Convert back to float for calculation\n",
    "    quantized_float = quantized.to(torch.float32)\n",
    "    \n",
    "    # Reverse the scaling\n",
    "    return quantized_float / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f9859",
   "metadata": {},
   "source": [
    "### Testing Quantization\n",
    "\n",
    "Let's see how much error this introduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7255123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.713016Z",
     "iopub.status.busy": "2025-11-20T06:03:37.712941Z",
     "iopub.status.idle": "2025-11-20T06:03:37.738877Z",
     "shell.execute_reply": "2025-11-20T06:03:37.738597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (First Row): tensor([-10.7216,  22.7951,  -2.6229, -12.3593,   7.3067])\n",
      "\n",
      "Quantized (int8): tensor([-60, 127, -15, -69,  41], dtype=torch.int8)\n",
      "Scale Factor: 5.5714\n",
      "\n",
      "Reconstructed: tensor([-10.7693,  22.7951,  -2.6923, -12.3847,   7.3591])\n",
      "\n",
      "Mean Squared Error: 0.002523\n",
      "Memory: 100 bytes -> 25 bytes (4.0x compression)\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor\n",
    "original = torch.randn(5, 5) * 10.0 # Scale it up a bit\n",
    "print(\"Original (First Row):\", original[0])\n",
    "\n",
    "# Quantize\n",
    "q_tensor, scale = quantize_int8(original)\n",
    "print(\"\\nQuantized (int8):\", q_tensor[0])\n",
    "print(f\"Scale Factor: {scale.item():.4f}\")\n",
    "\n",
    "# Dequantize\n",
    "reconstructed = dequantize_int8(q_tensor, scale)\n",
    "print(\"\\nReconstructed:\", reconstructed[0])\n",
    "\n",
    "# Calculate Error (MSE)\n",
    "mse = F.mse_loss(original, reconstructed)\n",
    "print(f\"\\nMean Squared Error: {mse.item():.6f}\")\n",
    "\n",
    "# Memory Savings\n",
    "orig_mem = original.element_size() * original.numel()\n",
    "quant_mem = q_tensor.element_size() * q_tensor.numel()\n",
    "print(f\"Memory: {orig_mem} bytes -> {quant_mem} bytes ({orig_mem/quant_mem}x compression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aee583",
   "metadata": {},
   "source": [
    "## 3. Implementing LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Fine-tuning a 70B model involves updating 70B weights. That's expensive.\n",
    "LoRA freezes the main weights $W$ and adds a parallel branch with two tiny matrices $A$ and $B$.\n",
    "\n",
    "$$ h = Wx + BAx $$\n",
    "\n",
    "Where $W$ is $[d_{in}, d_{out}]$, $A$ is $[d_{in}, r]$, and $B$ is $[r, d_{out}]$.\n",
    "If $r=8$, we save 99.9% of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe426ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.740071Z",
     "iopub.status.busy": "2025-11-20T06:03:37.740007Z",
     "iopub.status.idle": "2025-11-20T06:03:37.742261Z",
     "shell.execute_reply": "2025-11-20T06:03:37.741967Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. The Pre-trained Weight (Frozen)\n",
    "        # In a real library, this would wrap an existing layer\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.linear.weight.requires_grad = False # FREEZE IT!\n",
    "        self.linear.bias.requires_grad = False\n",
    "        \n",
    "        # 2. The LoRA Adapters (Trainable)\n",
    "        # A: [in, rank] -> Gaussian Init\n",
    "        # B: [rank, out] -> Zero Init (so training starts as identity)\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)\n",
    "        \n",
    "        # 3. Scaling\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original Path (Frozen)\n",
    "        original_output = self.linear(x)\n",
    "        \n",
    "        # LoRA Path (Trainable)\n",
    "        # x -> A -> B -> Scale\n",
    "        lora_output = self.lora_b(self.lora_a(x)) * self.scaling\n",
    "        \n",
    "        # Combine\n",
    "        return original_output + lora_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295531a",
   "metadata": {},
   "source": [
    "### Testing LoRA\n",
    "\n",
    "Let's verify that initially, LoRA does nothing (because B is zero), but it has way fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c43686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:03:37.743286Z",
     "iopub.status.busy": "2025-11-20T06:03:37.743218Z",
     "iopub.status.idle": "2025-11-20T06:03:37.753390Z",
     "shell.execute_reply": "2025-11-20T06:03:37.752995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,065,984\n",
      "Trainable Parameters (LoRA): 16,384\n",
      "Percentage Trainable: 1.54%\n",
      "Output Shape: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "d_in, d_out = 1024, 1024\n",
    "rank = 8\n",
    "\n",
    "# Create Layer\n",
    "layer = LoRALinear(d_in, d_out, rank=rank)\n",
    "\n",
    "# Count Parameters\n",
    "total_params = sum(p.numel() for p in layer.parameters())\n",
    "trainable_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"Percentage Trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Verify Output\n",
    "x = torch.randn(1, d_in)\n",
    "y = layer(x)\n",
    "print(f\"Output Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee88af",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "You have now implemented the two pillars of efficient AI:\n",
    "1.  **Quantization**: Reducing memory usage by 4x with minimal error.\n",
    "2.  **LoRA**: Reducing trainable parameters by 99% for fine-tuning.\n",
    "\n",
    "This concludes the PyTorch Tutorial Series. You have gone from Tensors (Notebook 00) to building Agents (18), Aligning them (19), and Optimizing them (20). You are ready for the industry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
