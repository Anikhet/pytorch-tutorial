{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Autoencoders and Variational Autoencoders (VAEs)\n",
    "\n",
    "Autoencoders are neural networks that learn to compress data into a lower-dimensional **latent space** and then reconstruct it. Variational Autoencoders (VAEs) extend this idea by learning a **probabilistic** latent space, enabling generation of new data.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the encoder-decoder architecture\n",
    "- Implement a vanilla autoencoder for image reconstruction\n",
    "- Learn the VAE reparameterization trick and ELBO loss\n",
    "- Visualize and navigate latent spaces\n",
    "- Implement beta-VAE for disentangled representations\n",
    "- Build a Conditional VAE (CVAE) for controlled generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We'll use MNIST for simplicity - 28x28 grayscale images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla Autoencoder\n",
    "\n",
    "An autoencoder has two parts:\n",
    "- **Encoder**: Compresses input x to latent representation z\n",
    "- **Decoder**: Reconstructs x_hat from z\n",
    "\n",
    "The bottleneck (latent dimension) forces the network to learn meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: 784 -> 256 -> 128 -> latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder: latent_dim -> 128 -> 256 -> 784\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28 * 28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "ae = Autoencoder(latent_dim=32).to(device)\n",
    "print(f\"Autoencoder parameters: {sum(p.numel() for p in ae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder\n",
    "\n",
    "Loss: Mean Squared Error between input and reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon = model(data)\n",
    "            loss = F.mse_loss(recon, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "ae = train_autoencoder(ae, train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, test_loader, n=10):\n",
    "    model.eval()\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:n].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon = model(data)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(data[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title('Original')\n",
    "        \n",
    "        axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstructions(ae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variational Autoencoder (VAE)\n",
    "\n",
    "The key difference: VAE learns a **probability distribution** in latent space, not just point embeddings.\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "Instead of encoding to a single point z, we encode to:\n",
    "- Mean: mu\n",
    "- Log variance: log(sigma^2)\n",
    "\n",
    "Then sample: z = mu + sigma * epsilon, where epsilon ~ N(0, 1)\n",
    "\n",
    "This allows gradients to flow through the sampling operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Outputs mu and log_var\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28 * 28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mu + std * epsilon\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "vae = VAE(latent_dim=2).to(device)\n",
    "print(f\"VAE parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Loss\n",
    "\n",
    "The VAE loss has two parts:\n",
    "\n",
    "L = E[log p(x|z)] - D_KL(q(z|x) || p(z))\n",
    "\n",
    "- **Reconstruction loss**: How well can we reconstruct the input?\n",
    "- **KL Divergence**: How close is the latent distribution to a standard normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    \"\"\"ELBO loss = Reconstruction + KL Divergence\"\"\"\n",
    "    # Reconstruction loss (binary cross entropy works well for normalized images)\n",
    "    recon_loss = F.binary_cross_entropy(recon.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    \n",
    "    # KL Divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "def train_vae(model, train_loader, epochs=20, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data)\n",
    "            loss = vae_loss(recon, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "vae = train_vae(vae, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Space Visualization\n",
    "\n",
    "With a 2D latent space, we can visualize where different digits are encoded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, test_loader):\n",
    "    model.eval()\n",
    "    z_points = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            z_points.append(mu.cpu())\n",
    "            labels.append(label)\n",
    "    \n",
    "    z_points = torch.cat(z_points).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(z_points[:, 0], z_points[:, 1], c=labels, cmap='tab10', alpha=0.5, s=2)\n",
    "    plt.colorbar(scatter, label='Digit')\n",
    "    plt.xlabel('z[0]')\n",
    "    plt.ylabel('z[1]')\n",
    "    plt.title('VAE Latent Space (2D)')\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space(vae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate from Latent Space Grid\n",
    "\n",
    "We can sample a grid of points in latent space and decode them to see what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_grid(model, n=20, range_val=3):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create grid of latent points\n",
    "    grid_x = np.linspace(-range_val, range_val, n)\n",
    "    grid_y = np.linspace(-range_val, range_val, n)\n",
    "    \n",
    "    figure = np.zeros((28 * n, 28 * n))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "                decoded = model.decode(z)\n",
    "                digit = decoded[0].cpu().squeeze().numpy()\n",
    "                figure[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = digit\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.title('Generated Digits from Latent Space Grid')\n",
    "    plt.xlabel('z[0]')\n",
    "    plt.ylabel('z[1]')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_grid(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Interpolation\n",
    "\n",
    "One powerful property of VAEs: we can smoothly interpolate between two images in latent space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(model, x1, x2, steps=10):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mu1, _ = model.encode(x1.to(device))\n",
    "        mu2, _ = model.encode(x2.to(device))\n",
    "        \n",
    "        # Linear interpolation in latent space\n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, steps):\n",
    "            z = (1 - alpha) * mu1 + alpha * mu2\n",
    "            decoded = model.decode(z)\n",
    "            interpolations.append(decoded.cpu())\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, steps, figsize=(15, 2))\n",
    "    for i, img in enumerate(interpolations):\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle('Latent Space Interpolation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get two different digits\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "idx1 = (test_labels == 3).nonzero()[0].item()\n",
    "idx2 = (test_labels == 8).nonzero()[0].item()\n",
    "\n",
    "interpolate(vae, test_data[idx1:idx1+1], test_data[idx2:idx2+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beta-VAE for Disentangled Representations\n",
    "\n",
    "Beta-VAE adds a weight to the KL term to encourage **disentangled** latent factors:\n",
    "\n",
    "L = Reconstruction - beta * D_KL\n",
    "\n",
    "- beta = 1: Standard VAE\n",
    "- beta > 1: Stronger regularization, more disentangled, but worse reconstruction\n",
    "- beta < 1: Better reconstruction, less regularized latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_vae_loss(recon, x, mu, logvar, beta=4.0):\n",
    "    \"\"\"Beta-VAE loss with adjustable KL weight\"\"\"\n",
    "    recon_loss = F.binary_cross_entropy(recon.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss\n",
    "\n",
    "def train_beta_vae(model, train_loader, beta=4.0, epochs=20, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data)\n",
    "            loss = beta_vae_loss(recon, data, mu, logvar, beta=beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train with different beta values\n",
    "print(\"Training Beta-VAE with beta=4.0...\")\n",
    "beta_vae = VAE(latent_dim=10).to(device)\n",
    "beta_vae = train_beta_vae(beta_vae, train_loader, beta=4.0, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse Individual Latent Dimensions\n",
    "\n",
    "With disentangled representations, each latent dimension should control a single factor of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_traversal(model, base_z, dim, range_val=3, steps=10):\n",
    "    \"\"\"Traverse a single latent dimension while keeping others fixed\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        for val in np.linspace(-range_val, range_val, steps):\n",
    "            z = base_z.clone()\n",
    "            z[0, dim] = val\n",
    "            decoded = model.decode(z)\n",
    "            images.append(decoded.cpu().squeeze())\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Get a base encoding\n",
    "test_img = test_data[0:1].to(device)\n",
    "with torch.no_grad():\n",
    "    base_mu, _ = beta_vae.encode(test_img)\n",
    "\n",
    "# Traverse first 5 latent dimensions\n",
    "fig, axes = plt.subplots(5, 10, figsize=(15, 8))\n",
    "for dim in range(5):\n",
    "    images = latent_traversal(beta_vae, base_mu.clone(), dim)\n",
    "    for j, img in enumerate(images):\n",
    "        axes[dim, j].imshow(img, cmap='gray')\n",
    "        axes[dim, j].axis('off')\n",
    "    axes[dim, 0].set_ylabel(f'z[{dim}]', fontsize=12)\n",
    "\n",
    "plt.suptitle('Latent Dimension Traversal (Beta-VAE)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional VAE (CVAE)\n",
    "\n",
    "A CVAE conditions on additional information (like class labels) to enable controlled generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder: image + one-hot label\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28 + num_classes, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder: latent + one-hot label\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28 * 28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float()\n",
    "        x_flat = x.view(-1, 28 * 28)\n",
    "        h = self.encoder(torch.cat([x_flat, y_onehot], dim=1))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mu + std * epsilon\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float()\n",
    "        h = torch.cat([z, y_onehot], dim=1)\n",
    "        return self.decoder(h).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, y)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "cvae = CVAE(latent_dim=2, num_classes=10).to(device)\n",
    "print(f\"CVAE parameters: {sum(p.numel() for p in cvae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cvae(model, train_loader, epochs=20, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data, labels)\n",
    "            loss = vae_loss(recon, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "cvae = train_cvae(cvae, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Specific Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digits(model, n_per_digit=5):\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(10, n_per_digit, figsize=(10, 20))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            for i in range(n_per_digit):\n",
    "                z = torch.randn(1, model.latent_dim).to(device)\n",
    "                y = torch.tensor([digit]).to(device)\n",
    "                generated = model.decode(z, y)\n",
    "                \n",
    "                axes[digit, i].imshow(generated.cpu().squeeze(), cmap='gray')\n",
    "                axes[digit, i].axis('off')\n",
    "            axes[digit, 0].set_ylabel(f'{digit}', fontsize=14)\n",
    "    \n",
    "    plt.suptitle('CVAE: Generated Digits by Class', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_digits(cvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FAANG Interview Questions\n",
    "\n",
    "### Q1: What is the reparameterization trick and why is it necessary?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "The reparameterization trick allows us to backpropagate through a sampling operation.\n",
    "\n",
    "**Problem**: We want to sample z from q(z|x) = N(mu, sigma^2), but sampling is non-differentiable.\n",
    "\n",
    "**Solution**: Express sampling as a deterministic function:\n",
    "z = mu + sigma * epsilon, where epsilon ~ N(0, 1)\n",
    "\n",
    "Now:\n",
    "- mu and sigma are outputs of the encoder (differentiable)\n",
    "- epsilon is sampled from a fixed distribution (no gradients needed)\n",
    "- Gradients can flow from the loss through z back to the encoder\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Explain the ELBO loss and why VAEs use it.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "ELBO = Evidence Lower BOund. We want to maximize log p(x) but it's intractable.\n",
    "\n",
    "log p(x) >= E[log p(x|z)] - D_KL(q(z|x) || p(z))\n",
    "\n",
    "- **Reconstruction term**: The decoder should reconstruct the input well\n",
    "- **KL term**: The encoder distribution should be close to the prior p(z) = N(0, I)\n",
    "\n",
    "The KL term prevents the model from just memorizing inputs - it forces a structured latent space.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: What is posterior collapse and how do you prevent it?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Posterior collapse**: The encoder ignores the input and outputs the prior (q(z|x) approx p(z)), while the decoder ignores z and models p(x) directly.\n",
    "\n",
    "**Causes**:\n",
    "- Powerful decoder (e.g., autoregressive) that doesn't need z\n",
    "- KL term dominates early in training\n",
    "\n",
    "**Solutions**:\n",
    "1. **KL annealing**: Start with low KL weight, gradually increase\n",
    "2. **Free bits**: Minimum information that must be encoded\n",
    "3. **Beta-VAE with beta < 1**: Weaker regularization\n",
    "4. **Weaker decoders**: Force the model to use z\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: Compare VAE vs GAN for generative modeling.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "| Aspect | VAE | GAN |\n",
    "|--------|-----|-----|\n",
    "| **Training** | Stable, uses reconstruction | Adversarial, can be unstable |\n",
    "| **Sample quality** | Often blurry | Sharp, realistic |\n",
    "| **Latent space** | Smooth, continuous | Often discontinuous |\n",
    "| **Inference** | Has encoder (can get z from x) | No encoder by default |\n",
    "| **Mode coverage** | Good (covers all modes) | May suffer mode collapse |\n",
    "| **Density estimation** | Provides likelihood bound | No density |\n",
    "\n",
    "**Use VAE when**: You need a latent space, inference, or smooth interpolation.\n",
    "**Use GAN when**: Sample quality is paramount.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: What is a Beta-VAE and what does disentanglement mean?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**Beta-VAE**: Modifies VAE loss with L = Recon - beta * D_KL\n",
    "\n",
    "**Disentanglement**: Each latent dimension controls ONE independent factor of variation.\n",
    "\n",
    "Example (faces): One dimension controls hair color, another controls smile, another controls age - independently.\n",
    "\n",
    "**Why beta > 1 helps**:\n",
    "- Stronger constraint to match prior N(0, I)\n",
    "- Independent latent dimensions (prior has independent factors)\n",
    "- Trade-off: Worse reconstruction for better disentanglement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Autoencoders** learn compressed representations through encoder-decoder architecture\n",
    "2. **VAEs** extend AEs with probabilistic latent spaces using the reparameterization trick\n",
    "3. **ELBO loss** balances reconstruction quality with latent space regularity\n",
    "4. **Latent space** enables interpolation, generation, and understanding of data structure\n",
    "5. **Beta-VAE** (beta > 1) encourages disentangled representations at the cost of reconstruction\n",
    "6. **CVAE** enables conditional generation by incorporating class information\n",
    "7. **Posterior collapse** is a key failure mode - mitigate with KL annealing or architectural choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
