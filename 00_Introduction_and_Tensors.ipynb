{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Introduction and Tensors\n",
    "\n",
    "Welcome to your first PyTorch tutorial! In this notebook, we'll cover:\n",
    "- What PyTorch is and why it's useful\n",
    "- Setting up your environment\n",
    "- Understanding tensors (the fundamental data structure)\n",
    "- Basic tensor operations\n",
    "- Working with different data types and devices\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what PyTorch is and its role in deep learning\n",
    "- Know how to create and manipulate tensors\n",
    "- Be able to perform basic mathematical operations on tensors\n",
    "- Understand tensor shapes, data types, and device placement\n",
    "\n",
    "---\n",
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "**PyTorch** is an open-source machine learning framework developed by Facebook's AI Research lab. It's designed to make building and training neural networks intuitive and flexible.\n",
    "\n",
    "### Why PyTorch?\n",
    "- **Pythonic**: Feels natural to Python developers\n",
    "- **Dynamic**: Build models that change during runtime\n",
    "- **Research-friendly**: Great for experimentation\n",
    "- **Production-ready**: Can deploy models to production\n",
    "- **GPU acceleration**: Automatically uses GPU if available\n",
    "\n",
    "### Key Concepts\n",
    "- **Tensors**: Multi-dimensional arrays (like NumPy arrays, but with GPU support)\n",
    "- **Automatic Differentiation**: Automatically computes gradients (we'll learn this in the next notebook)\n",
    "- **Neural Networks**: Built using `torch.nn` module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Before we start, let's verify that PyTorch is installed and check our setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "CUDA is not available. PyTorch will use CPU (this is fine for learning!)\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch - the main library we'll be using\n",
    "import torch\n",
    "\n",
    "# Also import NumPy for comparison (you'll see why)\n",
    "import numpy as np\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "# CUDA allows us to use GPU for faster computations\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU (this is fine for learning!)\")\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM SEEDS EXPLAINED - Why and What They Do\n",
    "# ============================================================================\n",
    "# \n",
    "# WHAT IS A RANDOM SEED?\n",
    "# ----------------------\n",
    "# A random seed is a starting point for generating random numbers. \n",
    "# Computers don't generate truly random numbers - they use algorithms called\n",
    "# \"pseudo-random number generators\" (PRNGs) that produce sequences that LOOK\n",
    "# random but are actually deterministic (predictable if you know the seed).\n",
    "#\n",
    "# WHY SET A RANDOM SEED?\n",
    "# ----------------------\n",
    "# 1. **Reproducibility**: Get the same \"random\" numbers every time you run your code\n",
    "#    - Critical for debugging: if something breaks, you can reproduce it exactly\n",
    "#    - Essential for sharing code: others get the same results as you\n",
    "#    - Important for experiments: compare results across different runs\n",
    "#\n",
    "# 2. **Consistency in Machine Learning**:\n",
    "#    - Neural network weights are initialized randomly\n",
    "#    - Data shuffling is random\n",
    "#    - Dropout layers use randomness\n",
    "#    - Without a seed, you'd get different results each run, making it hard to:\n",
    "#      * Debug issues\n",
    "#      * Compare different models fairly\n",
    "#      * Reproduce published results\n",
    "#\n",
    "# 3. **Testing and Validation**:\n",
    "#    - Unit tests need predictable behavior\n",
    "#    - You want to verify your code works the same way every time\n",
    "#\n",
    "# HOW DOES IT WORK?\n",
    "# -----------------\n",
    "# When you set a seed, you're telling the random number generator:\n",
    "# \"Start your sequence from this specific point\"\n",
    "# \n",
    "# Same seed ‚Üí Same sequence of \"random\" numbers\n",
    "# Different seed ‚Üí Different sequence\n",
    "# No seed ‚Üí Different sequence each time (truly unpredictable)\n",
    "#\n",
    "# EXAMPLE:\n",
    "# --------\n",
    "# Without seed:        With seed(42):\n",
    "# Run 1: [0.3, 0.7, 0.1]  Run 1: [0.4, 0.8, 0.2]\n",
    "# Run 2: [0.9, 0.2, 0.5]  Run 2: [0.4, 0.8, 0.2]  ‚Üê Same!\n",
    "# Run 3: [0.6, 0.1, 0.9]  Run 3: [0.4, 0.8, 0.2]  ‚Üê Same!\n",
    "#\n",
    "# WHY 42?\n",
    "# -------\n",
    "# It's a common convention (from \"The Hitchhiker's Guide to the Galaxy\")\n",
    "# You can use any number - 0, 1, 123, 999, etc. The specific value doesn't matter,\n",
    "# but using the SAME value ensures reproducibility.\n",
    "#\n",
    "# IMPORTANT NOTES:\n",
    "# ----------------\n",
    "# - torch.manual_seed() sets the seed for PyTorch's random number generator\n",
    "# - np.random.seed() sets the seed for NumPy's random number generator\n",
    "# - You need BOTH if you use both libraries (which we do!)\n",
    "# - Seeds only affect the SEQUENCE, not the DISTRIBUTION of random numbers\n",
    "#   (you still get random-looking numbers, just the same ones each time)\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)  # PyTorch random operations\n",
    "np.random.seed(42)     # NumPy random operations\n",
    "\n",
    "print(\"\\n‚úÖ Random seeds set to 42 for reproducibility\")\n",
    "print(\"   (You'll get the same 'random' numbers every time you run this notebook)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Seeing Seeds in Action\n",
    "\n",
    "Let's see the difference between seeded and unseeded random numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMONSTRATION: Random Seeds\n",
      "======================================================================\n",
      "\n",
      "1. WITHOUT a fixed seed (or after resetting):\n",
      "----------------------------------------------------------------------\n",
      "   First call:  tensor([0.8823, 0.9150, 0.3829])\n",
      "   Second call: tensor([0.9593, 0.3904, 0.6009])\n",
      "   Are they the same? False\n",
      "\n",
      "2. WITH a fixed seed (42):\n",
      "----------------------------------------------------------------------\n",
      "   First call:  tensor([0.8823, 0.9150, 0.3829])\n",
      "   Second call: tensor([0.8823, 0.9150, 0.3829])\n",
      "   Are they the same? True\n",
      "\n",
      "3. DIFFERENT seeds give DIFFERENT sequences:\n",
      "----------------------------------------------------------------------\n",
      "   Seed 42: tensor([0.8823, 0.9150, 0.3829])\n",
      "   Seed 123: tensor([0.2961, 0.5166, 0.2517])\n",
      "   Are they the same? False\n",
      "\n",
      "‚úÖ Seed reset to 42 for the rest of this notebook\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: Seeds in Action\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION: Random Seeds\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Without setting a seed (or resetting it), you get different numbers each time\n",
    "print(\"1. WITHOUT a fixed seed (or after resetting):\")\n",
    "print(\"-\" * 70)\n",
    "# torch.manual_seed()  # Reset to truly random\n",
    "rand1 = torch.rand(3)\n",
    "print(f\"   First call:  {rand1}\")\n",
    "\n",
    "rand2 = torch.rand(3)\n",
    "print(f\"   Second call: {rand2}\")\n",
    "print(f\"   Are they the same? {torch.equal(rand1, rand2)}\")\n",
    "print()\n",
    "\n",
    "# With a fixed seed, you get the SAME numbers every time\n",
    "print(\"2. WITH a fixed seed (42):\")\n",
    "print(\"-\" * 70)\n",
    "torch.manual_seed(42)  # Set seed to 42\n",
    "rand3 = torch.rand(3)\n",
    "print(f\"   First call:  {rand3}\")\n",
    "\n",
    "torch.manual_seed(42)  # Reset to same seed\n",
    "rand4 = torch.rand(3)\n",
    "print(f\"   Second call: {rand4}\")\n",
    "print(f\"   Are they the same? {torch.equal(rand3, rand4)}\")\n",
    "print()\n",
    "\n",
    "# Different seeds give different sequences\n",
    "print(\"3. DIFFERENT seeds give DIFFERENT sequences:\")\n",
    "print(\"-\" * 70)\n",
    "torch.manual_seed(42)\n",
    "seq1 = torch.rand(3)\n",
    "print(f\"   Seed 42: {seq1}\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "seq2 = torch.rand(3)\n",
    "print(f\"   Seed 123: {seq2}\")\n",
    "print(f\"   Are they the same? {torch.equal(seq1, seq2)}\")\n",
    "print()\n",
    "\n",
    "# Reset to our tutorial seed\n",
    "torch.manual_seed(42)\n",
    "print(\"‚úÖ Seed reset to 42 for the rest of this notebook\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors\n",
    "\n",
    "A **tensor** is a multi-dimensional array. Think of it as:\n",
    "- **0D tensor (scalar)**: A single number\n",
    "- **1D tensor (vector)**: A list of numbers `[1, 2, 3]`\n",
    "- **2D tensor (matrix)**: A table of numbers `[[1, 2], [3, 4]]`\n",
    "- **3D tensor**: A cube of numbers\n",
    "- **nD tensor**: Higher dimensions\n",
    "\n",
    "Tensors are similar to NumPy arrays, but with additional features:\n",
    "- Can run on GPU\n",
    "- Support automatic differentiation (for machine learning)\n",
    "- Optimized for deep learning operations\n",
    "\n",
    "Let's start creating tensors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "\n",
    "There are many ways to create tensors. Let's explore the most common methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from list: tensor([1, 2, 3, 4, 5])\n",
      "Shape: torch.Size([5])\n",
      "Number of dimensions: 1\n",
      "\n",
      "2D Tensor (matrix):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "Tensor of zeros:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "Tensor of ones:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Random tensor (0 to 1):\n",
      "tensor([[0.3068, 0.1165, 0.9103],\n",
      "        [0.6440, 0.7071, 0.6581],\n",
      "        [0.4913, 0.8913, 0.1447]])\n",
      "\n",
      "Random integers (0 to 9):\n",
      "tensor([[6, 0, 0, 0],\n",
      "        [0, 1, 3, 0]])\n",
      "\n",
      "Range tensor: tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "Evenly spaced values: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Create tensor from a Python list\n",
    "# This creates a 1D tensor (vector)\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Tensor from list:\", tensor_from_list)\n",
    "print(\"Shape:\", tensor_from_list.shape)  # Shape tells us the dimensions\n",
    "print(\"Number of dimensions:\", tensor_from_list.ndim)\n",
    "print()\n",
    "\n",
    "# Method 2: Create tensor from nested lists (2D tensor / matrix)\n",
    "matrix = torch.tensor([[1, 2, 3], \n",
    "                       [4, 5, 6]])\n",
    "print(\"2D Tensor (matrix):\")\n",
    "print(matrix)\n",
    "print(\"Shape:\", matrix.shape)  # (rows, columns)\n",
    "print()\n",
    "\n",
    "# Method 3: Create tensor of zeros\n",
    "zeros = torch.zeros(3, 4)  # 3 rows, 4 columns, all zeros\n",
    "print(\"Tensor of zeros:\")\n",
    "print(zeros)\n",
    "print()\n",
    "\n",
    "# Method 4: Create tensor of ones\n",
    "ones = torch.ones(2, 3)  # 2 rows, 3 columns, all ones\n",
    "print(\"Tensor of ones:\")\n",
    "print(ones)\n",
    "print()\n",
    "\n",
    "# Method 5: Create tensor with random values\n",
    "# Random values between 0 and 1\n",
    "random_tensor = torch.rand(3, 3)\n",
    "print(\"Random tensor (0 to 1):\")\n",
    "print(random_tensor)\n",
    "print()\n",
    "\n",
    "# Method 6: Create tensor with random integers\n",
    "random_int = torch.randint(low=0, high=10, size=(2, 4))\n",
    "print(\"Random integers (0 to 9):\")\n",
    "print(random_int)\n",
    "print()\n",
    "\n",
    "# Method 7: Create tensor with a range of values (like Python's range)\n",
    "range_tensor = torch.arange(0, 10, 2)  # Start, end (exclusive), step\n",
    "print(\"Range tensor:\", range_tensor)\n",
    "print()\n",
    "\n",
    "# Method 8: Create tensor with evenly spaced values\n",
    "linspace = torch.linspace(0, 1, 5)  # Start, end, number of points\n",
    "print(\"Evenly spaced values:\", linspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Properties\n",
    "\n",
    "Every tensor has important properties we should understand:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Shape: torch.Size([2, 3])\n",
      "Number of elements: 6\n",
      "Number of dimensions: 2\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Requires gradient: False\n"
     ]
    }
   ],
   "source": [
    "# Create a sample tensor to examine its properties\n",
    "sample = torch.tensor([[1, 2, 3], \n",
    "                       [4, 5, 6]], \n",
    "                      dtype=torch.float32)  # We'll learn about dtype next\n",
    "\n",
    "print(\"Tensor:\")\n",
    "print(sample)\n",
    "print()\n",
    "\n",
    "# Shape: dimensions of the tensor\n",
    "print(\"Shape:\", sample.shape)  # Also: sample.size()\n",
    "print(\"Number of elements:\", sample.numel())  # Total count of elements\n",
    "print(\"Number of dimensions:\", sample.ndim)  # Also: len(sample.shape)\n",
    "print(\"Data type:\", sample.dtype)  # What type of numbers are stored\n",
    "print(\"Device:\", sample.device)  # Where tensor is stored (CPU or GPU)\n",
    "print(\"Requires gradient:\", sample.requires_grad)  # For automatic differentiation (next notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types (dtype)\n",
    "\n",
    "Tensors can store different types of numbers. The most common ones are:\n",
    "- `torch.float32` or `torch.float`: 32-bit floating point (default for most operations)\n",
    "- `torch.float64` or `torch.double`: 64-bit floating point (more precision)\n",
    "- `torch.int32` or `torch.int`: 32-bit integers\n",
    "- `torch.int64` or `torch.long`: 64-bit integers\n",
    "- `torch.bool`: Boolean values (True/False)\n",
    "\n",
    "**Why does this matter?**\n",
    "- Different types use different amounts of memory\n",
    "- Some operations require specific types\n",
    "- Float32 is usually sufficient and faster than float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float tensor: tensor([1.5000, 2.7000, 3.9000]) | dtype: torch.float32\n",
      "Int tensor: tensor([1, 2, 3], dtype=torch.int32) | dtype: torch.int32\n",
      "Bool tensor: tensor([ True, False,  True]) | dtype: torch.bool\n",
      "\n",
      "Int converted to float: tensor([1., 2., 3.]) | dtype: torch.float32\n",
      "Float converted to int: tensor([1, 2, 3], dtype=torch.int32) | dtype: torch.int32\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with different data types\n",
    "float_tensor = torch.tensor([1.5, 2.7, 3.9], dtype=torch.float32)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "bool_tensor = torch.tensor([True, False, True], dtype=torch.bool)\n",
    "\n",
    "print(\"Float tensor:\", float_tensor, \"| dtype:\", float_tensor.dtype)\n",
    "print(\"Int tensor:\", int_tensor, \"| dtype:\", int_tensor.dtype)\n",
    "print(\"Bool tensor:\", bool_tensor, \"| dtype:\", bool_tensor.dtype)\n",
    "print()\n",
    "\n",
    "# Convert between types (called \"casting\")\n",
    "# This is important when you need to change types for operations\n",
    "int_to_float = int_tensor.float()  # Convert int to float\n",
    "print(\"Int converted to float:\", int_to_float, \"| dtype:\", int_to_float.dtype)\n",
    "\n",
    "# Or use .to() method\n",
    "float_to_int = float_tensor.int()  # Note: this truncates (removes decimal part)\n",
    "print(\"Float converted to int:\", float_to_int, \"| dtype:\", float_to_int.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n",
    "\n",
    "Now let's learn how to perform mathematical operations on tensors. These operations are fundamental to deep learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([1, 2, 3, 4])\n",
      "Tensor b: tensor([5, 6, 7, 8])\n",
      "\n",
      "a + b = tensor([ 6,  8, 10, 12])\n",
      "torch.add(a, b) = tensor([ 6,  8, 10, 12])\n",
      "\n",
      "a - b = tensor([-4, -4, -4, -4])\n",
      "\n",
      "a * b = tensor([ 5, 12, 21, 32])\n",
      "\n",
      "b / a = tensor([5.0000, 3.0000, 2.3333, 2.0000])\n",
      "\n",
      "a ** 2 = tensor([ 1,  4,  9, 16])\n",
      "\n",
      "a + 10 = tensor([11, 12, 13, 14])\n",
      "a * 2 = tensor([2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors for operations\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([5, 6, 7, 8])\n",
    "\n",
    "print(\"Tensor a:\", a)\n",
    "print(\"Tensor b:\", b)\n",
    "print()\n",
    "\n",
    "# Addition (element-wise: adds corresponding elements)\n",
    "print(\"a + b =\", a + b)\n",
    "print(\"torch.add(a, b) =\", torch.add(a, b))  # Alternative syntax\n",
    "print()\n",
    "\n",
    "# Subtraction\n",
    "print(\"a - b =\", a - b)\n",
    "print()\n",
    "\n",
    "# Multiplication (element-wise, NOT matrix multiplication)\n",
    "print(\"a * b =\", a * b)\n",
    "print()\n",
    "\n",
    "# Division (element-wise)\n",
    "print(\"b / a =\", b / a)\n",
    "print()\n",
    "\n",
    "# Power (element-wise)\n",
    "print(\"a ** 2 =\", a ** 2)  # Square each element\n",
    "print()\n",
    "\n",
    "# Scalar operations (operating with a single number)\n",
    "print(\"a + 10 =\", a + 10)  # Adds 10 to each element\n",
    "print(\"a * 2 =\", a * 2)  # Multiplies each element by 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations\n",
    "\n",
    "For neural networks, we often need matrix multiplication (not element-wise multiplication):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Matrix B:\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "\n",
      "Element-wise multiplication (A * B):\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "\n",
      "Matrix multiplication (A @ B):\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n",
      "torch.matmul(A, B):\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two matrices\n",
    "matrix_a = torch.tensor([[1, 2],\n",
    "                         [3, 4]])\n",
    "\n",
    "matrix_b = torch.tensor([[5, 6],\n",
    "                         [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(matrix_a)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(matrix_b)\n",
    "print()\n",
    "\n",
    "# Element-wise multiplication (NOT matrix multiplication)\n",
    "print(\"Element-wise multiplication (A * B):\")\n",
    "print(matrix_a * matrix_b)\n",
    "print()\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "# This is what we use in neural networks!\n",
    "print(\"Matrix multiplication (A @ B):\")\n",
    "print(matrix_a @ matrix_b)  # @ is the matrix multiplication operator\n",
    "print()\n",
    "\n",
    "# Alternative syntax for matrix multiplication\n",
    "print(\"torch.matmul(A, B):\")\n",
    "print(torch.matmul(matrix_a, matrix_b))\n",
    "print()\n",
    "\n",
    "# Note: For matrix multiplication, the number of columns in A \n",
    "# must equal the number of rows in B\n",
    "# Shape of A: (2, 2), Shape of B: (2, 2) ‚Üí Result: (2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mathematical Functions\n",
    "\n",
    "PyTorch provides many mathematical functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([-2., -1.,  0.,  1.,  2.])\n",
      "\n",
      "Absolute value: tensor([2., 1., 0., 1., 2.])\n",
      "\n",
      "Square root of tensor([ 1.,  4.,  9., 16.]) = tensor([1., 2., 3., 4.])\n",
      "\n",
      "Exponential of tensor([-2., -1.,  0.,  1.,  2.]) = tensor([0.1353, 0.3679, 1.0000, 2.7183, 7.3891])\n",
      "\n",
      "Natural log of tensor([1., 2., 3., 4.]) = tensor([0.0000, 0.6931, 1.0986, 1.3863])\n",
      "\n",
      "Sum of tensor([-2., -1.,  0.,  1.,  2.]) = tensor(0.)\n",
      "Mean of tensor([-2., -1.,  0.,  1.,  2.]) = tensor(0.)\n",
      "Max of tensor([-2., -1.,  0.,  1.,  2.]) = tensor(2.)\n",
      "Min of tensor([-2., -1.,  0.,  1.,  2.]) = tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with some values\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(\"Original tensor:\", x)\n",
    "print()\n",
    "\n",
    "# Absolute value\n",
    "print(\"Absolute value:\", torch.abs(x))\n",
    "print()\n",
    "\n",
    "# Square root (only works with non-negative values)\n",
    "positive = torch.tensor([1.0, 4.0, 9.0, 16.0])\n",
    "print(\"Square root of\", positive, \"=\", torch.sqrt(positive))\n",
    "print()\n",
    "\n",
    "# Exponential (e^x)\n",
    "print(\"Exponential of\", x, \"=\", torch.exp(x))\n",
    "print()\n",
    "\n",
    "# Logarithm (natural log)\n",
    "positive_only = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(\"Natural log of\", positive_only, \"=\", torch.log(positive_only))\n",
    "print()\n",
    "\n",
    "# Sum of all elements\n",
    "print(\"Sum of\", x, \"=\", torch.sum(x))\n",
    "print(\"Mean of\", x, \"=\", torch.mean(x.float()))  # Mean requires float\n",
    "print(\"Max of\", x, \"=\", torch.max(x))\n",
    "print(\"Min of\", x, \"=\", torch.min(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "Just like NumPy arrays, we can access specific elements or slices of tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Shape: torch.Size([3, 4])\n",
      "\n",
      "Element at [0, 2]: tensor(3)\n",
      "\n",
      "Row 1: tensor([5, 6, 7, 8])\n",
      "\n",
      "Column 2: tensor([ 3,  7, 11])\n",
      "\n",
      "Slice [0:2, 1:3]:\n",
      "tensor([[2, 3],\n",
      "        [6, 7]])\n",
      "\n",
      "Last row: tensor([ 9, 10, 11, 12])\n",
      "\n",
      "Last column: tensor([ 4,  8, 12])\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D tensor\n",
    "matrix = torch.tensor([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(\"Shape:\", matrix.shape)\n",
    "print()\n",
    "\n",
    "# Access a single element (row 0, column 2)\n",
    "print(\"Element at [0, 2]:\", matrix[0, 2])\n",
    "print()\n",
    "\n",
    "# Access an entire row (row 1)\n",
    "print(\"Row 1:\", matrix[1, :])  # : means \"all columns\"\n",
    "print()\n",
    "\n",
    "# Access an entire column (column 2)\n",
    "print(\"Column 2:\", matrix[:, 2])  # : means \"all rows\"\n",
    "print()\n",
    "\n",
    "# Slice: get rows 0 to 1, columns 1 to 3\n",
    "print(\"Slice [0:2, 1:3]:\")\n",
    "print(matrix[0:2, 1:3])\n",
    "print()\n",
    "\n",
    "# Access last row\n",
    "print(\"Last row:\", matrix[-1, :])\n",
    "print()\n",
    "\n",
    "# Access last column\n",
    "print(\"Last column:\", matrix[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "\n",
    "Often we need to change the shape of a tensor without changing its data. This is crucial for neural networks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Original shape: torch.Size([12])\n",
      "\n",
      "Reshaped to (3, 4):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "New shape: torch.Size([3, 4])\n",
      "\n",
      "Reshaped to (2, 6):\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "\n",
      "Flattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "Auto reshape (3, -1):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Using view (4, 3):\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with 12 elements\n",
    "original = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(\"Original tensor:\", original)\n",
    "print(\"Original shape:\", original.shape)\n",
    "print()\n",
    "\n",
    "# Reshape to 3 rows, 4 columns\n",
    "reshaped = original.reshape(3, 4)\n",
    "print(\"Reshaped to (3, 4):\")\n",
    "print(reshaped)\n",
    "print(\"New shape:\", reshaped.shape)\n",
    "print()\n",
    "\n",
    "# Reshape to 2 rows, 6 columns\n",
    "reshaped2 = original.reshape(2, 6)\n",
    "print(\"Reshaped to (2, 6):\")\n",
    "print(reshaped2)\n",
    "print()\n",
    "\n",
    "# Flatten: convert to 1D\n",
    "flattened = reshaped.flatten()\n",
    "print(\"Flattened:\", flattened)\n",
    "print()\n",
    "\n",
    "# Reshape using -1 (PyTorch calculates the dimension automatically)\n",
    "# -1 means \"figure this out for me\"\n",
    "auto_reshape = original.reshape(3, -1)  # 3 rows, auto-calculate columns\n",
    "print(\"Auto reshape (3, -1):\")\n",
    "print(auto_reshape)\n",
    "print()\n",
    "\n",
    "# View: similar to reshape but shares memory (faster, but be careful!)\n",
    "viewed = original.view(4, 3)\n",
    "print(\"Using view (4, 3):\")\n",
    "print(viewed)\n",
    "print()\n",
    "\n",
    "# Important: The total number of elements must stay the same!\n",
    "# original has 12 elements, so valid shapes are: (12,), (1, 12), (2, 6), (3, 4), (4, 3), (6, 2), (12, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between NumPy and PyTorch\n",
    "\n",
    "PyTorch tensors and NumPy arrays are similar and can be converted to each other:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison: Operations\n",
    "\n",
    "Let's see how similar operations work in both:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPERATION COMPARISON: NumPy vs PyTorch\n",
      "======================================================================\n",
      "\n",
      "Original data:\n",
      "NumPy:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "PyTorch:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "1. Getting shape:\n",
      "   NumPy:   np_arr.shape = (2, 3)\n",
      "   PyTorch: torch_tensor.shape = torch.Size([2, 3])\n",
      "\n",
      "2. Reshaping:\n",
      "   NumPy:   np_arr.reshape(3, 2) =\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "   PyTorch: torch_tensor.reshape(3, 2) =\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "3. Mathematical operations:\n",
      "   NumPy sum:   21\n",
      "   PyTorch sum: 21\n",
      "\n",
      "   NumPy mean:   3.5\n",
      "   PyTorch mean: 3.5\n",
      "\n",
      "4. Indexing (same syntax!):\n",
      "   NumPy [0, 1]:   2\n",
      "   PyTorch [0, 1]: 2\n",
      "\n",
      "5. Element-wise operations:\n",
      "   NumPy * 2:\n",
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n",
      "   PyTorch * 2:\n",
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Most operations are very similar!\n",
      "   The main difference: PyTorch supports GPU and gradients\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison of operations\n",
    "print(\"=\" * 70)\n",
    "print(\"OPERATION COMPARISON: NumPy vs PyTorch\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Create similar arrays/tensors\n",
    "np_arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "torch_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(f\"NumPy:\\n{np_arr}\")\n",
    "print(f\"PyTorch:\\n{torch_tensor}\")\n",
    "print()\n",
    "\n",
    "# 1. Shape\n",
    "print(\"1. Getting shape:\")\n",
    "print(f\"   NumPy:   np_arr.shape = {np_arr.shape}\")\n",
    "print(f\"   PyTorch: torch_tensor.shape = {torch_tensor.shape}\")\n",
    "print()\n",
    "\n",
    "# 2. Reshape\n",
    "print(\"2. Reshaping:\")\n",
    "print(f\"   NumPy:   np_arr.reshape(3, 2) =\\n{np_arr.reshape(3, 2)}\")\n",
    "print(f\"   PyTorch: torch_tensor.reshape(3, 2) =\\n{torch_tensor.reshape(3, 2)}\")\n",
    "print()\n",
    "\n",
    "# 3. Mathematical operations\n",
    "print(\"3. Mathematical operations:\")\n",
    "print(f\"   NumPy sum:   {np.sum(np_arr)}\")\n",
    "print(f\"   PyTorch sum: {torch.sum(torch_tensor).item()}\")\n",
    "print()\n",
    "\n",
    "print(f\"   NumPy mean:   {np.mean(np_arr)}\")\n",
    "print(f\"   PyTorch mean: {torch.mean(torch_tensor.float()).item()}\")\n",
    "print()\n",
    "\n",
    "# 4. Indexing (identical syntax!)\n",
    "print(\"4. Indexing (same syntax!):\")\n",
    "print(f\"   NumPy [0, 1]:   {np_arr[0, 1]}\")\n",
    "print(f\"   PyTorch [0, 1]: {torch_tensor[0, 1].item()}\")\n",
    "print()\n",
    "\n",
    "# 5. Element-wise operations\n",
    "print(\"5. Element-wise operations:\")\n",
    "print(f\"   NumPy * 2:\\n{np_arr * 2}\")\n",
    "print(f\"   PyTorch * 2:\\n{torch_tensor * 2}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Most operations are very similar!\")\n",
    "print(\"   The main difference: PyTorch supports GPU and gradients\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways: Tensors vs NumPy Arrays\n",
    "\n",
    "**Similarities:**\n",
    "- Both are multi-dimensional arrays\n",
    "- Similar syntax for most operations\n",
    "- Can convert between them easily\n",
    "- Both support indexing, slicing, reshaping\n",
    "\n",
    "**Differences:**\n",
    "- **Tensors** support GPU acceleration (CUDA/MPS)\n",
    "- **Tensors** support automatic differentiation (gradients)\n",
    "- **Tensors** are optimized for neural network training\n",
    "- **NumPy** is better for general scientific computing\n",
    "- **NumPy** is more widely used in non-ML contexts\n",
    "\n",
    "**Best Practice:**\n",
    "- Use **NumPy** for data preprocessing and general computation\n",
    "- Use **PyTorch Tensors** when building/training neural networks\n",
    "- Convert between them as needed (they work well together!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONVERSION EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONVERTING BETWEEN NUMPY AND PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 1. NumPy ‚Üí PyTorch Tensor\n",
    "print(\"1. Converting NumPy array to PyTorch tensor:\")\n",
    "print(\"-\" * 70)\n",
    "numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"NumPy array: {numpy_array}\")\n",
    "print(f\"Type: {type(numpy_array)}\")\n",
    "print(f\"dtype: {numpy_array.dtype}\")\n",
    "print()\n",
    "\n",
    "torch_tensor = torch.from_numpy(numpy_array)\n",
    "print(f\"PyTorch tensor: {torch_tensor}\")\n",
    "print(f\"Type: {type(torch_tensor)}\")\n",
    "print(f\"dtype: {torch_tensor.dtype}\")\n",
    "print()\n",
    "\n",
    "# 2. PyTorch ‚Üí NumPy\n",
    "print(\"2. Converting PyTorch tensor to NumPy array:\")\n",
    "print(\"-\" * 70)\n",
    "back_to_numpy = torch_tensor.numpy()\n",
    "print(f\"NumPy array: {back_to_numpy}\")\n",
    "print(f\"Type: {type(back_to_numpy)}\")\n",
    "print()\n",
    "\n",
    "# 3. Memory Sharing (Important!)\n",
    "print(\"3. Memory Sharing (they share the same memory!):\")\n",
    "print(\"-\" * 70)\n",
    "shared_numpy = np.array([10, 20, 30])\n",
    "shared_tensor = torch.from_numpy(shared_numpy)\n",
    "\n",
    "print(f\"Original NumPy: {shared_numpy}\")\n",
    "print(f\"Tensor: {shared_tensor}\")\n",
    "print()\n",
    "\n",
    "# Modify NumPy array\n",
    "shared_numpy[0] = 99\n",
    "print(\"After modifying NumPy array:\")\n",
    "print(f\"NumPy: {shared_numpy}\")\n",
    "print(f\"Tensor: {shared_tensor}  ‚Üê Also changed!\")\n",
    "print(\"‚ö†Ô∏è  They share memory - changes to one affect the other!\")\n",
    "print()\n",
    "\n",
    "# To avoid sharing, use .clone() or .copy()\n",
    "print(\"4. Creating independent copies:\")\n",
    "print(\"-\" * 70)\n",
    "independent_numpy = np.array([1, 2, 3])\n",
    "independent_tensor = torch.from_numpy(independent_numpy).clone()  # .clone() breaks the link\n",
    "\n",
    "independent_numpy[0] = 999\n",
    "print(f\"NumPy (modified): {independent_numpy}\")\n",
    "print(f\"Tensor (unchanged): {independent_tensor}\")\n",
    "print(\"‚úÖ Now they're independent!\")\n",
    "print()\n",
    "\n",
    "# 5. GPU Tensors (Important!)\n",
    "print(\"5. GPU Tensors (must move to CPU first):\")\n",
    "print(\"-\" * 70)\n",
    "if torch.backends.mps.is_available():\n",
    "    gpu_tensor = torch.tensor([1, 2, 3]).to('mps')\n",
    "    print(f\"GPU tensor: {gpu_tensor}\")\n",
    "    print(f\"Device: {gpu_tensor.device}\")\n",
    "    print()\n",
    "    print(\"To convert to NumPy, move to CPU first:\")\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    numpy_from_gpu = cpu_tensor.numpy()\n",
    "    print(f\"NumPy array: {numpy_from_gpu}\")\n",
    "    print(\"‚úÖ Always use: tensor.cpu().numpy() for GPU tensors\")\n",
    "elif torch.cuda.is_available():\n",
    "    gpu_tensor = torch.tensor([1, 2, 3]).cuda()\n",
    "    print(f\"GPU tensor: {gpu_tensor}\")\n",
    "    print(f\"Device: {gpu_tensor.device}\")\n",
    "    print()\n",
    "    print(\"To convert to NumPy, move to CPU first:\")\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    numpy_from_gpu = cpu_tensor.numpy()\n",
    "    print(f\"NumPy array: {numpy_from_gpu}\")\n",
    "    print(\"‚úÖ Always use: tensor.cpu().numpy() for GPU tensors\")\n",
    "else:\n",
    "    print(\"No GPU available, but the principle is the same:\")\n",
    "    print(\"tensor.cpu().numpy()  # Move to CPU, then convert\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Management (CPU vs GPU)\n",
    "\n",
    "PyTorch can run computations on either CPU or GPU. GPU is much faster for large operations, but CPU is fine for learning!\n",
    "\n",
    "**Key points:**\n",
    "- By default, tensors are created on CPU\n",
    "- GPU tensors are faster for large operations\n",
    "- You need a compatible GPU and CUDA installed to use GPU\n",
    "- For learning, CPU is perfectly fine!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU tensor: tensor([1, 2, 3, 4])\n",
      "Device: cpu\n",
      "\n",
      "GPU not available. Using CPU (this is fine for learning!)\n",
      "All operations will run on CPU automatically.\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor (defaults to CPU)\n",
    "cpu_tensor = torch.tensor([1, 2, 3, 4])\n",
    "print(\"CPU tensor:\", cpu_tensor)\n",
    "print(\"Device:\", cpu_tensor.device)\n",
    "print()\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Move tensor to GPU\n",
    "    gpu_tensor = cpu_tensor.cuda()  # or .to('cuda')\n",
    "    print(\"GPU tensor:\", gpu_tensor)\n",
    "    print(\"Device:\", gpu_tensor.device)\n",
    "    print()\n",
    "    \n",
    "    # Move back to CPU\n",
    "    back_to_cpu = gpu_tensor.cpu()\n",
    "    print(\"Back to CPU:\", back_to_cpu)\n",
    "    print(\"Device:\", back_to_cpu.device)\n",
    "else:\n",
    "    print(\"GPU not available. Using CPU (this is fine for learning!)\")\n",
    "    print(\"All operations will run on CPU automatically.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce what you've learned:\n",
    "\n",
    "### Exercise 1: Create and Manipulate Tensors\n",
    "1. Create a tensor with values from 0 to 9\n",
    "2. Reshape it to a 2x5 matrix\n",
    "3. Calculate the sum of all elements\n",
    "4. Calculate the mean of each row\n",
    "\n",
    "### Exercise 2: Matrix Operations\n",
    "1. Create two 3x3 matrices with random values\n",
    "2. Perform element-wise multiplication\n",
    "3. Perform matrix multiplication\n",
    "4. Calculate the sum of the diagonal elements\n",
    "\n",
    "### Exercise 3: Indexing\n",
    "1. Create a 4x4 tensor with values from 0 to 15\n",
    "2. Extract the first row\n",
    "3. Extract the last column\n",
    "4. Extract a 2x2 submatrix from the center\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to Exercises\n",
    "\n",
    "### Exercise 1 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "# 1. Create tensor with values 0-9\n",
    "ex1_tensor = torch.arange(10)\n",
    "print(\"1. Tensor 0-9:\", ex1_tensor)\n",
    "\n",
    "# 2. Reshape to 2x5\n",
    "ex1_reshaped = ex1_tensor.reshape(2, 5)\n",
    "print(\"\\n2. Reshaped to 2x5:\")\n",
    "print(ex1_reshaped)\n",
    "\n",
    "# 3. Sum of all elements\n",
    "ex1_sum = torch.sum(ex1_tensor)\n",
    "print(\"\\n3. Sum of all elements:\", ex1_sum)\n",
    "\n",
    "# 4. Mean of each row\n",
    "ex1_row_means = torch.mean(ex1_reshaped.float(), dim=1)  # dim=1 means along columns (per row)\n",
    "print(\"\\n4. Mean of each row:\", ex1_row_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "# 1. Create two 3x3 random matrices\n",
    "ex2_a = torch.rand(3, 3)\n",
    "ex2_b = torch.rand(3, 3)\n",
    "print(\"Matrix A:\")\n",
    "print(ex2_a)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(ex2_b)\n",
    "\n",
    "# 2. Element-wise multiplication\n",
    "ex2_elementwise = ex2_a * ex2_b\n",
    "print(\"\\n2. Element-wise multiplication:\")\n",
    "print(ex2_elementwise)\n",
    "\n",
    "# 3. Matrix multiplication\n",
    "ex2_matmul = ex2_a @ ex2_b\n",
    "print(\"\\n3. Matrix multiplication:\")\n",
    "print(ex2_matmul)\n",
    "\n",
    "# 4. Sum of diagonal elements (trace)\n",
    "ex2_trace = torch.trace(ex2_a)  # Sum of diagonal\n",
    "print(\"\\n4. Sum of diagonal of A (trace):\", ex2_trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "# 1. Create 4x4 tensor with values 0-15\n",
    "ex3_tensor = torch.arange(16).reshape(4, 4)\n",
    "print(\"1. 4x4 tensor:\")\n",
    "print(ex3_tensor)\n",
    "\n",
    "# 2. Extract first row\n",
    "ex3_first_row = ex3_tensor[0, :]\n",
    "print(\"\\n2. First row:\", ex3_first_row)\n",
    "\n",
    "# 3. Extract last column\n",
    "ex3_last_col = ex3_tensor[:, -1]\n",
    "print(\"\\n3. Last column:\", ex3_last_col)\n",
    "\n",
    "# 4. Extract 2x2 submatrix from center (rows 1-2, columns 1-2)\n",
    "ex3_submatrix = ex3_tensor[1:3, 1:3]\n",
    "print(\"\\n4. 2x2 submatrix from center:\")\n",
    "print(ex3_submatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Tensors** are multi-dimensional arrays, similar to NumPy arrays but with GPU support\n",
    "2. **Creating tensors**: Use `torch.tensor()`, `torch.zeros()`, `torch.ones()`, `torch.rand()`, etc.\n",
    "3. **Operations**: Element-wise (`+`, `-`, `*`, `/`) vs matrix multiplication (`@` or `torch.matmul()`)\n",
    "4. **Shape matters**: Use `.shape` to check dimensions, `.reshape()` to change them\n",
    "5. **Data types**: Specify with `dtype` parameter, convert with `.float()`, `.int()`, etc.\n",
    "6. **Indexing**: Works like NumPy - use `[row, col]` or slices `[start:end]`\n",
    "7. **Device**: Tensors default to CPU; can move to GPU with `.cuda()` if available\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook, we'll learn about:\n",
    "- **Automatic Differentiation**: How PyTorch automatically computes gradients\n",
    "- **Gradients**: Understanding the math behind neural network training\n",
    "- **Backpropagation**: The algorithm that makes deep learning possible\n",
    "\n",
    "This is where PyTorch really shines - automatic gradient computation is what makes training neural networks feasible!\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing your first PyTorch notebook! üéâ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
