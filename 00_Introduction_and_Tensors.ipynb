{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12d7ac8",
   "metadata": {},
   "source": [
    "# PyTorch Deep Dive: Introduction and Tensors\n",
    "\n",
    "Welcome to the absolute foundation of Deep Learning. \n",
    "\n",
    "Before we write code, we need to define the **Objects** we are working with.\n",
    "\n",
    "## Learning Objectives\n",
    "- **The Vocabulary**: What is a \"Tensor\", \"Scalar\", \"Vector\", and \"Matrix\"?\n",
    "- **The Intuition**: Tensors as \"Excel Sheets on Steroids\".\n",
    "- **The Hardware**: CPU vs GPU (Minivan vs Sports Car).\n",
    "- **The Mechanics**: Shapes, Dtypes, and Broadcasting.\n",
    "- **The Deep Dive**: Strides and Memory Layout (What actually happens in RAM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84392d28",
   "metadata": {},
   "source": [
    "## Part 1: The Vocabulary (Definitions First)\n",
    "\n",
    "In Data Science, we have specific names for containers of numbers based on their dimensions.\n",
    "\n",
    "### 1. Scalar (0D)\n",
    "- A single number.\n",
    "- Example: `7`, `3.14`.\n",
    "- Use case: A loss value, a learning rate.\n",
    "\n",
    "### 2. Vector (1D)\n",
    "- A list of numbers.\n",
    "- Example: `[1, 2, 3]`.\n",
    "- Use case: A row of data, a bias term.\n",
    "\n",
    "### 3. Matrix (2D)\n",
    "- A grid of numbers (Rows and Columns).\n",
    "- Example: An Excel sheet, a black-and-white image.\n",
    "- Use case: A dataset, weights of a linear layer.\n",
    "\n",
    "### 4. Tensor (ND)\n",
    "- A generic term for N-dimensional arrays (3D, 4D, etc.).\n",
    "- Example: A color image (Height x Width x 3 Channels), a video (Time x H x W x C).\n",
    "- Use case: Complex data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233bafd",
   "metadata": {},
   "source": [
    "## Part 2: The Intuition (Excel on Steroids)\n",
    "\n",
    "Now that we know the terms, let's visualize them.\n",
    "\n",
    "- **2D Tensor**: Imagine a single Excel sheet.\n",
    "- **3D Tensor**: Imagine a **Workbook** containing multiple Excel sheets.\n",
    "- **4D Tensor**: Imagine a **Filing Cabinet** containing multiple Workbooks.\n",
    "- **5D Tensor**: Imagine a **Library** containing multiple Filing Cabinets.\n",
    "\n",
    "PyTorch Tensors are just these containers, but optimized for math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0D Tensor (Scalar)\n",
    "scalar = torch.tensor(7)\n",
    "print(f\"Scalar: {scalar.item()} (Shape: {scalar.shape})\")\n",
    "\n",
    "# 1D Tensor (Vector)\n",
    "vector = torch.tensor([7, 2, 5])\n",
    "print(f\"Vector: {vector} (Shape: {vector.shape})\")\n",
    "\n",
    "# 2D Tensor (Matrix)\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "print(f\"Matrix:\\n{matrix} (Shape: {matrix.shape})\")\n",
    "\n",
    "# 3D Tensor (Image-like)\n",
    "tensor3d = torch.rand(3, 4, 4) # 3 Channels, 4x4 pixels\n",
    "print(f\"3D Tensor Shape: {tensor3d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c8263",
   "metadata": {},
   "source": [
    "## Part 3: The Hardware (Minivan vs Sports Car)\n",
    "\n",
    "Why do we use PyTorch Tensors instead of Python lists or NumPy arrays?\n",
    "\n",
    "**The GPU.**\n",
    "\n",
    "- **CPU (Central Processing Unit)**: Like a **Minivan**. It can carry a few things (instructions) very quickly and flexibly. Good for sequential logic.\n",
    "- **GPU (Graphics Processing Unit)**: Like a **Train** or **Fleet of Sports Cars**. It can carry MASSIVE amounts of data at once, but it's hard to turn. Good for parallel math (matrix multiplication).\n",
    "\n",
    "PyTorch Tensors can live on the GPU. NumPy arrays cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cff95",
   "metadata": {},
   "outputs": [],
   "source": "# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif device.type == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Move tensor to GPU\nx = torch.tensor([1, 2, 3])\nx_gpu = x.to(device)\n\nprint(f\"x is on: {x.device}\")\nprint(f\"x_gpu is on: {x_gpu.device}\")\n\n# Note: You cannot add a CPU tensor to a GPU tensor. They live in different worlds!\n# x + x_gpu # This would error"
  },
  {
   "cell_type": "markdown",
   "id": "84446d6c",
   "metadata": {},
   "source": [
    "## Part 4: The Mechanics (Broadcasting)\n",
    "\n",
    "Broadcasting is magic. It allows you to do math on tensors of different shapes.\n",
    "\n",
    "Imagine you have a matrix of students' scores (Rows=Students, Cols=Subjects).\n",
    "You want to add 5 bonus points to **every** score.\n",
    "\n",
    "You don't need to create a matrix of 5s. You just add `5`.\n",
    "PyTorch \"broadcasts\" (stretches) the `5` to match the matrix shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1654a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.tensor([[10, 20, 30], \n",
    "                       [40, 50, 60]])\n",
    "\n",
    "# Add scalar (Broadcasting 5 to match 2x3)\n",
    "print(\"Matrix + 5:\\n\", matrix + 5)\n",
    "\n",
    "# Add vector (Broadcasting [1, 2, 3] to every row)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"Matrix + Vector:\\n\", matrix + vector)\n",
    "\n",
    "# Visualizing what happened:\n",
    "# [10, 20, 30] + [1, 2, 3] = [11, 22, 33]\n",
    "# [40, 50, 60] + [1, 2, 3] = [41, 52, 63]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7610967",
   "metadata": {},
   "source": [
    "## Part 5: The Deep Dive (Strides & Memory)\n",
    "\n",
    "Here is the secret: **A Tensor is just a 1D array in memory.**\n",
    "\n",
    "The \"shape\" (rows, columns) is an illusion created by **Strides**.\n",
    "\n",
    "- **Stride**: How many steps in memory do I need to jump to get to the next element in this dimension?\n",
    "\n",
    "Example: Matrix 2x3\n",
    "`[[1, 2, 3], [4, 5, 6]]`\n",
    "\n",
    "In memory (RAM): `[1, 2, 3, 4, 5, 6]`\n",
    "\n",
    "To go from `1` to `2` (next column), jump 1 step.\n",
    "To go from `1` to `4` (next row), jump 3 steps.\n",
    "\n",
    "Stride = `(3, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(f\"Shape: {t.shape}\")\n",
    "print(f\"Stride: {t.stride()}\")\n",
    "\n",
    "# Transpose it (swap rows/cols)\n",
    "t_T = t.t()\n",
    "print(f\"\\nTransposed Shape: {t_T.shape}\")\n",
    "print(f\"Transposed Stride: {t_T.stride()}\")\n",
    "\n",
    "# Notice: The DATA in memory didn't move! PyTorch just swapped the strides.\n",
    "# This makes transposing instant, even for huge matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Checklist\n",
    "\n",
    "1. **Tensor** = N-dimensional container for numbers.\n",
    "2. **GPU** = Parallel processing beast (use `.to(device)`).\n",
    "3. **Broadcasting** = Magic stretching of shapes for math.\n",
    "4. **Strides** = The metadata that defines the shape over linear memory.\n",
    "\n",
    "You now understand the data structure. Next, we'll learn how to calculate gradients (Autograd)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}